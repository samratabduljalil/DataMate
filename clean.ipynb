{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "loader = UnstructuredPDFLoader(file_path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_without_newlines\n",
    "for document in documents:\n",
    "    text = document.page_content\n",
    "    text_without_newlines = re.sub(r'\\n+', ' ', text)\n",
    "\n",
    "\n",
    "text_without_newlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Ollama server details\n",
    "OLLAMA_URL = \"http://127.0.0.1:12345\"\n",
    "\n",
    "def get_nomic_embedding(text):\n",
    "    payload = {\n",
    "        \"model\": \"nomic-embed-text\",\n",
    "        \"prompt\": text,\n",
    "        \"embed\": True\n",
    "    }\n",
    "    response = requests.post(f\"{OLLAMA_URL}/api/embeddings\", json=payload)\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"embedding\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "\n",
    "# Test with a sample text\n",
    "text = \"The quick brown fox jumps over the lazy.\"\n",
    "embedding = get_nomic_embedding(text)\n",
    "\n",
    "if embedding:\n",
    "    print(\"Embedding Generated:\")\n",
    "    print(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "# Connect to the Ollama server\n",
    "ollama_llm = Ollama(\n",
    "    model=\"llama3\",\n",
    "    base_url=\"http://127.0.0.1:12345\"\n",
    ")\n",
    "\n",
    "# Test with a prompt\n",
    "prompt = \"who is samrat abdul jalil\"\n",
    "response = ollama_llm(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with this code add croma db as vector database create collection using file path name . make chunk ,overlap using langchain. when i qury in chroma db it will give me 3 chunck .using this data send it to ollama for genarating text according to query full local rag system . now give code sell wise . like pdf loading , chroma query then ollama text genaration . i will use it in jupiter notebook .  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "UniqueConstraintError",
     "evalue": "Collection machin2e already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUniqueConstraintError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m client \u001b[38;5;241m=\u001b[39m chromadb\u001b[38;5;241m.\u001b[39mClient()\n\u001b[0;32m     35\u001b[0m collection_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmachin2e\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 36\u001b[0m collection \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Generate embeddings for each text chunk and add to Chroma collection\u001b[39;00m\n\u001b[0;32m     39\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m [get_nomic_embedding(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\chromadb\\api\\client.py:147\u001b[0m, in \u001b[0;36mClient.create_collection\u001b[1;34m(self, name, configuration, metadata, embedding_function, data_loader, get_or_create)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_collection\u001b[39m(\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    145\u001b[0m     get_or_create: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    146\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Collection:\n\u001b[1;32m--> 147\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_or_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_or_create\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Collection(\n\u001b[0;32m    156\u001b[0m         client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server,\n\u001b[0;32m    157\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    158\u001b[0m         embedding_function\u001b[38;5;241m=\u001b[39membedding_function,\n\u001b[0;32m    159\u001b[0m         data_loader\u001b[38;5;241m=\u001b[39mdata_loader,\n\u001b[0;32m    160\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\chromadb\\telemetry\\opentelemetry\\__init__.py:150\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[1;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\chromadb\\api\\segment.py:103\u001b[0m, in \u001b[0;36mrate_limit.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rate_limit_enforcer\u001b[38;5;241m.\u001b[39mrate_limit(func)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\chromadb\\rate_limit\\simple_rate_limit\\__init__.py:23\u001b[0m, in \u001b[0;36mSimpleRateLimitEnforcer.rate_limit.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\chromadb\\api\\segment.py:226\u001b[0m, in \u001b[0;36mSegmentAPI.create_collection\u001b[1;34m(self, name, configuration, metadata, get_or_create, tenant, database)\u001b[0m\n\u001b[0;32m    213\u001b[0m model \u001b[38;5;241m=\u001b[39m CollectionModel(\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m,\n\u001b[0;32m    215\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m     dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    223\u001b[0m )\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# TODO: Let sysdb create the collection directly from the model\u001b[39;00m\n\u001b[1;32m--> 226\u001b[0m coll, created \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sysdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_configuration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43msegments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Passing empty till backend changes are deployed.\u001b[39;49;00m\n\u001b[0;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdimension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# This is lazily populated on the first add\u001b[39;49;00m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_or_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_or_create\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created:\n\u001b[0;32m    239\u001b[0m     segments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39mprepare_segments_for_new_collection(coll)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\chromadb\\telemetry\\opentelemetry\\__init__.py:150\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[1;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\chromadb\\db\\mixins\\sysdb.py:241\u001b[0m, in \u001b[0;36mSqlSysDB.create_collection\u001b[1;34m(self, id, name, configuration, segments, metadata, dimension, get_or_create, tenant, database)\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    235\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_collections(\n\u001b[0;32m    236\u001b[0m                 \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mcollection\u001b[38;5;241m.\u001b[39mid, tenant\u001b[38;5;241m=\u001b[39mtenant, database\u001b[38;5;241m=\u001b[39mdatabase\n\u001b[0;32m    237\u001b[0m             )[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    238\u001b[0m             \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    239\u001b[0m         )\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 241\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UniqueConstraintError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollection \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    243\u001b[0m collection \u001b[38;5;241m=\u001b[39m Collection(\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m,\n\u001b[0;32m    245\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    251\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    252\u001b[0m )\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtx() \u001b[38;5;28;01mas\u001b[39;00m cur:\n",
      "\u001b[1;31mUniqueConstraintError\u001b[0m: Collection machin2e already exists"
     ]
    }
   ],
   "source": [
    "# Step 1: PDF Loading\n",
    "import re\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "file_path = \"machine learning.pdf\"\n",
    "loader = UnstructuredPDFLoader(file_path)\n",
    "documents = loader.load()\n",
    "\n",
    "texts = []\n",
    "for document in documents:\n",
    "    text = document.page_content\n",
    "    text_without_newlines = re.sub(r'\\n+', ' ', text)\n",
    "    texts.append(text_without_newlines)\n",
    "\n",
    "# Step 2: Chroma Setup with `nomic-embed-text`\n",
    "import chromadb\n",
    "import requests\n",
    "\n",
    "OLLAMA_URL = \"http://127.0.0.1:12345\"\n",
    "\n",
    "def get_nomic_embedding(text):\n",
    "    payload = {\n",
    "        \"model\": \"nomic-embed-text\",\n",
    "        \"prompt\": text,\n",
    "        \"embed\": True\n",
    "    }\n",
    "    response = requests.post(f\"{OLLAMA_URL}/api/embeddings\", json=payload)\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"embedding\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection_name = \"machin22e\"\n",
    "collection = client.create_collection(collection_name)\n",
    "\n",
    "# Generate embeddings for each text chunk and add to Chroma collection\n",
    "embeddings = [get_nomic_embedding(text) for text in texts]\n",
    "collection.add(texts, embeddings=embeddings)\n",
    "\n",
    "# Step 3: Chunking\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Step 4: Querying Chroma DB\n",
    "def query_chroma_db(query, collection):\n",
    "    query_embedding = get_nomic_embedding(query)\n",
    "    results = collection.query(query_embedding, n_results=1)\n",
    "    print(results[\"documents\"])\n",
    "    return results[\"documents\"]\n",
    "\n",
    "query = \"What is the topic of the document?\"\n",
    "relevant_chunks = query_chroma_db(query, collection)\n",
    "\n",
    "# Step 5: Text Generation with Ollama\n",
    "def generate_ollama_response(prompt):\n",
    "    payload = {\n",
    "        \"model\": \"llama3\",\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 200\n",
    "    }\n",
    "    response = requests.post(f\"{OLLAMA_URL}/api/generate\", json=payload)\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"generated_text\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "\n",
    "full_input = \" \".join(relevant_chunks)\n",
    "generated_text = generate_ollama_response(full_input)\n",
    "\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding successful for text:  SAMRAT ABDUL JALIL AI and Back\n",
      "Embedding generation successful. Now storing in Chroma...\n",
      "Successfully stored in Chroma.\n",
      "Chunks created successfully:\n",
      "SAMRAT ABDUL JALIL AI and Backend Enthusiast\n",
      "\n",
      "SKILLS\n",
      "\n",
      "Back-End: Python, C/C++,Java ,Php, FastAPI, Ex\n",
      "WORK EXPERIENCE\n",
      "\n",
      "Undergraduate Thesis: GradeVision (in collaboration with Edusoftbd Ltd) ● Developed\n",
      "Tech Stack : Python, FastAPI, Flutter, MySQL .\n",
      "\n",
      "DL Sprint 2.0: Bengali Document Layout Analysis ● Fi\n",
      "Embedding successful for text:  What is the topic of the docum\n",
      "Querying successful. Retrieved results:\n",
      "[[None]]\n",
      "Generated text from Ollama successfully.\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 2 column 1 (char 92)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\requests\\models.py:963\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(encoding), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;66;03m# Wrong UTF codec detected; usually because it's not UTF-8\u001b[39;00m\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;66;03m# but some other 8-bit codec.  This is an RFC violation,\u001b[39;00m\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;66;03m# and the server didn't bother to tell us what codec *was*\u001b[39;00m\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;66;03m# used.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\json\\decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 92)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 94\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m relevant_chunks:\n\u001b[0;32m     93\u001b[0m     full_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(chunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m relevant_chunks])\n\u001b[1;32m---> 94\u001b[0m     generated_text \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_ollama_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Text:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28mprint\u001b[39m(generated_text)\n",
      "Cell \u001b[1;32mIn[13], line 86\u001b[0m, in \u001b[0;36mgenerate_ollama_response\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated text from Ollama successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\requests\\models.py:971\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    969\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    970\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 971\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 92)"
     ]
    }
   ],
   "source": [
    "# Step 1: PDF Loading\n",
    "import re\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "file_path = \"machine learning.pdf\"\n",
    "loader = UnstructuredPDFLoader(file_path)\n",
    "documents = loader.load()\n",
    "\n",
    "texts = []\n",
    "for document in documents:\n",
    "    text = document.page_content\n",
    "    text_without_newlines = re.sub(r'\\n+', ' ', text)\n",
    "    texts.append(text_without_newlines)\n",
    "\n",
    "# Step 2: Chroma Setup with `nomic-embed-text`\n",
    "import chromadb\n",
    "import requests\n",
    "\n",
    "OLLAMA_URL = \"http://127.0.0.1:12345\"\n",
    "\n",
    "def get_nomic_embedding(text):\n",
    "    payload = {\n",
    "        \"model\": \"nomic-embed-text\",\n",
    "        \"prompt\": text,\n",
    "        \"embed\": True\n",
    "    }\n",
    "    response = requests.post(f\"{OLLAMA_URL}/api/embeddings\", json=payload)\n",
    "    if response.status_code == 200:\n",
    "        print(\"Embedding successful for text: \", text[:30])  # Print part of the text to confirm embedding\n",
    "        return response.json().get(\"embedding\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection_name = \"machin212e\"\n",
    "collection = client.create_collection(collection_name)\n",
    "\n",
    "# Generate embeddings for each text chunk and add to Chroma collection\n",
    "embeddings = []\n",
    "for text in texts:\n",
    "    embedding = get_nomic_embedding(text)\n",
    "    if embedding:\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "# Ensure that embeddings are created successfully before adding them to Chroma\n",
    "if embeddings:\n",
    "    print(\"Embedding generation successful. Now storing in Chroma...\")\n",
    "    collection.add(texts, embeddings=embeddings)\n",
    "    print(\"Successfully stored in Chroma.\")\n",
    "else:\n",
    "    print(\"No embeddings were generated.\")\n",
    "\n",
    "# Step 3: Chunking\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Print the first few chunks to verify chunking\n",
    "print(\"Chunks created successfully:\")\n",
    "for chunk in chunks[:3]:\n",
    "    print(chunk.page_content[:100])  # Print the first 100 characters of each chunk\n",
    "\n",
    "# Step 4: Querying Chroma DB\n",
    "def query_chroma_db(query, collection):\n",
    "    query_embedding = get_nomic_embedding(query)\n",
    "    results = collection.query(query_embedding, n_results=1)\n",
    "    print(\"Querying successful. Retrieved results:\")\n",
    "    print(results[\"documents\"])  # Print the retrieved documents\n",
    "    return results[\"documents\"]\n",
    "\n",
    "query = \"What is the topic of the document?\"\n",
    "relevant_chunks = query_chroma_db(query, collection)\n",
    "\n",
    "# Step 5: Text Generation with Ollama\n",
    "def generate_ollama_response(prompt):\n",
    "    payload = {\n",
    "        \"model\": \"llama3\",\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 200\n",
    "    }\n",
    "    response = requests.post(f\"{OLLAMA_URL}/api/generate\", json=payload)\n",
    "    if response.status_code == 200:\n",
    "        print(\"Generated text from Ollama successfully.\")\n",
    "        return response.json().get(\"generated_text\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "\n",
    "# Flatten the list of relevant chunks before passing to the model\n",
    "if relevant_chunks:\n",
    "    full_input = \" \".join([str(chunk) for chunk in relevant_chunks])\n",
    "    generated_text = generate_ollama_response(full_input)\n",
    "\n",
    "    print(\"Generated Text:\")\n",
    "    print(generated_text)\n",
    "else:\n",
    "    print(\"No relevant chunks found for generation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding successful for text: SAMRAT ABDUL JALIL AI and Back...\n",
      "Embedding vector: [-0.6538950204849243, 0.2558712065219879, -2.2708866596221924, -0.2998715341091156, 0.6298578977584839, -0.6392511129379272, 0.7918146848678589, -0.08320767432451248, 0.6520728468894958, -0.5487140417098999]...\n",
      "Embedding generation successful. Now storing in Chroma...\n",
      "Successfully stored in Chroma.\n",
      "Chunks created successfully:\n",
      "SAMRAT ABDUL JALIL AI and Backend Enthusiast\n",
      "\n",
      "SKILLS\n",
      "\n",
      "Back-End: Python, C/C++,Java ,Php, FastAPI, Ex\n",
      "WORK EXPERIENCE\n",
      "\n",
      "Undergraduate Thesis: GradeVision (in collaboration with Edusoftbd Ltd) ● Developed\n",
      "Tech Stack : Python, FastAPI, Flutter, MySQL .\n",
      "\n",
      "DL Sprint 2.0: Bengali Document Layout Analysis ● Fi\n",
      "Embedding successful for text: SAMRAT ABDUL JALIL?...\n",
      "Embedding vector: [0.38792097568511963, -0.1647554337978363, -3.3390893936157227, 0.1795051395893097, 0.08914053440093994, 0.7341902852058411, -0.6030028462409973, 0.40323227643966675, -0.11605999618768692, 0.5501057505607605]...\n",
      "Query embedding generated: [0.38792097568511963, -0.1647554337978363, -3.3390893936157227, 0.1795051395893097, 0.08914053440093994, 0.7341902852058411, -0.6030028462409973, 0.40323227643966675, -0.11605999618768692, 0.5501057505607605]...\n",
      "Querying successful. Retrieved results:\n",
      "[[None]]\n",
      "Retrieved embeddings for the documents:\n",
      "Document: [None]\n",
      "Generated text from Ollama successfully.\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 2 column 1 (char 92)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\requests\\models.py:963\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(encoding), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;66;03m# Wrong UTF codec detected; usually because it's not UTF-8\u001b[39;00m\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;66;03m# but some other 8-bit codec.  This is an RFC violation,\u001b[39;00m\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;66;03m# and the server didn't bother to tell us what codec *was*\u001b[39;00m\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;66;03m# used.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\json\\decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 92)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 106\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m relevant_chunks:\n\u001b[0;32m    105\u001b[0m     full_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(chunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m relevant_chunks])\n\u001b[1;32m--> 106\u001b[0m     generated_text \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_ollama_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Text:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28mprint\u001b[39m(generated_text)\n",
      "Cell \u001b[1;32mIn[16], line 98\u001b[0m, in \u001b[0;36mgenerate_ollama_response\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated text from Ollama successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\requests\\models.py:971\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    969\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    970\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 971\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 92)"
     ]
    }
   ],
   "source": [
    "# Step 1: PDF Loading\n",
    "import re\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "file_path = \"machine learning.pdf\"\n",
    "loader = UnstructuredPDFLoader(file_path)\n",
    "documents = loader.load()\n",
    "\n",
    "texts = []\n",
    "for document in documents:\n",
    "    text = document.page_content\n",
    "    text_without_newlines = re.sub(r'\\n+', ' ', text)\n",
    "    texts.append(text_without_newlines)\n",
    "\n",
    "# Step 2: Chroma Setup with `nomic-embed-text`\n",
    "import chromadb\n",
    "import requests\n",
    "\n",
    "OLLAMA_URL = \"http://127.0.0.1:12345\"\n",
    "\n",
    "def get_nomic_embedding(text):\n",
    "    payload = {\n",
    "        \"model\": \"nomic-embed-text\",\n",
    "        \"prompt\": text,\n",
    "        \"embed\": True\n",
    "    }\n",
    "    response = requests.post(f\"{OLLAMA_URL}/api/embeddings\", json=payload)\n",
    "    if response.status_code == 200:\n",
    "        embedding = response.json().get(\"embedding\")\n",
    "        print(f\"Embedding successful for text: {text[:30]}...\")  # Print part of the text to confirm embedding\n",
    "        print(f\"Embedding vector: {embedding[:10]}...\")  # Print the first 10 values of the embedding vector\n",
    "        return embedding\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection_name = \"machin21142e\"\n",
    "collection = client.create_collection(collection_name)\n",
    "\n",
    "# Generate embeddings for each text chunk and add to Chroma collection\n",
    "embeddings = []\n",
    "for text in texts:\n",
    "    embedding = get_nomic_embedding(text)\n",
    "    if embedding:\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "# Ensure that embeddings are created successfully before adding them to Chroma\n",
    "if embeddings:\n",
    "    print(\"Embedding generation successful. Now storing in Chroma...\")\n",
    "    collection.add(texts, embeddings=embeddings)\n",
    "    print(\"Successfully stored in Chroma.\")\n",
    "else:\n",
    "    print(\"No embeddings were generated.\")\n",
    "\n",
    "# Step 3: Chunking\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Print the first few chunks to verify chunking\n",
    "print(\"Chunks created successfully:\")\n",
    "for chunk in chunks[:3]:\n",
    "    print(chunk.page_content[:100])  # Print the first 100 characters of each chunk\n",
    "\n",
    "# Step 4: Querying Chroma DB\n",
    "def query_chroma_db(query, collection):\n",
    "    query_embedding = get_nomic_embedding(query)\n",
    "    if query_embedding is None:\n",
    "        print(\"Failed to generate embedding for the query.\")\n",
    "        return []\n",
    "    print(f\"Query embedding generated: {query_embedding[:10]}...\")  # Print first 10 values of query embedding\n",
    "    results = collection.query(query_embedding, n_results=1)\n",
    "    \n",
    "    # Debug: Print the retrieved documents and their embeddings\n",
    "    print(\"Querying successful. Retrieved results:\")\n",
    "    print(results[\"documents\"])  # Print the retrieved documents\n",
    "    print(\"Retrieved embeddings for the documents:\")\n",
    "    for doc in results[\"documents\"]:\n",
    "        print(f\"Document: {doc}\")\n",
    "        # You can check the embedding or content in the document as needed\n",
    "    return results[\"documents\"]\n",
    "\n",
    "query = \"SAMRAT ABDUL JALIL?\"\n",
    "relevant_chunks = query_chroma_db(query, collection)\n",
    "\n",
    "# Step 5: Text Generation with Ollama\n",
    "def generate_ollama_response(prompt):\n",
    "    payload = {\n",
    "        \"model\": \"llama3\",\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 200\n",
    "    }\n",
    "    response = requests.post(f\"{OLLAMA_URL}/api/generate\", json=payload)\n",
    "    if response.status_code == 200:\n",
    "        print(\"Generated text from Ollama successfully.\")\n",
    "        return response.json().get(\"generated_text\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "\n",
    "# Flatten the list of relevant chunks before passing to the model\n",
    "if relevant_chunks:\n",
    "    full_input = \" \".join([str(chunk) for chunk in relevant_chunks])\n",
    "    generated_text = generate_ollama_response(full_input)\n",
    "\n",
    "    print(\"Generated Text:\")\n",
    "    print(generated_text)\n",
    "else:\n",
    "    print(\"No relevant chunks found for generation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
