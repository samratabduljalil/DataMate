{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Using cached langchain-0.3.13-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Using cached SQLAlchemy-2.0.36-cp310-cp310-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Using cached aiohttp-3.11.11-cp310-cp310-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting langchain-core<0.4.0,>=0.3.26 (from langchain)\n",
      "  Using cached langchain_core-0.3.28-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.3 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.4-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.3,>=0.1.17 (from langchain)\n",
      "  Using cached langsmith-0.2.6-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy<2,>=1.22.4 (from langchain)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl.metadata (61 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Using cached pydantic-2.10.4-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain)\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached frozenlist-1.5.0-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached multidict-6.1.0-cp310-cp310-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached propcache-0.2.1-cp310-cp310-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached yarl-1.18.3-cp310-cp310-win_amd64.whl.metadata (71 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.26->langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.26->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.26->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langsmith<0.3,>=0.1.17->langchain) (0.27.0)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.3,>=0.1.17->langchain)\n",
      "  Using cached orjson-3.10.12-cp310-none-win_amd64.whl.metadata (42 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.3,>=0.1.17->langchain)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached pydantic_core-2.27.2-cp310-cp310-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Using cached greenlet-3.1.1-cp310-cp310-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (4.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.26->langchain)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.0)\n",
      "Using cached langchain-0.3.13-py3-none-any.whl (1.0 MB)\n",
      "Using cached aiohttp-3.11.11-cp310-cp310-win_amd64.whl (442 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached langchain_core-0.3.28-py3-none-any.whl (411 kB)\n",
      "Using cached langchain_text_splitters-0.3.4-py3-none-any.whl (27 kB)\n",
      "Using cached langsmith-0.2.6-py3-none-any.whl (325 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "Using cached pydantic-2.10.4-py3-none-any.whl (431 kB)\n",
      "Using cached pydantic_core-2.27.2-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "Using cached SQLAlchemy-2.0.36-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached frozenlist-1.5.0-cp310-cp310-win_amd64.whl (51 kB)\n",
      "Using cached greenlet-3.1.1-cp310-cp310-win_amd64.whl (298 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached multidict-6.1.0-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Using cached orjson-3.10.12-cp310-none-win_amd64.whl (135 kB)\n",
      "Using cached propcache-0.2.1-cp310-cp310-win_amd64.whl (44 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached yarl-1.18.3-cp310-cp310-win_amd64.whl (90 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Installing collected packages: tenacity, pydantic-core, propcache, orjson, numpy, multidict, jsonpointer, greenlet, frozenlist, async-timeout, annotated-types, aiohappyeyeballs, yarl, SQLAlchemy, requests-toolbelt, pydantic, jsonpatch, aiosignal, langsmith, aiohttp, langchain-core, langchain-text-splitters, langchain\n",
      "Successfully installed SQLAlchemy-2.0.36 aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 annotated-types-0.7.0 async-timeout-4.0.3 frozenlist-1.5.0 greenlet-3.1.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.13 langchain-core-0.3.28 langchain-text-splitters-0.3.4 langsmith-0.2.6 multidict-6.1.0 numpy-1.26.4 orjson-3.10.12 propcache-0.2.1 pydantic-2.10.4 pydantic-core-2.27.2 requests-toolbelt-1.0.0 tenacity-9.0.0 yarl-1.18.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Optional\n",
    "import requests\n",
    "\n",
    "class OllamaLLM(LLM):\n",
    "    model_name: str  # Define model_name as a class attribute\n",
    "    server_url: str = \"http://localhost:11434\"  # Default server URL\n",
    "\n",
    "    def __init__(self, model_name: str, server_url: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        if server_url:\n",
    "            self.server_url = server_url\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ollama\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: list = None) -> str:\n",
    "        # Make a request to the Ollama server\n",
    "        response = requests.post(\n",
    "            f\"{self.server_url}/api/generate\",\n",
    "            json={\"model\": self.model_name, \"prompt\": prompt},\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result.get(\"response\", \"\")\n",
    "        else:\n",
    "            raise ValueError(f\"Error from Ollama server: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OllamaLLM\nmodel_name\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ollama_llm \u001b[38;5;241m=\u001b[39m \u001b[43mOllamaLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama2-7b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Test with a simple prompt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain quantum mechanics in simple terms.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m, in \u001b[0;36mOllamaLLM.__init__\u001b[1;34m(self, model_name, server_url)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name: \u001b[38;5;28mstr\u001b[39m, server_url: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m model_name\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m server_url:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\langchain_core\\load\\serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\pydantic\\main.py:214\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    213\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    216\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    220\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    221\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for OllamaLLM\nmodel_name\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "ollama_llm = OllamaLLM(model_name=\"llama2-7b\")\n",
    "\n",
    "# Test with a simple prompt\n",
    "prompt = \"Explain quantum mechanics in simple terms.\"\n",
    "response = ollama_llm(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samrat Abdul Jalil is a Malaysian professional squash player. He was born on August 20, 1982, in Kuala Lumpur, Malaysia.\n",
      "\n",
      "Abdul Jalil has had a successful career in squash, representing Malaysia at the international level. Some of his notable achievements include:\n",
      "\n",
      "1. Asian Games Medalist: He won a silver medal at the 2006 Asian Games in Doha, Qatar, in the men's individual event.\n",
      "2. Commonwealth Games Participant: Abdul Jalil has represented Malaysia at three Commonwealth Games (2002, 2006, and 2010).\n",
      "3. South East Asian Games Champion: He won multiple titles at the South East Asian Games, a regional multi-sport event.\n",
      "\n",
      "Abdul Jalil is considered one of Malaysia's most successful squash players in recent history.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "# Connect to the Ollama server\n",
    "ollama_llm = Ollama(\n",
    "    model=\"llama3\",\n",
    "    base_url=\"http://127.0.0.1:12345\"\n",
    ")\n",
    "\n",
    "# Test with a prompt\n",
    "prompt = \"who is samrat abdul jalil\"\n",
    "response = ollama_llm(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Using cached langchain_community-0.3.13-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain-community) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain-community) (3.11.11)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.13 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain-community) (0.3.13)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain-community) (0.3.28)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain-community) (0.2.6)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Using cached pydantic_settings-2.7.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached marshmallow-3.23.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain<0.4.0,>=0.3.13->langchain-community) (0.3.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain<0.4.0,>=0.3.13->langchain-community) (2.10.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (1.0.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from requests<3,>=2->langchain-community) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (4.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.13->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.13->langchain-community) (2.27.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.2.0)\n",
      "Using cached langchain_community-0.3.13-py3-none-any.whl (2.5 MB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached pydantic_settings-2.7.0-py3-none-any.whl (29 kB)\n",
      "Using cached marshmallow-3.23.2-py3-none-any.whl (49 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
      "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.13 marshmallow-3.23.2 mypy-extensions-1.0.0 pydantic-settings-2.7.0 python-dotenv-1.0.1 typing-inspect-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nomic\n",
      "  Downloading nomic-3.3.4.tar.gz (49 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting click (from nomic)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting jsonlines (from nomic)\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting loguru (from nomic)\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting rich (from nomic)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from nomic) (2.32.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from nomic) (1.26.4)\n",
      "Collecting pandas (from nomic)\n",
      "  Using cached pandas-2.2.3-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: pydantic in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from nomic) (2.10.4)\n",
      "Collecting tqdm (from nomic)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting pyarrow (from nomic)\n",
      "  Downloading pyarrow-18.1.0-cp310-cp310-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting pillow (from nomic)\n",
      "  Using cached pillow-11.0.0-cp310-cp310-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyjwt (from nomic)\n",
      "  Downloading PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from click->nomic) (0.4.6)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from jsonlines->nomic) (24.3.0)\n",
      "Collecting win32-setctime>=1.0.0 (from loguru->nomic)\n",
      "  Downloading win32_setctime-1.2.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from pandas->nomic) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from pandas->nomic) (2024.1)\n",
      "Collecting tzdata>=2022.7 (from pandas->nomic)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from pydantic->nomic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from pydantic->nomic) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from pydantic->nomic) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from requests->nomic) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from requests->nomic) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from requests->nomic) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from requests->nomic) (2024.12.14)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->nomic)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from rich->nomic) (2.15.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->nomic)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->nomic) (1.16.0)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "Using cached pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "Using cached pillow-11.0.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "Downloading pyarrow-18.1.0-cp310-cp310-win_amd64.whl (25.1 MB)\n",
      "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/25.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/25.1 MB 3.0 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 1.3/25.1 MB 2.9 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 1.8/25.1 MB 2.5 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 2.1/25.1 MB 2.3 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 2.6/25.1 MB 2.2 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.9/25.1 MB 2.2 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.4/25.1 MB 2.1 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.7/25.1 MB 2.1 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.2/25.1 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.5/25.1 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 5.0/25.1 MB 2.0 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 5.2/25.1 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.5/25.1 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 6.0/25.1 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.3/25.1 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.8/25.1 MB 2.0 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 7.1/25.1 MB 1.9 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 7.6/25.1 MB 1.9 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 7.9/25.1 MB 1.9 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.4/25.1 MB 1.9 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.7/25.1 MB 1.9 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 8.9/25.1 MB 1.9 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 9.4/25.1 MB 1.9 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 9.7/25.1 MB 1.9 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 10.2/25.1 MB 1.9 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 10.7/25.1 MB 1.9 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 11.0/25.1 MB 1.9 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 11.5/25.1 MB 1.9 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 11.8/25.1 MB 1.9 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 12.3/25.1 MB 1.9 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.6/25.1 MB 1.9 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.8/25.1 MB 1.9 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 13.4/25.1 MB 1.9 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 13.6/25.1 MB 1.9 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 14.2/25.1 MB 1.9 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 14.4/25.1 MB 1.9 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.9/25.1 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 15.2/25.1 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 15.7/25.1 MB 1.9 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 16.0/25.1 MB 1.9 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.5/25.1 MB 1.9 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.8/25.1 MB 1.9 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 17.3/25.1 MB 1.9 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 17.6/25.1 MB 1.9 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 18.1/25.1 MB 1.9 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.4/25.1 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.9/25.1 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 19.1/25.1 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 19.7/25.1 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.9/25.1 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.2/25.1 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.7/25.1 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 21.0/25.1 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 21.5/25.1 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.0/25.1 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.3/25.1 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.8/25.1 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 23.1/25.1 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 23.6/25.1 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.9/25.1 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.1/25.1 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/25.1 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.1 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.1/25.1 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Downloading win32_setctime-1.2.0-py3-none-any.whl (4.1 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: nomic\n",
      "  Building wheel for nomic (pyproject.toml): started\n",
      "  Building wheel for nomic (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for nomic: filename=nomic-3.3.4-py3-none-any.whl size=49635 sha256=f169ff72289a2fbab58dc823da311eee0ab2806c6a8ed974dfc9900fcba3a2c5\n",
      "  Stored in directory: c:\\users\\samra\\appdata\\local\\pip\\cache\\wheels\\c8\\1d\\bb\\b2f4390b876d28324d00cfbddc4fda198c1c448a27d972b8e4\n",
      "Successfully built nomic\n",
      "Installing collected packages: win32-setctime, tzdata, tqdm, pyjwt, pyarrow, pillow, mdurl, jsonlines, click, pandas, markdown-it-py, loguru, rich, nomic\n",
      "Successfully installed click-8.1.8 jsonlines-4.0.0 loguru-0.7.3 markdown-it-py-3.0.0 mdurl-0.1.2 nomic-3.3.4 pandas-2.2.3 pillow-11.0.0 pyarrow-18.1.0 pyjwt-2.10.1 rich-13.9.4 tqdm-4.67.1 tzdata-2024.2 win32-setctime-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nomic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'nomic.atlas' has no attribute 'EmbeddingCollection'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 24\u001b[0m\n\u001b[0;32m     17\u001b[0m texts \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is quantum mechanics?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe theory of relativity is fundamental to physics.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMachine learning is a subset of AI.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m ]\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Generate Nomic embeddings\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m embedding_collection \u001b[38;5;241m=\u001b[39m \u001b[43msetup_nomic_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Step 2: Store embeddings in Chroma\u001b[39;00m\n\u001b[0;32m     27\u001b[0m documents \u001b[38;5;241m=\u001b[39m [Document(page_content\u001b[38;5;241m=\u001b[39mtext) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m, in \u001b[0;36msetup_nomic_embeddings\u001b[1;34m(texts, embedding_collection_name)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_nomic_embeddings\u001b[39m(texts, embedding_collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnomic_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate embeddings locally using Nomic and store them in an embedding collection.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     collection \u001b[38;5;241m=\u001b[39m \u001b[43matlas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCollection\u001b[49m(name\u001b[38;5;241m=\u001b[39membedding_collection_name)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[0;32m     12\u001b[0m         collection\u001b[38;5;241m.\u001b[39madd_entry(text)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'nomic.atlas' has no attribute 'EmbeddingCollection'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nomic import atlas\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Step 1: Set up Nomic Atlas Embedding Collection\n",
    "def setup_nomic_embeddings(texts, embedding_collection_name=\"nomic_embeddings\"):\n",
    "    \"\"\"Generate embeddings locally using Nomic and store them in an embedding collection.\"\"\"\n",
    "    collection = atlas.EmbeddingCollection(name=embedding_collection_name)\n",
    "    for text in texts:\n",
    "        collection.add_entry(text)\n",
    "    collection.wait_for_completion()  # Wait until the embeddings are computed\n",
    "    return collection\n",
    "\n",
    "# Example texts\n",
    "texts = [\n",
    "    \"What is quantum mechanics?\",\n",
    "    \"The theory of relativity is fundamental to physics.\",\n",
    "    \"Machine learning is a subset of AI.\"\n",
    "]\n",
    "\n",
    "# Generate Nomic embeddings\n",
    "embedding_collection = setup_nomic_embeddings(texts)\n",
    "\n",
    "# Step 2: Store embeddings in Chroma\n",
    "documents = [Document(page_content=text) for text in texts]\n",
    "\n",
    "# Assuming you have embeddings locally via LangChain-compatible Embeddings class\n",
    "class NomicEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts):\n",
    "        return embedding_collection.get_embeddings(texts)\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return embedding_collection.get_embedding(text)\n",
    "\n",
    "# Initialize Nomic embeddings\n",
    "nomic_embeddings = NomicEmbeddings()\n",
    "\n",
    "# Create a Chroma vector store\n",
    "vector_store = Chroma.from_documents(documents, nomic_embeddings)\n",
    "\n",
    "# Step 3: Query the Vector Store\n",
    "query = \"Explain the basics of physics.\"\n",
    "similar_docs = vector_store.similarity_search(query)\n",
    "\n",
    "# Print results\n",
    "for i, doc in enumerate(similar_docs, 1):\n",
    "    print(f\"Document {i}: {doc.page_content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'nomic.atlas' has no attribute 'EmbeddingCollection'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Generate embeddings\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating embeddings...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m collection \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Display the embeddings\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEmbeddings Generated:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m, in \u001b[0;36mgenerate_embeddings\u001b[1;34m(texts, embedding_collection_name)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mGenerates embeddings for a list of texts using Nomic and returns the collection.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Create a new embedding collection\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m collection \u001b[38;5;241m=\u001b[39m \u001b[43matlas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCollection\u001b[49m(name\u001b[38;5;241m=\u001b[39membedding_collection_name)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Add texts to the collection for embedding\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'nomic.atlas' has no attribute 'EmbeddingCollection'"
     ]
    }
   ],
   "source": [
    "from nomic import atlas\n",
    "\n",
    "# Initialize an Embedding Collection\n",
    "def generate_embeddings(texts, embedding_collection_name=\"test_embeddings\"):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a list of texts using Nomic and returns the collection.\n",
    "    \"\"\"\n",
    "    # Create a new embedding collection\n",
    "    collection = atlas.EmbeddingCollection(name=embedding_collection_name)\n",
    "    \n",
    "    # Add texts to the collection for embedding\n",
    "    for text in texts:\n",
    "        collection.add_entry(text)\n",
    "    \n",
    "    # Wait until the embeddings are computed\n",
    "    collection.wait_for_completion()\n",
    "    \n",
    "    return collection\n",
    "\n",
    "# Sample texts to embed\n",
    "texts = [\n",
    "    \"Artificial intelligence is transforming industries.\",\n",
    "    \"Quantum mechanics is the study of very small particles.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"The universe is vast and full of mysteries.\"\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "print(\"Generating embeddings...\")\n",
    "collection = generate_embeddings(texts)\n",
    "\n",
    "# Display the embeddings\n",
    "print(\"\\nEmbeddings Generated:\")\n",
    "for i, text in enumerate(texts):\n",
    "    embedding = collection.get_embedding(text)  # Retrieve embedding for the text\n",
    "    print(f\"Text {i + 1}: {text}\")\n",
    "    print(f\"Embedding {i + 1}: {embedding[:5]}... (truncated for display)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AtlasProject' from 'nomic' (C:\\Users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\nomic\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnomic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AtlasProject\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize an Atlas project\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_embeddings\u001b[39m(texts, project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Embedding Project\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'AtlasProject' from 'nomic' (C:\\Users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\nomic\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from nomic import AtlasProject\n",
    "\n",
    "# Initialize an Atlas project\n",
    "def generate_embeddings(texts, project_name=\"Test Embedding Project\"):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a list of texts using Nomic and uploads them to an Atlas project.\n",
    "    \"\"\"\n",
    "    # Create or open an Atlas project\n",
    "    project = AtlasProject(name=project_name)\n",
    "\n",
    "    # Add data to the project with embeddings\n",
    "    data = [{'text': text} for text in texts]\n",
    "    project.add_text(data=data, name=\"Sample Text Embeddings\")\n",
    "\n",
    "    # Optionally, visualize the project\n",
    "    print(f\"View your project here: {project.url}\")\n",
    "    return project\n",
    "\n",
    "# Sample texts to embed\n",
    "texts = [\n",
    "    \"Artificial intelligence is transforming industries.\",\n",
    "    \"Quantum mechanics is the study of very small particles.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"The universe is vast and full of mysteries.\"\n",
    "]\n",
    "\n",
    "# Generate embeddings and upload to Atlas\n",
    "print(\"Generating embeddings...\")\n",
    "project = generate_embeddings(texts)\n",
    "\n",
    "# Output the project URL\n",
    "print(\"\\nEmbeddings uploaded successfully!\")\n",
    "print(f\"Project URL: {project.url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Generated:\n",
      "[-0.29197952151298523, 1.27434504032135, -3.2297379970550537, 0.049403682351112366, 0.19612421095371246, 0.7218914031982422, -0.4212694466114044, -0.24382901191711426, 0.9706432819366455, -1.4466849565505981, -0.565799355506897, 1.041719675064087, 0.9503485560417175, 0.08554767072200775, 0.33005398511886597, -0.3670889139175415, 1.4649914503097534, -0.9208197593688965, 1.7554038763046265, -0.8139787912368774, 0.07750110328197479, 0.881016194820404, 0.937093198299408, -0.4547045826911926, 1.852592945098877, 0.47455093264579773, -1.3073588609695435, 0.9793509244918823, -0.7587403059005737, 1.9731777906417847, 0.18370240926742554, 0.016278110444545746, -0.5300453901290894, -0.8587548732757568, -0.4014658033847809, -1.377687692642212, 1.1880080699920654, -0.5455273389816284, 1.360038161277771, 0.5207986235618591, -0.32429394125938416, 0.3409191966056824, -0.005868557840585709, -0.3421463966369629, 1.6914187669754028, -1.0156300067901611, 1.023075819015503, 0.6021881103515625, -0.2366887927055359, -0.18817958235740662, -0.939825713634491, 0.7778087258338928, -0.8715156316757202, -0.7814778089523315, 1.4608560800552368, 1.2932405471801758, -0.2835470736026764, -0.4255557954311371, -0.5405891537666321, -0.22216510772705078, 0.13023072481155396, -0.10476274788379669, -0.6779084801673889, -0.1739957332611084, -0.17000269889831543, -1.100224256515503, 0.3831609785556793, 0.8374190330505371, -0.7473664879798889, -1.126997947692871, 0.03692389279603958, -0.467280775308609, -0.08052704483270645, 0.28157731890678406, -1.5558466911315918, -0.11241814494132996, -0.6362879276275635, 0.3941761255264282, -0.21134522557258606, 0.27935636043548584, 1.0897809267044067, -0.20174001157283783, -0.2967815399169922, -0.009089415892958641, 0.9505264759063721, 0.2731103301048279, 0.1529848724603653, 0.49324724078178406, -0.30038735270500183, 0.9455274343490601, 0.882043719291687, -0.21387827396392822, -0.0416121743619442, 0.5977675318717957, -1.5210624933242798, -0.10003066807985306, -0.43012553453445435, 0.5757986903190613, -0.6526575684547424, -1.4385960102081299, -1.1107858419418335, 0.5029064416885376, -0.31738215684890747, -0.9774829745292664, 0.5361185073852539, 1.0240246057510376, -0.0900987833738327, 0.49304062128067017, -1.1929658651351929, 0.11205293238162994, -0.5288537740707397, -0.06248386204242706, 0.7836841940879822, -0.5396804809570312, 0.5391288995742798, -0.8566330075263977, 0.4327050447463989, -0.7494385838508606, 0.0984734371304512, 0.9015607833862305, -0.9495140910148621, 0.23325969278812408, 0.5511248111724854, 0.31916284561157227, -0.010339108295738697, 1.0964877605438232, -0.5329288840293884, 0.7592070698738098, -0.4251991808414459, 0.03404609113931656, 0.5413931012153625, -0.20852404832839966, -0.6161842346191406, -0.5153748393058777, 0.6346502900123596, 0.4236162304878235, -0.1878005862236023, -0.5443505644798279, 0.07236649841070175, -0.28854936361312866, 0.658758282661438, 1.0325284004211426, 0.5776519775390625, -0.911504328250885, -1.037488341331482, -1.2223334312438965, 1.1804299354553223, -0.906796395778656, -0.11758842319250107, -0.038594771176576614, -0.515731930732727, -0.2648751735687256, 0.04841490834951401, 0.22790953516960144, 0.6621187329292297, -0.44730281829833984, 0.8774786591529846, 0.7499642372131348, 0.23615774512290955, -0.2319682240486145, 0.4925718903541565, -0.977434515953064, -0.22944970428943634, 0.6094051003456116, -0.6661196947097778, 0.22133541107177734, 0.7002052664756775, 0.5438463687896729, 0.6367184519767761, 1.2578543424606323, -0.4772367775440216, -1.2413538694381714, 0.30806663632392883, -0.045134834945201874, -0.7007416486740112, -0.7454420328140259, 1.1819244623184204, -1.5258355140686035, -0.7425746917724609, -1.0659103393554688, 0.46950122714042664, -0.1918213963508606, 0.18840311467647552, 0.7266859412193298, 0.11116435378789902, -1.4212961196899414, -0.21116675436496735, -0.4337024390697479, -1.5354194641113281, -1.2919987440109253, -0.5372464656829834, 0.11236123740673065, -0.23003710806369781, -1.534735083580017, -1.3840525150299072, -0.11877728253602982, 1.1413321495056152, 1.1278942823410034, 0.9308663606643677, -0.3134734630584717, 1.1130262613296509, -1.4391897916793823, -0.9624934196472168, 0.12846052646636963, -1.0948814153671265, 0.7095270156860352, -0.7373949289321899, 0.8756555914878845, -0.8167074918746948, 0.39453116059303284, 1.4730912446975708, 0.739820122718811, -1.3250353336334229, 0.6708458065986633, 0.022229518741369247, 0.048109643161296844, -1.4634547233581543, 0.5075624585151672, -0.855196475982666, 0.19359464943408966, 1.830521821975708, 0.3735833168029785, -0.2993663549423218, -1.3816174268722534, 0.41332703828811646, 0.5353366136550903, -0.8320764899253845, -1.0554025173187256, -0.4680856466293335, 0.6405994892120361, 0.14628127217292786, -0.7740312814712524, 0.8157875537872314, 0.805175244808197, 0.12236292660236359, 1.0427144765853882, 0.8937469124794006, 1.1554741859436035, 0.3702707886695862, 0.2860826849937439, -0.03221585601568222, 0.8590084314346313, -0.47902539372444153, 0.5750424265861511, -1.032762050628662, 0.11127668619155884, -0.35031089186668396, -0.8913814425468445, -0.42564523220062256, 0.26474729180336, -0.22729451954364777, -0.2395252287387848, 1.3513025045394897, -0.23645219206809998, 0.5966256260871887, -1.0842695236206055, -0.6754068732261658, -1.0760325193405151, -0.7383678555488586, -0.3379848301410675, 0.24960872530937195, -0.4218267798423767, 0.2231770157814026, -0.5302796959877014, -0.7913974523544312, -1.1933190822601318, 0.143130823969841, -0.034155990928411484, 0.14403682947158813, -0.8540143966674805, 0.2722086012363434, -0.15891027450561523, 0.11427102982997894, 1.095110535621643, -0.17662857472896576, -0.056532032787799835, -0.4057529866695404, -0.4394131898880005, -0.04805731773376465, 0.9606565833091736, 0.2558898329734802, 0.09180180728435516, -0.21954287588596344, -0.03737996146082878, 0.5206973552703857, 0.09323544055223465, 0.9430612325668335, 0.22987185418605804, -0.9627905488014221, -0.8108236789703369, 0.20928354561328888, 0.6494125127792358, -0.2742415964603424, 2.290024757385254, -0.22168277204036713, 0.6941782832145691, 1.2123481035232544, 0.06163329631090164, 0.15075653791427612, 0.3903823792934418, 0.7385947108268738, -0.6948657035827637, 0.012846332043409348, 1.0584015846252441, 0.8770491480827332, -0.779617428779602, 2.18770432472229, 0.18813079595565796, -0.10793325304985046, -0.7655891180038452, -0.38294172286987305, 1.1414886713027954, -1.1308056116104126, 0.1571539044380188, -0.8617007732391357, -0.7692755460739136, 0.27312415838241577, -0.03856819495558739, 1.5277249813079834, -0.6260300874710083, 0.4204169511795044, -0.5620664954185486, -1.794262409210205, -0.5477975606918335, -0.2479371875524521, 0.6096433401107788, -0.5405647158622742, 1.1150662899017334, 0.4164735972881317, -0.2319364994764328, 0.6002006530761719, 1.6523429155349731, -0.2665421962738037, -0.5890309810638428, -0.8788667917251587, 1.0279895067214966, 0.3398876190185547, 0.9533271193504333, -0.2304907590150833, 0.9911532402038574, 0.30950912833213806, -0.5973490476608276, 0.3025408387184143, -1.0125362873077393, -0.5524846911430359, 0.018785033375024796, -0.2570090591907501, -0.4403766095638275, -0.07653652876615524, -1.0539690256118774, -0.929511308670044, 0.041899874806404114, -0.7273908257484436, 0.4174743592739105, -0.013274223543703556, 0.3033663332462311, -0.29410678148269653, 0.13672009110450745, -0.07978133857250214, 0.599102258682251, 0.7698108553886414, -0.19474919140338898, 0.5720739364624023, 0.7493818998336792, -0.26761093735694885, 0.5539870858192444, -0.7467843294143677, 0.7370584011077881, 1.123777151107788, 0.7380090951919556, -0.3382514417171478, -0.15257367491722107, 0.5383551716804504, 0.6254680752754211, 0.4413626194000244, 0.893889844417572, -0.058454494923353195, -0.8763105869293213, 0.6169967651367188, -0.9059776067733765, -0.015439314767718315, -0.26941609382629395, 0.25817981362342834, 0.41290706396102905, 1.3378938436508179, 0.301982045173645, -0.0909188836812973, -0.39445409178733826, 0.3581119179725647, 1.556794285774231, -0.008194578811526299, -0.8207228779792786, -1.2053232192993164, -0.18273192644119263, 0.6902364492416382, 1.2269092798233032, 0.5622899532318115, -0.6519564390182495, -0.5108059644699097, 0.5448848009109497, -0.7631531953811646, -0.05778626352548599, -0.36636459827423096, -0.5635920166969299, -1.180112361907959, 1.036309838294983, -0.267887681722641, -1.3448222875595093, -0.1604120433330536, -0.21366946399211884, -0.12101142108440399, 0.9383553862571716, -0.7983403205871582, -0.6461501121520996, -1.865351676940918, 0.4646994173526764, 0.43638306856155396, 0.4898512065410614, -1.1815049648284912, 0.010988660156726837, 0.2788071036338806, 0.43780288100242615, -0.3071547746658325, -0.372507244348526, -0.598792552947998, -0.42145469784736633, 1.074891448020935, 1.0528755187988281, 0.0933733657002449, -0.7670060992240906, 0.2207317352294922, 1.1721705198287964, 0.9218027591705322, 0.0885932520031929, 0.04525943845510483, 0.5199176669120789, -0.6480259299278259, 0.9729795455932617, -0.10095168650150299, 1.8632258176803589, 0.1347767412662506, -0.7401108741760254, -0.21575488150119781, 0.15220223367214203, -0.013808093965053558, 0.670606255531311, 1.9655601978302002, -0.9824840426445007, -0.777992308139801, 0.20970681309700012, 0.8429826498031616, 1.1247681379318237, 0.264456182718277, 1.370848298072815, 1.2192026376724243, -0.28933313488960266, -1.3926342725753784, 0.37748783826828003, 0.2668359875679016, 0.834693193435669, 1.3284844160079956, 0.37792685627937317, -1.093821406364441, -0.036519527435302734, 0.15919138491153717, -0.3181854486465454, 0.4146031141281128, 0.11102059483528137, 0.29739609360694885, 0.5467884540557861, -0.09273246675729752, 0.8158641457557678, -0.323591947555542, 0.7262466549873352, -0.15530705451965332, -0.49222972989082336, -0.4276096820831299, 0.8962469100952148, 1.3467543125152588, 0.6054179072380066, -0.03649704158306122, -1.007293462753296, -1.2572156190872192, -1.2470935583114624, 0.20174583792686462, 1.0992828607559204, 0.728571891784668, -0.4105278551578522, 0.8443732857704163, -0.24137070775032043, 0.5465595126152039, 0.7062829732894897, 0.25371310114860535, -0.6362028121948242, -0.18552066385746002, 0.0607607439160347, -0.12106222659349442, 0.41464459896087646, -0.9332518577575684, 0.18513143062591553, 0.4615422189235687, 1.2236974239349365, -0.7369654774665833, 0.20970112085342407, 0.9027175307273865, -0.027899840846657753, -0.20804791152477264, -1.0746698379516602, -0.6516050696372986, 0.9223793148994446, -1.3540600538253784, 0.16193826496601105, 0.4210520088672638, -0.41137784719467163, 0.7474482655525208, -0.9440703988075256, 0.3008512556552887, -0.5822439789772034, -0.5313451290130615, -0.15431243181228638, 0.41971927881240845, -1.036162257194519, -0.6786850690841675, 0.34884190559387207, -0.18186385929584503, 0.19554166495800018, -0.7067260146141052, -0.6387649178504944, -0.04632168635725975, -0.2565973401069641, -0.046959538012742996, -0.8303349614143372, -0.3186459243297577, 0.04832347854971886, -0.3132302761077881, 0.8047810792922974, -1.2405946254730225, 0.3149620592594147, -0.19127626717090607, 0.13711971044540405, -0.03757976368069649, 0.7280561327934265, -0.19293147325515747, 0.2602125406265259, -0.26917538046836853, -0.3128855526447296, -0.31716373562812805, 0.6033657789230347, -0.10760696232318878, -1.2435649633407593, 0.9428441524505615, 0.027052879333496094, -0.8174108862876892, -0.3076488673686981, -0.19868887960910797, -1.1001038551330566, -0.21188277006149292, 0.6544549465179443, -0.5607208609580994, -0.3430567681789398, 0.25141772627830505, 0.41947826743125916, 0.6552987098693848, -0.35569074749946594, 0.37040939927101135, 0.052851397544145584, -0.12138938158750534, 0.9610031843185425, 0.46012890338897705, 0.07161986827850342, 0.43750399351119995, -0.4701102077960968, -0.5220722556114197, 0.19654318690299988, 0.032872363924980164, -0.12562090158462524, 0.3607043921947479, 0.5823103785514832, -1.6250009536743164, -0.6810791492462158, 0.07541053742170334, -0.12919335067272186, -0.8803033232688904, 1.2235506772994995, -1.0698412656784058, 0.23147161304950714, -0.6763591170310974, 0.17455190420150757, 1.4080859422683716, -1.144381046295166, -0.7008707523345947, -0.1704602986574173, -0.24767965078353882, -0.9654209613800049, -0.17669257521629333, -0.7637926340103149, -1.5473895072937012, -1.3147876262664795, -0.6714938879013062, -0.21452313661575317, 0.562924861907959, -0.5590213537216187, 2.2857720851898193, -0.9361194372177124, -0.28731775283813477, 1.1733943223953247, -0.2201332151889801, -0.05755094438791275, -0.6918628811836243, -0.6130871772766113, 0.8515833020210266, 0.6234744191169739, 1.145529866218567, -0.09104295074939728, -0.1792299747467041, -0.029883883893489838, 1.1864211559295654, -0.5883612036705017, -0.050085581839084625, -0.3792967200279236, -1.278565526008606, -2.081597328186035, 0.4521316587924957, -1.163957953453064, 0.9035536050796509, -0.5784232020378113, -0.33802929520606995, -1.031806230545044, 0.2743982672691345, 0.5177639126777649, -0.19756920635700226, 1.0024020671844482, -1.0988153219223022, -1.1224998235702515, -0.3810895085334778, -0.009778368286788464, -1.874314546585083, 0.02830367721617222, -0.19230936467647552, 1.5780874490737915, 1.4911001920700073, -0.06242777407169342, -0.24558256566524506, 0.1390087902545929, 0.8076659440994263, 0.00742852920666337, -0.0858713760972023, 0.15669865906238556, 1.332929253578186, -0.6112174987792969, 1.2603883743286133, 1.0204474925994873, 0.4248219132423401, -0.010861115530133247, -0.36434775590896606, -0.3478507995605469, 1.1009271144866943, -0.877191960811615, -0.6050300598144531, -0.16243936121463776, 0.2471112608909607, -0.28359130024909973, -0.2385547012090683, 0.3843693733215332, 0.6056872010231018, -1.128277063369751, 0.42960673570632935, -0.33027219772338867, -0.9414247274398804, -0.168540820479393, -0.11091547459363937, 0.26046377420425415, -0.09574418514966965, -0.12403957545757294, 0.5706455707550049, 0.7312937378883362, -0.00193749088793993, 1.0177524089813232, 1.1962319612503052, -0.2469550222158432, -0.40992340445518494, -1.0507673025131226, 0.4235159754753113, -0.42448896169662476, -1.4505618810653687, 0.1434335857629776, 0.9284776449203491, 0.3169987201690674, -0.34473493695259094, -0.2542683780193329, 0.12978945672512054, 0.7855414748191833, 0.7985362410545349, -0.4594614803791046, -1.090949296951294, -0.06712078303098679, -0.73646479845047, -0.6922885775566101, 0.2658938765525818, 0.5993107557296753, 0.09047574549913406, 0.5718227028846741, 0.5836776494979858, 0.47943389415740967, -0.16105207800865173, -0.37695786356925964, 0.7000206708908081, 0.3347909152507782, -0.5058838725090027, -0.6015501618385315, 0.5520305633544922, 0.09083590656518936, 0.04596295952796936, 0.9505884051322937, 0.4112851619720459, -0.11654826998710632, 1.121293306350708, 0.22824019193649292, -0.1557670682668686, 0.8024351596832275, 2.1932644844055176, -0.3757886290550232, -0.8873815536499023, -0.955962598323822, -0.9917334914207458, 0.8898617029190063, 0.10007618367671967, -1.5832502841949463, 0.19119203090667725, -0.5486313700675964, 0.4305437505245209, -0.6859533786773682, -0.90931236743927, 0.11314082145690918, 0.37844252586364746, -0.5172845125198364, -1.0586042404174805, 0.1907622516155243, -1.1557443141937256, 0.8252583742141724, -0.01724928990006447, -0.5708369612693787, 0.4810085892677307, 2.1913201808929443, -0.19720999896526337, -0.04736755043268204, 0.483786016702652, 1.1635863780975342, 0.9644410610198975, -0.02186642587184906, 0.49116984009742737, 0.5770859718322754, -0.8549023866653442, 0.5493804216384888, 0.17038187384605408, 0.9535785913467407, -0.9617277383804321, 0.7227381467819214, 0.73997563123703, 0.04808010905981064, 0.6097393035888672, -0.7850260734558105, 0.4890987277030945, -0.6761204600334167, -0.6953967809677124, -0.7794307470321655, -1.304259181022644, 0.22989246249198914]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Ollama server details\n",
    "OLLAMA_URL = \"http://127.0.0.1:12345\"\n",
    "\n",
    "def get_nomic_embedding(text):\n",
    "    payload = {\n",
    "        \"model\": \"nomic-embed-text\",\n",
    "        \"prompt\": text,\n",
    "        \"embed\": True\n",
    "    }\n",
    "    response = requests.post(f\"{OLLAMA_URL}/api/embeddings\", json=payload)\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"embedding\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "\n",
    "# Test with a sample text\n",
    "text = \"The quick brown fox jumps over the lazy.\"\n",
    "embedding = get_nomic_embedding(text)\n",
    "\n",
    "if embedding:\n",
    "    print(\"Embedding Generated:\")\n",
    "    print(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Ollama' object has no attribute 'embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Test with a sample text\u001b[39;00m\n\u001b[0;32m     16\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe quick brown fox jumps over the lazy dog.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 17\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Embedding:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(embedding)\n",
      "Cell \u001b[1;32mIn[19], line 12\u001b[0m, in \u001b[0;36mgenerate_embedding\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_embedding\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m---> 12\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mollama_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m(text)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\pydantic\\main.py:892\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 892\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Ollama' object has no attribute 'embeddings'"
     ]
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "\n",
    "# Connect to the Ollama server\n",
    "ollama_llm = Ollama(\n",
    "    model=\"nomic-embed-text\",\n",
    "    base_url=\"http://127.0.0.1:12345\"\n",
    ")\n",
    "\n",
    "# Function to generate embeddings for a text\n",
    "def generate_embedding(text: str):\n",
    "    response = ollama_llm.embeddings(text)\n",
    "    return response\n",
    "\n",
    "# Test with a sample text\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "embedding = generate_embedding(text)\n",
    "\n",
    "print(\"Generated Embedding:\")\n",
    "print(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Ollama' object has no attribute 'embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 10\u001b[0m\n\u001b[0;32m      4\u001b[0m ollama_llm \u001b[38;5;241m=\u001b[39m Ollama(\n\u001b[0;32m      5\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnomic-embed-text\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://127.0.0.1:12345\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Generate embedding for the prompt\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mollama_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnomic-embed-text\u001b[39m\u001b[38;5;124m\"\u001b[39m, prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe sky is blue because of Rayleigh scattering\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Print the generated embedding\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Embedding:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\pydantic\\main.py:892\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 892\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Ollama' object has no attribute 'embeddings'"
     ]
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "# Connect to the Ollama server\n",
    "ollama_llm = Ollama(\n",
    "    model=\"nomic-embed-text\",\n",
    "    base_url=\"http://127.0.0.1:12345\"\n",
    ")\n",
    "\n",
    "# Generate embedding for the prompt\n",
    "embedding = ollama_llm.embeddings(model=\"nomic-embed-text\", prompt=\"The sky is blue because of Rayleigh scattering\")\n",
    "\n",
    "# Print the generated embedding\n",
    "print(\"Generated Embedding:\")\n",
    "print(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting chromadb\n",
      "  Using cached chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Using cached build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from chromadb) (2.10.4)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
      "  Using cached chroma_hnswlib-0.7.6-cp310-cp310-win_amd64.whl.metadata (262 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb)\n",
      "  Using cached fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from chromadb) (1.26.4)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Using cached posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from chromadb) (4.12.2)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Using cached onnxruntime-1.20.1-cp310-cp310-win_amd64.whl.metadata (4.7 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_api-1.29.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Using cached opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_sdk-1.29.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting tokenizers<=0.20.3,>=0.13.2 (from chromadb)\n",
      "  Using cached tokenizers-0.20.3-cp310-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Using cached PyPika-0.48.9-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from chromadb) (7.4.0)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Using cached importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Using cached grpcio-1.68.1-cp310-cp310-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Using cached bcrypt-4.2.1-cp39-abi3-win_amd64.whl.metadata (10 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Using cached typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Using cached kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from chromadb) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Using cached mmh3-5.0.1-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from chromadb) (3.10.12)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from chromadb) (0.27.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: tomli>=1.1.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
      "Collecting starlette<0.42.0,>=0.40.0 (from fastapi>=0.95.2->chromadb)\n",
      "  Using cached starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.6.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached google_auth-2.37.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached flatbuffers-24.12.23-py2.py3-none-any.whl.metadata (876 bytes)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached protobuf-5.29.2-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached Deprecated-1.2.15-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting importlib-metadata<=8.5.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached wrapt-1.17.0-cp310-cp310-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.27.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.15.1)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers<=0.20.3,>=0.13.2->chromadb)\n",
      "  Using cached huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached httptools-0.6.4-cp310-cp310-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached watchfiles-1.0.3-cp310-cp310-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached websockets-14.1-cp310-cp310-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting filelock (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.3.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Using cached chromadb-0.5.23-py3-none-any.whl (628 kB)\n",
      "Using cached chroma_hnswlib-0.7.6-cp310-cp310-win_amd64.whl (150 kB)\n",
      "Using cached bcrypt-4.2.1-cp39-abi3-win_amd64.whl (153 kB)\n",
      "Using cached build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Using cached fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
      "Using cached grpcio-1.68.1-cp310-cp310-win_amd64.whl (4.4 MB)\n",
      "Using cached kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
      "Using cached mmh3-5.0.1-cp310-cp310-win_amd64.whl (39 kB)\n",
      "Using cached onnxruntime-1.20.1-cp310-cp310-win_amd64.whl (11.3 MB)\n",
      "Using cached opentelemetry_api-1.29.0-py3-none-any.whl (64 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\n",
      "Using cached opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl (12 kB)\n",
      "Using cached opentelemetry_instrumentation-0.50b0-py3-none-any.whl (30 kB)\n",
      "Using cached opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl (16 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl (166 kB)\n",
      "Using cached opentelemetry_util_http-0.50b0-py3-none-any.whl (6.9 kB)\n",
      "Using cached opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\n",
      "Using cached posthog-3.7.4-py2.py3-none-any.whl (54 kB)\n",
      "Using cached tokenizers-0.20.3-cp310-none-win_amd64.whl (2.4 MB)\n",
      "Using cached typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Using cached uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "Using cached importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached Deprecated-1.2.15-py2.py3-none-any.whl (9.9 kB)\n",
      "Using cached durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Using cached google_auth-2.37.0-py2.py3-none-any.whl (209 kB)\n",
      "Using cached googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "Using cached httptools-0.6.4-cp310-cp310-win_amd64.whl (88 kB)\n",
      "Using cached huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "Using cached importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
      "Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached protobuf-5.29.2-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached starlette-0.41.3-py3-none-any.whl (73 kB)\n",
      "Using cached watchfiles-1.0.3-cp310-cp310-win_amd64.whl (284 kB)\n",
      "Using cached websockets-14.1-cp310-cp310-win_amd64.whl (163 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached flatbuffers-24.12.23-py2.py3-none-any.whl (30 kB)\n",
      "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/6.2 MB 3.7 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.0/6.2 MB 2.7 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.0/6.2 MB 2.7 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.3/6.2 MB 1.4 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 1.6/6.2 MB 1.5 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 2.6/6.2 MB 2.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 3.1/6.2 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.4/6.2 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.4/6.2 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.4/6.2 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 4.7/6.2 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.0/6.2 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.5/6.2 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.8/6.2 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 1.9 MB/s eta 0:00:00\n",
      "Using cached asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached wrapt-1.17.0-cp310-cp310-win_amd64.whl (38 kB)\n",
      "Using cached zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: pypika, mpmath, monotonic, flatbuffers, durationpy, zipp, wrapt, websockets, sympy, shellingham, pyreadline3, pyproject_hooks, PyPDF2, pyasn1, protobuf, opentelemetry-util-http, oauthlib, mmh3, importlib-resources, httptools, grpcio, fsspec, filelock, chroma-hnswlib, cachetools, bcrypt, backoff, asgiref, watchfiles, uvicorn, starlette, rsa, requests-oauthlib, pyasn1-modules, posthog, opentelemetry-proto, importlib-metadata, humanfriendly, huggingface-hub, googleapis-common-protos, deprecated, build, typer, tokenizers, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, google-auth, fastapi, coloredlogs, opentelemetry-semantic-conventions, onnxruntime, kubernetes, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
      "Successfully installed PyPDF2-3.0.1 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 cachetools-5.5.0 chroma-hnswlib-0.7.6 chromadb-0.5.23 coloredlogs-15.0.1 deprecated-1.2.15 durationpy-0.9 fastapi-0.115.6 filelock-3.16.1 flatbuffers-24.12.23 fsspec-2024.12.0 google-auth-2.37.0 googleapis-common-protos-1.66.0 grpcio-1.68.1 httptools-0.6.4 huggingface-hub-0.27.0 humanfriendly-10.0 importlib-metadata-8.5.0 importlib-resources-6.4.5 kubernetes-31.0.0 mmh3-5.0.1 monotonic-1.6 mpmath-1.3.0 oauthlib-3.2.2 onnxruntime-1.20.1 opentelemetry-api-1.29.0 opentelemetry-exporter-otlp-proto-common-1.29.0 opentelemetry-exporter-otlp-proto-grpc-1.29.0 opentelemetry-instrumentation-0.50b0 opentelemetry-instrumentation-asgi-0.50b0 opentelemetry-instrumentation-fastapi-0.50b0 opentelemetry-proto-1.29.0 opentelemetry-sdk-1.29.0 opentelemetry-semantic-conventions-0.50b0 opentelemetry-util-http-0.50b0 posthog-3.7.4 protobuf-5.29.2 pyasn1-0.6.1 pyasn1-modules-0.4.1 pypika-0.48.9 pyproject_hooks-1.2.0 pyreadline3-3.5.4 requests-oauthlib-2.0.0 rsa-4.9 shellingham-1.5.4 starlette-0.41.3 sympy-1.13.3 tokenizers-0.20.3 typer-0.15.1 uvicorn-0.34.0 watchfiles-1.0.3 websockets-14.1 wrapt-1.17.0 zipp-3.21.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2 chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Embedding' from 'langchain.embeddings' (C:\\Users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\langchain\\embeddings\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mPyPDF2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embedding\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Embedding' from 'langchain.embeddings' (C:\\Users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\langchain\\embeddings\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import PyPDF2\n",
    "import chromadb\n",
    "from langchain.embeddings import Embedding\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Ollama server details\n",
    "OLLAMA_URL = \"http://127.0.0.1:12345\"\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Collection name in ChromaDB\n",
    "collection_name = \"pdf_embeddings\"\n",
    "\n",
    "# Create or load the collection\n",
    "if collection_name not in client.list_collections():\n",
    "    collection = client.create_collection(collection_name)\n",
    "else:\n",
    "    collection = client.get_collection(collection_name)\n",
    "\n",
    "# LangChain embedding class for Nomic model\n",
    "class NomicEmbedding(Embedding):\n",
    "    def __init__(self, model_url):\n",
    "        self.model_url = model_url\n",
    "    \n",
    "    def embed_documents(self, texts):\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            embedding = self.get_embedding(text)\n",
    "            if embedding:\n",
    "                embeddings.append(embedding)\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.get_embedding(text)\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        payload = {\n",
    "            \"model\": \"nomic-embed-text\",\n",
    "            \"prompt\": text,\n",
    "            \"embed\": True\n",
    "        }\n",
    "        response = requests.post(f\"{self.model_url}/api/embeddings\", json=payload)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"embedding\")\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {response.text}\")\n",
    "            return None\n",
    "\n",
    "# Initialize Nomic embedding and LangChain's Chroma vector store\n",
    "nomic_embedding = NomicEmbedding(OLLAMA_URL)\n",
    "vectorstore = Chroma(collection_name=collection_name, embedding_function=nomic_embedding, client=client)\n",
    "\n",
    "def split_pdf_into_chunks_with_overlap(pdf_path, chunk_size=1000, overlap_size=200):\n",
    "    \"\"\"\n",
    "    Split a PDF into overlapping chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "        \n",
    "        # Split the full text into chunks with overlap\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = start + chunk_size\n",
    "            chunk = text[start:end]\n",
    "            chunks.append(chunk)\n",
    "            \n",
    "            # Move start position by (chunk_size - overlap_size) to ensure overlap\n",
    "            start = end - overlap_size\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def store_pdf_embeddings(pdf_path):\n",
    "    \"\"\"\n",
    "    Store PDF text chunks and their embeddings in ChromaDB using LangChain.\n",
    "    \"\"\"\n",
    "    chunks = split_pdf_into_chunks_with_overlap(pdf_path)\n",
    "    documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "    vectorstore.add_documents(documents)\n",
    "    print(f\"Stored {len(documents)} document chunks with overlap.\")\n",
    "\n",
    "def retrieve_pdf_embeddings(query_text):\n",
    "    \"\"\"\n",
    "    Retrieve similar document chunks based on a query text using LangChain's Chroma vector store.\n",
    "    \"\"\"\n",
    "    query_embedding = nomic_embedding.embed_query(query_text)\n",
    "    results = vectorstore.similarity_search_by_vector(query_embedding, k=5)  # Retrieve top 5 most similar chunks\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "pdf_path = \"sample.pdf\"\n",
    "store_pdf_embeddings(pdf_path)\n",
    "\n",
    "query_text = \"search for specific information\"\n",
    "retrieved_chunks = retrieve_pdf_embeddings(query_text)\n",
    "\n",
    "print(\"Retrieved Chunks:\")\n",
    "for chunk in retrieved_chunks:\n",
    "    print(chunk.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Embedding' from 'langchain.embeddings' (C:\\Users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\langchain\\embeddings\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mPyPDF2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embedding\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Embedding' from 'langchain.embeddings' (C:\\Users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\langchain\\embeddings\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import PyPDF2\n",
    "import chromadb\n",
    "from langchain.embeddings import Embedding\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Ollama server details\n",
    "OLLAMA_URL = \"http://127.0.0.1:12345\"\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Collection name in ChromaDB\n",
    "collection_name = \"pdf_embeddings\"\n",
    "\n",
    "# Create or load the collection\n",
    "if collection_name not in client.list_collections():\n",
    "    collection = client.create_collection(collection_name)\n",
    "else:\n",
    "    collection = client.get_collection(collection_name)\n",
    "\n",
    "# LangChain embedding class for Nomic model\n",
    "class NomicEmbedding(Embedding):\n",
    "    def __init__(self, model_url):\n",
    "        self.model_url = model_url\n",
    "    \n",
    "    def embed_documents(self, texts):\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            embedding = self.get_embedding(text)\n",
    "            if embedding:\n",
    "                embeddings.append(embedding)\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.get_embedding(text)\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        payload = {\n",
    "            \"model\": \"nomic-embed-text\",\n",
    "            \"prompt\": text,\n",
    "            \"embed\": True\n",
    "        }\n",
    "        response = requests.post(f\"{self.model_url}/api/embeddings\", json=payload)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"embedding\")\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {response.text}\")\n",
    "            return None\n",
    "\n",
    "# Initialize Nomic embedding and LangChain's Chroma vector store\n",
    "nomic_embedding = NomicEmbedding(OLLAMA_URL)\n",
    "vectorstore = Chroma(collection_name=collection_name, embedding_function=nomic_embedding, client=client)\n",
    "\n",
    "def split_pdf_into_chunks_with_overlap(pdf_path, chunk_size=1000, overlap_size=200):\n",
    "    \"\"\"\n",
    "    Split a PDF into overlapping chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "        \n",
    "        # Split the full text into chunks with overlap\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = start + chunk_size\n",
    "            chunk = text[start:end]\n",
    "            chunks.append(chunk)\n",
    "            \n",
    "            # Move start position by (chunk_size - overlap_size) to ensure overlap\n",
    "            start = end - overlap_size\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def store_pdf_embeddings(pdf_path):\n",
    "    \"\"\"\n",
    "    Store PDF text chunks and their embeddings in ChromaDB using LangChain.\n",
    "    \"\"\"\n",
    "    chunks = split_pdf_into_chunks_with_overlap(pdf_path)\n",
    "    documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "    vectorstore.add_documents(documents)\n",
    "    print(f\"Stored {len(documents)} document chunks with overlap.\")\n",
    "\n",
    "def retrieve_pdf_embeddings(query_text):\n",
    "    \"\"\"\n",
    "    Retrieve similar document chunks based on a query text using LangChain's Chroma vector store.\n",
    "    \"\"\"\n",
    "    query_embedding = nomic_embedding.embed_query(query_text)\n",
    "    results = vectorstore.similarity_search_by_vector(query_embedding, k=5)  # Retrieve top 5 most similar chunks\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "pdf_path = \"sample.pdf\"\n",
    "store_pdf_embeddings(pdf_path)\n",
    "\n",
    "query_text = \"search for specific information\"\n",
    "retrieved_chunks = retrieve_pdf_embeddings(query_text)\n",
    "\n",
    "print(\"Retrieved Chunks:\")\n",
    "for chunk in retrieved_chunks:\n",
    "    print(chunk.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 1 document chunks with overlap.\n",
      "Retrieved Chunks:\n",
      "SAMRAT\n",
      "ABDUL\n",
      "JALIL\n",
      "AI\n",
      "and\n",
      "Backend\n",
      "Enthusiast\n",
      "SKILLS\n",
      "Back-End:\n",
      "Python,\n",
      "C/C++,Java\n",
      ",Php,\n",
      "FastAPI,\n",
      "Express.js,\n",
      "Node.js\n",
      "Database:\n",
      "MySQL,\n",
      "MongoDB\n",
      ".\n",
      "Front-End:\n",
      "JavaScript,\n",
      "TypeScript,\n",
      "React.js,\n",
      "HTML,\n",
      "CSS\n",
      ".\n",
      "Machine\n",
      "Learning:\n",
      "TensorFlow,\n",
      "PyTorch,\n",
      "Keras,\n",
      "Scikit-learn,\n",
      "NumPy,\n",
      "Pandas,\n",
      "Seaborn,Matplotlib,\n",
      "OpenCV,\n",
      "MediaPipe\n",
      "etc.\n",
      "App\n",
      "Development:\n",
      "Flutter\n",
      ".\n",
      "Tools:\n",
      "Git,\n",
      "Docker,\n",
      "Github\n",
      "Action\n",
      ",JIRA,\n",
      "VS\n",
      "Code,\n",
      "Anaconda\n",
      ".\n",
      "WORK\n",
      "EXPERIENCE\n",
      "Undergraduate\n",
      "Thesis:\n",
      "GradeVision\n",
      "(in\n",
      "collaboration\n",
      "with\n",
      "Edusoftbd\n",
      "Ltd)\n",
      "\n",
      "Developed\n",
      "an\n",
      "OCR-based\n",
      "application\n",
      "to\n",
      "automate\n",
      "result\n",
      "management\n",
      "by\n",
      "capturing\n",
      "result\n",
      "data\n",
      "from\n",
      "images.\n",
      "Features\n",
      "include\n",
      "real-time\n",
      "data\n",
      "editing,\n",
      "updates,\n",
      "centralized\n",
      "database\n",
      "storage,\n",
      "section-wise\n",
      "statistical\n",
      "views,\n",
      "authentication,\n",
      "and\n",
      "OTP\n",
      "verication\n",
      "via\n",
      "email.\n",
      "\n",
      "Tech\n",
      "Stack\n",
      ":\n",
      "Python,\n",
      "FastAPI,\n",
      "Flutter,\n",
      "MySQL\n",
      ".\n",
      "DL\n",
      "Sprint\n",
      "2.0:\n",
      "Bengali\n",
      "Document\n",
      "Layout\n",
      "Analysis\n",
      "\n",
      "Fine-tuned\n",
      "YOLO\n",
      "and\n",
      "Detectron\n",
      "models\n",
      "to\n",
      "analyze\n",
      "Bengali \n",
      "document\n",
      "layouts,\n",
      "tackling\n",
      "challenges\n",
      "with\n",
      "historical \n",
      "newspapers,\n",
      "government\n",
      "documents,\n",
      "magazines,\n",
      "and\n",
      "books. \n",
      "The\n",
      "models\n",
      "identied\n",
      "paragraphs,\n",
      "text\n",
      "boxes,\n",
      "images,\n",
      "and \n",
      "tables,\n",
      "with\n",
      "Detectron\n",
      "delivering\n",
      "superior\n",
      "results \n",
      "\n",
      "Tech\n",
      "Stack\n",
      ":\n",
      "Pytorch,\n",
      "Detectron2,\n",
      "Yolo\n",
      ".\n",
      "Bhasha\n",
      "Bichitra\n",
      ":\n",
      "Bengali\n",
      "Speech-to-Text\n",
      "for\n",
      "Regional\n",
      "Dialects\n",
      "\n",
      "Fine-tuned\n",
      "the\n",
      "Whisper\n",
      "model\n",
      "for\n",
      "Automatic\n",
      "Speech \n",
      "Recognition\n",
      "(ASR)\n",
      "to\n",
      "convert\n",
      "speech\n",
      "into\n",
      "Bengali\n",
      "regional\n",
      "text. \n",
      "The\n",
      "model\n",
      "was\n",
      "trained\n",
      "on\n",
      "diverse\n",
      "datasets\n",
      "to\n",
      "handle\n",
      "variations \n",
      "in\n",
      "pronunciation\n",
      "and\n",
      "vocabulary\n",
      "across\n",
      "dialects\n",
      ". \n",
      "\n",
      "Tech\n",
      "Stack\n",
      ":\n",
      "Pytorch,\n",
      "Whisper\n",
      ".\n",
      "Bhashamul:\n",
      "Bengali\n",
      "Regional\n",
      "IPA\n",
      "Transcription\n",
      "\n",
      "Fine-tuned\n",
      "Google\n",
      "ByT5\n",
      "(small\n",
      "and\n",
      "base\n",
      "versions)\n",
      "and\n",
      "mBART-5\n",
      "(base\n",
      "and\n",
      "small\n",
      "versions)\n",
      "for\n",
      "Bengali\n",
      "regional\n",
      "IPA\n",
      "transcription.\n",
      "After\n",
      "thorough\n",
      "evaluation,\n",
      "the\n",
      "ByT5\n",
      "small\n",
      "model\n",
      "outperformed\n",
      "others,\n",
      "providing\n",
      "superior\n",
      "transcription\n",
      "accuracy\n",
      "for\n",
      "regional\n",
      "phonetic\n",
      "variations.\n",
      "This\n",
      "model\n",
      "improves\n",
      "transcription\n",
      "quality\n",
      "for\n",
      "regional\n",
      "speech-to-text\n",
      "applications.\n",
      "\n",
      "Tech\n",
      "Stack\n",
      ":\n",
      "Pytorch,\n",
      "UMT5\n",
      "small\n",
      "and\n",
      "base\n",
      ",\n",
      "byt5\n",
      "small\n",
      "and \n",
      "base\n",
      "CONTACT\n",
      "+880\n",
      "1518445128\n",
      "Email:\n",
      "samratabduljalil21@gmail.com\n",
      "LinkedIn:\n",
      "linkedin.com/samrat-abdul-jalil\n",
      "GitHub:\n",
      "github.com/samratabduljalil\n",
      "Kaggle:\n",
      "kaggle/samratabduljalil\n",
      "Leetcode:\n",
      "leetcode/Samrat_Abdul_Jalil\n",
      "Badda,\n",
      "Dhaka.\n",
      "AWARD\n",
      "\n",
      "Kaggle\n",
      "Expert\n",
      "\n",
      "2024\n",
      "\n",
      "Finalist\n",
      "in\n",
      "Bhashamul\n",
      ":\n",
      "Bengali\n",
      "Regional\n",
      "IPA\n",
      "Transcription\n",
      "\n",
      "2024\n",
      "\n",
      "Top\n",
      "10\n",
      "Finalist\n",
      "in\n",
      "Bhasha\n",
      "Bichitra\n",
      ":\n",
      "ASR\n",
      "for\n",
      "Regional\n",
      "Dialects\n",
      "\n",
      "2024\n",
      "\n",
      "Top\n",
      "10\n",
      "Finalist\n",
      "in\n",
      "DL\n",
      "Enigma\n",
      "1.0\n",
      "\n",
      "SUST\n",
      "CSE\n",
      "Carnival\n",
      "2024\n",
      "\n",
      "8th\n",
      "in\n",
      "ML\n",
      "Olympiad\n",
      "-\n",
      "CO2\n",
      "Emissions\n",
      "Prediction\n",
      "Challenge\n",
      "\n",
      "2024\n",
      "\n",
      "Champion\n",
      "in\n",
      "Winner\n",
      "Career\n",
      "Launchpad\n",
      "Quiz\n",
      "competition\n",
      "\n",
      "2023\n",
      "\n",
      "Finalist\n",
      "in\n",
      "DL\n",
      "Sprint\n",
      "2.0\n",
      "-\n",
      "BUET\n",
      "CSE\n",
      "Fest\n",
      "\n",
      "2023\n",
      "\n",
      "Top\n",
      "10\n",
      "Finalist\n",
      "in\n",
      "Skitto\n",
      "Hackathon\n",
      "\n",
      "2022\n",
      "\n",
      "Champion\n",
      "in\n",
      "UIU\n",
      "Online\n",
      "Cisco\n",
      "Quiz\n",
      "Competition\n",
      "Organized\n",
      "by\n",
      "GP\n",
      "Academy\n",
      "\n",
      "2022\n",
      "\n",
      "Champion\n",
      "in\n",
      "UIU\n",
      "AI\n",
      "Contes\n",
      "t\n",
      "\n",
      "2022\n",
      "\n",
      "Champion\n",
      "in\n",
      "Programming\n",
      "for\n",
      "beginners\n",
      "\n",
      "2021\n",
      "EDUCATION\n",
      "B.Sc.\n",
      "in\n",
      "Computer\n",
      "Science\n",
      "and\n",
      "Engineering\n",
      "United\n",
      "International\n",
      "University\n",
      "July\n",
      "2020\n",
      "-\n",
      "Present\n",
      "Major\n",
      "In\n",
      ":\n",
      "Data\n",
      "Science \n",
      "Current\n",
      "CGPA:\n",
      "3.30DL\n",
      "Enigma\n",
      "1.0\n",
      ":\n",
      "Object\n",
      "Detection\n",
      "for\n",
      "Bangladeshi\n",
      "Autonomous\n",
      "Driving\n",
      "\n",
      "Developed\n",
      "model\n",
      "by\n",
      "ne-tuning\n",
      "YOLO\n",
      "and\n",
      "Detectron\n",
      "models\n",
      "for\n",
      "object\n",
      "detection\n",
      "in\n",
      "autonomous\n",
      "driving\n",
      "applications\n",
      "in\n",
      "Bangladesh.\n",
      "Addressed\n",
      "challenges\n",
      "with\n",
      "unique\n",
      "objects\n",
      "such\n",
      "as\n",
      "three-wheelers\n",
      "and\n",
      "wheelchairs\n",
      "to\n",
      "enhance\n",
      "detection\n",
      "accuracy\n",
      "\n",
      "Tech\n",
      "Stack\n",
      ":\n",
      "Pytorch,\n",
      "Yolov8,\n",
      "Detectron2\n",
      ".\n",
      "Undergraduate\n",
      "project:\n",
      "AgriCareHub\n",
      "\n",
      "Developed\n",
      "a\n",
      "web\n",
      "application\n",
      "that\n",
      "allows\n",
      "farmers\n",
      "to\n",
      "upload \n",
      "images\n",
      "of\n",
      "their\n",
      "crops\n",
      "to\n",
      "check\n",
      "for\n",
      "diseases.\n",
      "The\n",
      "app\n",
      "uses\n",
      "image \n",
      "recognition\n",
      "to\n",
      "analyze\n",
      "crop\n",
      "health\n",
      "and,\n",
      "if\n",
      "a\n",
      "disease\n",
      "is\n",
      "detected, \n",
      "recommends\n",
      "medication\n",
      "and\n",
      "proper\n",
      "treatment\n",
      "in\n",
      "Bengali.\n",
      "It \n",
      "also\n",
      "provides\n",
      "audio\n",
      "instructions\n",
      "on\n",
      "how\n",
      "to\n",
      "apply\n",
      "the \n",
      "medication.. \n",
      "\n",
      "Tech\n",
      "Stack\n",
      ":\n",
      "Python,\n",
      "FastAPI,\n",
      "TensorFlow,\n",
      "Express,\n",
      "React.js, \n",
      "MySQL\n",
      ".\n",
      "ML\n",
      "Olympiad\n",
      ":\n",
      "CO2\n",
      "Emissions\n",
      "Prediction\n",
      "Challenge\n",
      "\n",
      "Developed\n",
      "and\n",
      "evaluated\n",
      "various\n",
      "forecasting\n",
      "models\n",
      "for\n",
      "CO2 \n",
      "emissions\n",
      "prediction.\n",
      "After\n",
      "thorough\n",
      "analysis,\n",
      "ARIMA,\n",
      "LSTM, \n",
      "and\n",
      "Logistic\n",
      "Regression\n",
      "models\n",
      "demonstrated\n",
      "the\n",
      "best \n",
      "performance\n",
      "in\n",
      "accurately\n",
      "predicting\n",
      "emissions\n",
      "trends. \n",
      "\n",
      "Tech\n",
      "Stack\n",
      ":\n",
      "ARIMA,\n",
      "SARIMA,\n",
      "LSTM,\n",
      "Logistic \n",
      "Regression,Facebook\n",
      "Prophet,\n",
      "pandas\n",
      "Undergraduate\n",
      "project:\n",
      "Learn2Resume\n",
      "\n",
      "Developed\n",
      "a\n",
      "CV\n",
      "generator\n",
      "and\n",
      "e-learning\n",
      "platform\n",
      "allowing \n",
      "users\n",
      "to\n",
      "create\n",
      "and\n",
      "share\n",
      "CVs,\n",
      "purchase\n",
      "certication\n",
      "courses, \n",
      "and\n",
      "download\n",
      "certicates.\n",
      "Integrated\n",
      "live\n",
      "chat\n",
      "support,\n",
      "secure \n",
      "online\n",
      "payments,\n",
      "and\n",
      "implemented\n",
      "features\n",
      "for\n",
      "course\n",
      "and\n",
      "user \n",
      "management. \n",
      "\n",
      "Tech\n",
      "Stack\n",
      ":\n",
      "PHP,\n",
      "JavaScript,\n",
      "MySQL,\n",
      "HTML,\n",
      "CSS\n",
      ". \n",
      ".\n",
      "Undergraduate\n",
      "project:\n",
      "easy-Publication\n",
      "\n",
      "Developed\n",
      "a\n",
      "self-publishing\n",
      "platform\n",
      "enabling\n",
      "authors\n",
      "to\n",
      "easily \n",
      "publish\n",
      "their\n",
      "work\n",
      "with\n",
      "exible\n",
      "terms,\n",
      "revenue\n",
      "sharing,\n",
      "and \n",
      "24-hour\n",
      "delivery\n",
      "through\n",
      "a\n",
      "network\n",
      "of\n",
      "64\n",
      "printing\n",
      "presses \n",
      "across\n",
      "districts.\n",
      "The\n",
      "platform\n",
      "addresses\n",
      "challenges\n",
      "faced\n",
      "by \n",
      "new\n",
      "authors\n",
      "in\n",
      "traditional\n",
      "publishing.\n",
      "Additionally,\n",
      "it\n",
      "integrates \n",
      "online\n",
      "payment\n",
      "systems\n",
      "for\n",
      "smooth\n",
      "transactions\n",
      "and\n",
      "live\n",
      "chat \n",
      "support\n",
      "for\n",
      "real-time\n",
      "author\n",
      "assistance.. \n",
      "\n",
      "Tech\n",
      "Stack\n",
      ":\n",
      "PHP\n",
      ",\n",
      "MySQL,\n",
      "Bootstrap\n",
      ",HTML,\n",
      "CSS\n",
      ".\n",
      "Undergraduate\n",
      "project:\n",
      "Medind\n",
      "\n",
      "Medind\n",
      "is\n",
      "a\n",
      "web\n",
      "application\n",
      "designed\n",
      "to\n",
      "help\n",
      "users\n",
      "nd\n",
      "the \n",
      "nearest\n",
      "medicine\n",
      "shops\n",
      "and\n",
      "check\n",
      "the\n",
      "availability\n",
      "of\n",
      "their \n",
      "desired\n",
      "medicines.\n",
      "The\n",
      "app\n",
      "also\n",
      "provides\n",
      "information\n",
      "on\n",
      "the \n",
      "operating\n",
      "hours\n",
      "of\n",
      "these\n",
      "shops,\n",
      "so\n",
      "users\n",
      "can\n",
      "plan\n",
      "their\n",
      "visit \n",
      "accordingly.\n",
      "Additionally,\n",
      "shopkeepers\n",
      "can\n",
      "upload\n",
      "and\n",
      "manage \n",
      "the\n",
      "list\n",
      "of\n",
      "medicines\n",
      "available\n",
      "at\n",
      "their\n",
      "stores. \n",
      "\n",
      "Tech\n",
      "Stack\n",
      ":\n",
      "PHP\n",
      ",\n",
      "MySQL,\n",
      "Bootstrap\n",
      ",HTML,\n",
      "CSS\n",
      ".\n",
      "COURSES\n",
      "Machine\n",
      "Learning\n",
      "|\n",
      "Digital\n",
      "Image\n",
      "Processing\n",
      "|\n",
      "Bioinformatics\n",
      "|\n",
      "Human\n",
      "Computer\n",
      "Interaction\n",
      "|\n",
      "Database\n",
      "Management\n",
      "Systems\n",
      "|\n",
      "Software\n",
      "Engineering\n",
      "|\n",
      "Data\n",
      "Structures\n",
      "and\n",
      "Algorithms\n",
      "|\n",
      "Object-Oriented\n",
      "Programming\n",
      "|\n",
      "System\n",
      "Design\n",
      "and\n",
      "Analysis\n",
      "|\n",
      "INTEREST\n",
      "Multimodal\n",
      "Large\n",
      "Language\n",
      "Model\n",
      "|\n",
      "Retrieval-Augmented\n",
      "Generation\n",
      "|\n",
      "AI\n",
      "Agent\n",
      "|\n",
      "Back-end\n",
      "development\n",
      "|\n",
      "Computer\n",
      "Vision\n",
      "|\n",
      "Natural\n",
      "Language\n",
      "Processing\n",
      "|\n",
      "Automatic\n",
      "Speech\n",
      "Recognition\n",
      "|\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import PyPDF2\n",
    "import chromadb\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Ollama server details\n",
    "OLLAMA_URL = \"http://127.0.0.1:12345\"\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Collection name in ChromaDB\n",
    "collection_name = \"pdf_embedding11\"\n",
    "\n",
    "# Create or load the collection\n",
    "if collection_name not in client.list_collections():\n",
    "    collection = client.create_collection(collection_name)\n",
    "else:\n",
    "    collection = client.get_collection(collection_name)\n",
    "\n",
    "# LangChain embedding class for Nomic model\n",
    "class NomicEmbedding(Embeddings):\n",
    "    def __init__(self, model_url):\n",
    "        self.model_url = model_url\n",
    "    \n",
    "    def embed_documents(self, texts):\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            embedding = self.get_embedding(text)\n",
    "            if embedding:\n",
    "                embeddings.append(embedding)\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.get_embedding(text)\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        payload = {\n",
    "            \"model\": \"nomic-embed-text\",\n",
    "            \"prompt\": text,\n",
    "            \"embed\": True\n",
    "        }\n",
    "        response = requests.post(f\"{self.model_url}/api/embeddings\", json=payload)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"embedding\")\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {response.text}\")\n",
    "            return None\n",
    "\n",
    "# Initialize Nomic embedding and LangChain's Chroma vector store\n",
    "nomic_embedding = NomicEmbedding(OLLAMA_URL)\n",
    "vectorstore = Chroma(collection_name=collection_name, embedding_function=nomic_embedding, client=client)\n",
    "\n",
    "def split_pdf_into_chunks_with_overlap(pdf_path, chunk_size=500, overlap_size=50):\n",
    "    \"\"\"\n",
    "    Split a PDF into overlapping chunks using LangChain's CharacterTextSplitter.\n",
    "    \"\"\"\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "\n",
    "        # Initialize LangChain's CharacterTextSplitter with overlap\n",
    "        text_splitter = CharacterTextSplitter(\n",
    "            chunk_size=chunk_size, \n",
    "            chunk_overlap=overlap_size,\n",
    "            length_function=len\n",
    "        )\n",
    "        \n",
    "        # Split the full text into chunks\n",
    "        chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def store_pdf_embeddings(pdf_path):\n",
    "    \"\"\"\n",
    "    Store PDF text chunks and their embeddings in ChromaDB using LangChain.\n",
    "    \"\"\"\n",
    "    chunks = split_pdf_into_chunks_with_overlap(pdf_path)\n",
    "    documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "    vectorstore.add_documents(documents)\n",
    "    print(f\"Stored {len(documents)} document chunks with overlap.\")\n",
    "\n",
    "def retrieve_pdf_embeddings(query_text):\n",
    "    \"\"\"\n",
    "    Retrieve similar document chunks based on a query text using LangChain's Chroma vector store.\n",
    "    \"\"\"\n",
    "    query_embedding = nomic_embedding.embed_query(query_text)\n",
    "    results = vectorstore.similarity_search_by_vector(query_embedding, k=1)  # Retrieve top 5 most similar chunks\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "pdf_path = \"machine learning.pdf\"\n",
    "store_pdf_embeddings(pdf_path)\n",
    "\n",
    "query_text = \"kaggle expert year ?\"\n",
    "retrieved_chunks = retrieve_pdf_embeddings(query_text)\n",
    "\n",
    "print(\"Retrieved Chunks:\")\n",
    "for chunk in retrieved_chunks:\n",
    "    print(chunk.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Chunks:\n",
      "SAMRAT\n",
      "ABDUL\n",
      "JALIL\n",
      "AI\n",
      "and\n",
      "Backend\n",
      "Enthusiast\n",
      "SKILLS\n",
      "Back-End:\n",
      "Python,\n",
      "C/C++,Java\n",
      ",Php,\n",
      "FastAPI,\n",
      "Express.js,\n",
      "Node.js\n",
      "Database:\n",
      "MySQL,\n",
      "MongoDB\n",
      ".\n",
      "Front-End:\n",
      "JavaScript,\n",
      "TypeScript,\n",
      "React.js,\n",
      "HTML,\n",
      "CSS\n",
      ".\n",
      "Machine\n",
      "Learning:\n",
      "TensorFlow,\n",
      "PyTorch,\n",
      "Keras,\n",
      "Scikit-learn,\n",
      "NumPy,\n",
      "Pandas,\n",
      "Seaborn,Matplotlib,\n",
      "OpenCV,\n",
      "MediaPipe\n",
      "etc.\n",
      "App\n",
      "Development:\n",
      "Flutter\n",
      ".\n",
      "Tools:\n",
      "Git,\n",
      "Docker,\n",
      "Github\n",
      "Action\n",
      ",JIRA,\n",
      "VS\n",
      "Code,\n",
      "Anaconda\n",
      ".\n",
      "WORK\n",
      "EXPERIENCE\n",
      "Undergraduate\n",
      "Thesis:\n",
      "GradeVision\n",
      "(in\n",
      "collaboration\n",
      "with\n",
      "Edusoftbd\n",
      "Ltd)\n",
      "\n",
      "Developed\n",
      "an\n",
      "OCR-based\n",
      "application\n",
      "to\n",
      "automate\n",
      "result\n",
      "management\n",
      "by\n",
      "capturing\n",
      "result\n",
      "data\n",
      "from\n",
      "images.\n",
      "Features\n",
      "include\n",
      "real-time\n",
      "data\n",
      "editing,\n",
      "updates,\n",
      "centralized\n",
      "database\n",
      "storage,\n",
      "section-wise\n",
      "statistical\n",
      "views,\n",
      "authentication,\n",
      "and\n",
      "OTP\n",
      "verication\n",
      "via\n",
      "email.\n",
      "\n",
      "Tech\n",
      "Stack\n",
      ":\n",
      "Python,\n",
      "FastAPI,\n",
      "Flutter,\n",
      "MySQL\n",
      ".\n",
      "DL\n",
      "Sprint\n",
      "2.0:\n",
      "Bengali\n",
      "Document\n",
      "Layout\n",
      "Analysis\n",
      "\n",
      "Fine-tuned\n",
      "YOLO\n",
      "and\n",
      "Detectron\n",
      "models\n",
      "to\n",
      "analyze\n",
      "Bengali \n",
      "document\n",
      "layouts,\n",
      "tackling\n",
      "challenges\n",
      "with\n",
      "historical \n",
      "newspapers,\n",
      "government\n",
      "documents,\n",
      "magazines,\n",
      "and\n",
      "books. \n",
      "The\n",
      "models\n",
      "identied\n",
      "paragraphs,\n",
      "text\n",
      "boxes,\n",
      "images,\n",
      "and \n",
      "tables,\n",
      "with\n",
      "Detectron\n",
      "delivering\n",
      "superior\n",
      "results \n",
      "\n",
      "Tech\n",
      "Stack\n",
      ":\n",
      "Pytorch,\n",
      "Detectron2,\n",
      "Yolo\n",
      ".\n",
      "Bhasha\n",
      "Bichitra\n",
      ":\n",
      "Bengali\n",
      "Speech-to-Text\n",
      "for\n",
      "Regional\n",
      "Dialects\n",
      "\n",
      "Fine-tuned\n",
      "the\n",
      "Whisper\n",
      "model\n",
      "for\n",
      "Automatic\n",
      "Speech \n",
      "Recognition\n",
      "(ASR)\n",
      "to\n",
      "convert\n",
      "speech\n",
      "into\n",
      "Bengali\n",
      "regional\n",
      "text. \n",
      "The\n",
      "model\n",
      "was\n",
      "trained\n",
      "on\n",
      "diverse\n",
      "datasets\n",
      "to\n",
      "handle\n",
      "variations \n",
      "in\n",
      "pronunciation\n",
      "and\n",
      "vocabulary\n",
      "across\n",
      "dialects\n",
      ". \n",
      "\n",
      "Tech\n",
      "Stack\n",
      ":\n",
      "Pytorch,\n",
      "Whisper\n",
      ".\n",
      "Bhashamul:\n",
      "Bengali\n",
      "Regional\n",
      "IPA\n",
      "Transcription\n",
      "\n",
      "Fine-tuned\n",
      "Google\n",
      "ByT5\n",
      "(small\n",
      "and\n",
      "base\n",
      "versions)\n",
      "and\n",
      "mBART-5\n",
      "(base\n",
      "and\n",
      "small\n",
      "versions)\n",
      "for\n",
      "Bengali\n",
      "regional\n",
      "IPA\n",
      "transcription.\n",
      "After\n",
      "thorough\n",
      "evaluation,\n",
      "the\n",
      "ByT5\n",
      "small\n",
      "model\n",
      "outperformed\n",
      "others,\n",
      "providing\n",
      "superior\n",
      "transcription\n",
      "accuracy\n",
      "for\n",
      "regional\n",
      "phonetic\n",
      "variations.\n",
      "This\n",
      "model\n",
      "improves\n",
      "transcription\n",
      "quality\n",
      "for\n",
      "regional\n",
      "speech-to-text\n",
      "applications.\n",
      "\n",
      "Tech\n",
      "Stack\n",
      ":\n",
      "Pytorch,\n",
      "UMT5\n",
      "small\n",
      "and\n",
      "base\n",
      ",\n",
      "byt5\n",
      "small\n",
      "and \n",
      "base\n",
      "CONTACT\n",
      "+880\n",
      "1518445128\n",
      "Email:\n",
      "samratabduljalil21@gmail.com\n",
      "LinkedIn:\n",
      "linkedin.com/samrat-abdul-jalil\n",
      "GitHub:\n",
      "github.com/samratabduljalil\n",
      "Kaggle:\n",
      "kaggle/samratabduljalil\n",
      "Leetcode:\n",
      "leetcode/Samrat_Abdul_Jalil\n",
      "Badda,\n",
      "Dhaka.\n",
      "AWARD\n",
      "\n",
      "Kaggle\n",
      "Expert\n",
      "\n",
      "2024\n",
      "\n",
      "Finalist\n",
      "in\n",
      "Bhashamul\n",
      ":\n",
      "Bengali\n",
      "Regional\n",
      "IPA\n",
      "Transcription\n",
      "\n",
      "2024\n",
      "\n",
      "Top\n",
      "10\n",
      "Finalist\n",
      "in\n",
      "Bhasha\n",
      "Bichitra\n",
      ":\n",
      "ASR\n",
      "for\n",
      "Regional\n",
      "Dialects\n",
      "\n",
      "2024\n",
      "\n",
      "Top\n",
      "10\n",
      "Finalist\n",
      "in\n",
      "DL\n",
      "Enigma\n",
      "1.0\n",
      "\n",
      "SUST\n",
      "CSE\n",
      "Carnival\n",
      "2024\n",
      "\n",
      "8th\n",
      "in\n",
      "ML\n",
      "Olympiad\n",
      "-\n",
      "CO2\n",
      "Emissions\n",
      "Prediction\n",
      "Challenge\n",
      "\n",
      "2024\n",
      "\n",
      "Champion\n",
      "in\n",
      "Winner\n",
      "Career\n",
      "Launchpad\n",
      "Quiz\n",
      "competition\n",
      "\n",
      "2023\n",
      "\n",
      "Finalist\n",
      "in\n",
      "DL\n",
      "Sprint\n",
      "2.0\n",
      "-\n",
      "BUET\n",
      "CSE\n",
      "Fest\n",
      "\n",
      "2023\n",
      "\n",
      "Top\n",
      "10\n",
      "Finalist\n",
      "in\n",
      "Skitto\n",
      "Hackathon\n",
      "\n",
      "2022\n",
      "\n",
      "Champion\n",
      "in\n",
      "UIU\n",
      "Online\n",
      "Cisco\n",
      "Quiz\n",
      "Competition\n",
      "Organized\n",
      "by\n",
      "GP\n",
      "Academy\n",
      "\n",
      "2022\n",
      "\n",
      "Champion\n",
      "in\n",
      "UIU\n",
      "AI\n",
      "Contes\n",
      "t\n",
      "\n",
      "2022\n",
      "\n",
      "Champion\n",
      "in\n",
      "Programming\n",
      "for\n",
      "beginners\n",
      "\n",
      "2021\n",
      "EDUCATION\n",
      "B.Sc.\n",
      "in\n",
      "Computer\n",
      "Science\n",
      "and\n",
      "Engineering\n",
      "United\n",
      "International\n",
      "University\n",
      "July\n",
      "2020\n",
      "-\n",
      "Present\n",
      "Major\n",
      "In\n",
      ":\n",
      "Data\n",
      "Science \n",
      "Current\n",
      "CGPA:\n",
      "3.30DL\n",
      "Enigma\n",
      "1.0\n",
      ":\n",
      "Object\n",
      "Detection\n",
      "for\n",
      "Bangladeshi\n",
      "Autonomous\n",
      "Driving\n",
      "\n",
      "Developed\n",
      "model\n",
      "by\n",
      "ne-tuning\n",
      "YOLO\n",
      "and\n",
      "Detectron\n",
      "models\n",
      "for\n",
      "object\n",
      "detection\n",
      "in\n",
      "autonomous\n",
      "driving\n",
      "applications\n",
      "in\n",
      "Bangladesh.\n",
      "Addressed\n",
      "challenges\n",
      "with\n",
      "unique\n",
      "objects\n",
      "such\n",
      "as\n",
      "three-wheelers\n",
      "and\n",
      "wheelchairs\n",
      "to\n",
      "enhance\n",
      "detection\n",
      "accuracy\n",
      "\n",
      "Tech\n",
      "Stack\n",
      ":\n",
      "Pytorch,\n",
      "Yolov8,\n",
      "Detectron2\n",
      ".\n",
      "Undergraduate\n",
      "project:\n",
      "AgriCareHub\n",
      "\n",
      "Developed\n",
      "a\n",
      "web\n",
      "application\n",
      "that\n",
      "allows\n",
      "farmers\n",
      "to\n",
      "upload \n",
      "images\n",
      "of\n",
      "their\n",
      "crops\n",
      "to\n",
      "check\n",
      "for\n",
      "diseases.\n",
      "The\n",
      "app\n",
      "uses\n",
      "image \n",
      "recognition\n",
      "to\n",
      "analyze\n",
      "crop\n",
      "health\n",
      "and,\n",
      "if\n",
      "a\n",
      "disease\n",
      "is\n",
      "detected, \n",
      "recommends\n",
      "medication\n",
      "and\n",
      "proper\n",
      "treatment\n",
      "in\n",
      "Bengali.\n",
      "It \n",
      "also\n",
      "provides\n",
      "audio\n",
      "instructions\n",
      "on\n",
      "how\n",
      "to\n",
      "apply\n",
      "the \n",
      "medication.. \n",
      "\n",
      "Tech\n",
      "Stack\n",
      ":\n",
      "Python,\n",
      "FastAPI,\n",
      "TensorFlow,\n",
      "Express,\n",
      "React.js, \n",
      "MySQL\n",
      ".\n",
      "ML\n",
      "Olympiad\n",
      ":\n",
      "CO2\n",
      "Emissions\n",
      "Prediction\n",
      "Challenge\n",
      "\n",
      "Developed\n",
      "and\n",
      "evaluated\n",
      "various\n",
      "forecasting\n",
      "models\n",
      "for\n",
      "CO2 \n",
      "emissions\n",
      "prediction.\n",
      "After\n",
      "thorough\n",
      "analysis,\n",
      "ARIMA,\n",
      "LSTM, \n",
      "and\n",
      "Logistic\n",
      "Regression\n",
      "models\n",
      "demonstrated\n",
      "the\n",
      "best \n",
      "performance\n",
      "in\n",
      "accurately\n",
      "predicting\n",
      "emissions\n",
      "trends. \n",
      "\n",
      "Tech\n",
      "Stack\n",
      ":\n",
      "ARIMA,\n",
      "SARIMA,\n",
      "LSTM,\n",
      "Logistic \n",
      "Regression,Facebook\n",
      "Prophet,\n",
      "pandas\n",
      "Undergraduate\n",
      "project:\n",
      "Learn2Resume\n",
      "\n",
      "Developed\n",
      "a\n",
      "CV\n",
      "generator\n",
      "and\n",
      "e-learning\n",
      "platform\n",
      "allowing \n",
      "users\n",
      "to\n",
      "create\n",
      "and\n",
      "share\n",
      "CVs,\n",
      "purchase\n",
      "certication\n",
      "courses, \n",
      "and\n",
      "download\n",
      "certicates.\n",
      "Integrated\n",
      "live\n",
      "chat\n",
      "support,\n",
      "secure \n",
      "online\n",
      "payments,\n",
      "and\n",
      "implemented\n",
      "features\n",
      "for\n",
      "course\n",
      "and\n",
      "user \n",
      "management. \n",
      "\n",
      "Tech\n",
      "Stack\n",
      ":\n",
      "PHP,\n",
      "JavaScript,\n",
      "MySQL,\n",
      "HTML,\n",
      "CSS\n",
      ". \n",
      ".\n",
      "Undergraduate\n",
      "project:\n",
      "easy-Publication\n",
      "\n",
      "Developed\n",
      "a\n",
      "self-publishing\n",
      "platform\n",
      "enabling\n",
      "authors\n",
      "to\n",
      "easily \n",
      "publish\n",
      "their\n",
      "work\n",
      "with\n",
      "exible\n",
      "terms,\n",
      "revenue\n",
      "sharing,\n",
      "and \n",
      "24-hour\n",
      "delivery\n",
      "through\n",
      "a\n",
      "network\n",
      "of\n",
      "64\n",
      "printing\n",
      "presses \n",
      "across\n",
      "districts.\n",
      "The\n",
      "platform\n",
      "addresses\n",
      "challenges\n",
      "faced\n",
      "by \n",
      "new\n",
      "authors\n",
      "in\n",
      "traditional\n",
      "publishing.\n",
      "Additionally,\n",
      "it\n",
      "integrates \n",
      "online\n",
      "payment\n",
      "systems\n",
      "for\n",
      "smooth\n",
      "transactions\n",
      "and\n",
      "live\n",
      "chat \n",
      "support\n",
      "for\n",
      "real-time\n",
      "author\n",
      "assistance.. \n",
      "\n",
      "Tech\n",
      "Stack\n",
      ":\n",
      "PHP\n",
      ",\n",
      "MySQL,\n",
      "Bootstrap\n",
      ",HTML,\n",
      "CSS\n",
      ".\n",
      "Undergraduate\n",
      "project:\n",
      "Medind\n",
      "\n",
      "Medind\n",
      "is\n",
      "a\n",
      "web\n",
      "application\n",
      "designed\n",
      "to\n",
      "help\n",
      "users\n",
      "nd\n",
      "the \n",
      "nearest\n",
      "medicine\n",
      "shops\n",
      "and\n",
      "check\n",
      "the\n",
      "availability\n",
      "of\n",
      "their \n",
      "desired\n",
      "medicines.\n",
      "The\n",
      "app\n",
      "also\n",
      "provides\n",
      "information\n",
      "on\n",
      "the \n",
      "operating\n",
      "hours\n",
      "of\n",
      "these\n",
      "shops,\n",
      "so\n",
      "users\n",
      "can\n",
      "plan\n",
      "their\n",
      "visit \n",
      "accordingly.\n",
      "Additionally,\n",
      "shopkeepers\n",
      "can\n",
      "upload\n",
      "and\n",
      "manage \n",
      "the\n",
      "list\n",
      "of\n",
      "medicines\n",
      "available\n",
      "at\n",
      "their\n",
      "stores. \n",
      "\n",
      "Tech\n",
      "Stack\n",
      ":\n",
      "PHP\n",
      ",\n",
      "MySQL,\n",
      "Bootstrap\n",
      ",HTML,\n",
      "CSS\n",
      ".\n",
      "COURSES\n",
      "Machine\n",
      "Learning\n",
      "|\n",
      "Digital\n",
      "Image\n",
      "Processing\n",
      "|\n",
      "Bioinformatics\n",
      "|\n",
      "Human\n",
      "Computer\n",
      "Interaction\n",
      "|\n",
      "Database\n",
      "Management\n",
      "Systems\n",
      "|\n",
      "Software\n",
      "Engineering\n",
      "|\n",
      "Data\n",
      "Structures\n",
      "and\n",
      "Algorithms\n",
      "|\n",
      "Object-Oriented\n",
      "Programming\n",
      "|\n",
      "System\n",
      "Design\n",
      "and\n",
      "Analysis\n",
      "|\n",
      "INTEREST\n",
      "Multimodal\n",
      "Large\n",
      "Language\n",
      "Model\n",
      "|\n",
      "Retrieval-Augmented\n",
      "Generation\n",
      "|\n",
      "AI\n",
      "Agent\n",
      "|\n",
      "Back-end\n",
      "development\n",
      "|\n",
      "Computer\n",
      "Vision\n",
      "|\n",
      "Natural\n",
      "Language\n",
      "Processing\n",
      "|\n",
      "Automatic\n",
      "Speech\n",
      "Recognition\n",
      "|\n"
     ]
    }
   ],
   "source": [
    "query_text = \"edusoft\"\n",
    "retrieved_chunks = retrieve_pdf_embeddings(query_text)\n",
    "\n",
    "print(\"Retrieved Chunks:\")\n",
    "for chunk in retrieved_chunks:\n",
    "    print(chunk.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'machine_learning.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 99\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m     98\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmachine_learning.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 99\u001b[0m \u001b[43mstore_pdf_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m query_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkaggle\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m retrieved_chunks \u001b[38;5;241m=\u001b[39m retrieve_pdf_embeddings(query_text)\n",
      "Cell \u001b[1;32mIn[38], line 84\u001b[0m, in \u001b[0;36mstore_pdf_embeddings\u001b[1;34m(pdf_path)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstore_pdf_embeddings\u001b[39m(pdf_path):\n\u001b[0;32m     81\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    Store PDF text chunks and their embeddings in ChromaDB using LangChain.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[43msplit_pdf_into_chunks_with_overlap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     documents \u001b[38;5;241m=\u001b[39m [Document(page_content\u001b[38;5;241m=\u001b[39mchunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks]\n\u001b[0;32m     86\u001b[0m     vectorstore\u001b[38;5;241m.\u001b[39madd_documents(documents)\n",
      "Cell \u001b[1;32mIn[38], line 61\u001b[0m, in \u001b[0;36msplit_pdf_into_chunks_with_overlap\u001b[1;34m(pdf_path, chunk_size, overlap_size)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_pdf_into_chunks_with_overlap\u001b[39m(pdf_path, chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, overlap_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m    Split a PDF into overlapping chunks using LangChain's CharacterTextSplitter.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     62\u001b[0m         reader \u001b[38;5;241m=\u001b[39m PyPDF2\u001b[38;5;241m.\u001b[39mPdfReader(f)\n\u001b[0;32m     63\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'machine_learning.pdf'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import PyPDF2\n",
    "import chromadb\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Ollama server details\n",
    "OLLAMA_URL = \"http://127.0.0.1:12345\"\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Collection name in ChromaDB\n",
    "collection_name = \"pdf_embedding7\"\n",
    "#client.delete_collection(collection_name)\n",
    "# Create or load the collection safely\n",
    "if collection_name not in client.list_collections():\n",
    "    collection = client.create_collection(collection_name)\n",
    "else:\n",
    "    collection = client.get_collection(collection_name)\n",
    "\n",
    "# LangChain embedding class for Nomic model\n",
    "class NomicEmbedding(Embeddings):\n",
    "    def __init__(self, model_url):\n",
    "        self.model_url = model_url\n",
    "    \n",
    "    def embed_documents(self, texts):\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            embedding = self.get_embedding(text)\n",
    "            if embedding:\n",
    "                embeddings.append(embedding)\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.get_embedding(text)\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        payload = {\n",
    "            \"model\": \"nomic-embed-text\",\n",
    "            \"prompt\": text,\n",
    "            \"embed\": True\n",
    "        }\n",
    "        response = requests.post(f\"{self.model_url}/api/embeddings\", json=payload)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"embedding\")\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {response.text}\")\n",
    "            return None\n",
    "\n",
    "# Initialize Nomic embedding and LangChain's Chroma vector store\n",
    "nomic_embedding = NomicEmbedding(OLLAMA_URL)\n",
    "vectorstore = Chroma(collection_name=collection_name, embedding_function=nomic_embedding, client=client)\n",
    "\n",
    "def split_pdf_into_chunks_with_overlap(pdf_path, chunk_size=500, overlap_size=100):\n",
    "    \"\"\"\n",
    "    Split a PDF into overlapping chunks using LangChain's CharacterTextSplitter.\n",
    "    \"\"\"\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "\n",
    "        # Initialize LangChain's CharacterTextSplitter with overlap\n",
    "        text_splitter = CharacterTextSplitter(\n",
    "            chunk_size=chunk_size, \n",
    "            chunk_overlap=overlap_size,\n",
    "            length_function=len\n",
    "        )\n",
    "        \n",
    "        # Split the full text into chunks\n",
    "        chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def store_pdf_embeddings(pdf_path):\n",
    "    \"\"\"\n",
    "    Store PDF text chunks and their embeddings in ChromaDB using LangChain.\n",
    "    \"\"\"\n",
    "    chunks = split_pdf_into_chunks_with_overlap(pdf_path)\n",
    "    documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "    vectorstore.add_documents(documents)\n",
    "    print(f\"Stored {len(documents)} document chunks with overlap.\")\n",
    "\n",
    "def retrieve_pdf_embeddings(query_text):\n",
    "    \"\"\"\n",
    "    Retrieve similar document chunks based on a query text using LangChain's Chroma vector store.\n",
    "    \"\"\"\n",
    "    query_embedding = nomic_embedding.embed_query(query_text)\n",
    "    results = vectorstore.similarity_search_by_vector(query_embedding, k=3)  # Retrieve top 3 most similar chunks\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "pdf_path = \"machine_learning.pdf\"\n",
    "store_pdf_embeddings(pdf_path)\n",
    "\n",
    "query_text = \"kaggle\"\n",
    "retrieved_chunks = retrieve_pdf_embeddings(query_text)\n",
    "\n",
    "print(\"Retrieved Chunks:\")\n",
    "for chunk in retrieved_chunks:\n",
    "    print(chunk.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UniqueConstraintError",
     "evalue": "Collection pdf_embedding13 already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUniqueConstraintError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Create or load the collection\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collection_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m client\u001b[38;5;241m.\u001b[39mlist_collections():\n\u001b[1;32m---> 21\u001b[0m     collection \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     collection \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mget_collection(collection_name)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\chromadb\\api\\client.py:147\u001b[0m, in \u001b[0;36mClient.create_collection\u001b[1;34m(self, name, configuration, metadata, embedding_function, data_loader, get_or_create)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_collection\u001b[39m(\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    145\u001b[0m     get_or_create: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    146\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Collection:\n\u001b[1;32m--> 147\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_or_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_or_create\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Collection(\n\u001b[0;32m    156\u001b[0m         client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server,\n\u001b[0;32m    157\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    158\u001b[0m         embedding_function\u001b[38;5;241m=\u001b[39membedding_function,\n\u001b[0;32m    159\u001b[0m         data_loader\u001b[38;5;241m=\u001b[39mdata_loader,\n\u001b[0;32m    160\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\chromadb\\telemetry\\opentelemetry\\__init__.py:150\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[1;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\chromadb\\api\\segment.py:103\u001b[0m, in \u001b[0;36mrate_limit.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rate_limit_enforcer\u001b[38;5;241m.\u001b[39mrate_limit(func)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\chromadb\\rate_limit\\simple_rate_limit\\__init__.py:23\u001b[0m, in \u001b[0;36mSimpleRateLimitEnforcer.rate_limit.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\chromadb\\api\\segment.py:226\u001b[0m, in \u001b[0;36mSegmentAPI.create_collection\u001b[1;34m(self, name, configuration, metadata, get_or_create, tenant, database)\u001b[0m\n\u001b[0;32m    213\u001b[0m model \u001b[38;5;241m=\u001b[39m CollectionModel(\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m,\n\u001b[0;32m    215\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m     dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    223\u001b[0m )\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# TODO: Let sysdb create the collection directly from the model\u001b[39;00m\n\u001b[1;32m--> 226\u001b[0m coll, created \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sysdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_configuration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43msegments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Passing empty till backend changes are deployed.\u001b[39;49;00m\n\u001b[0;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdimension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# This is lazily populated on the first add\u001b[39;49;00m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_or_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_or_create\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created:\n\u001b[0;32m    239\u001b[0m     segments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39mprepare_segments_for_new_collection(coll)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\chromadb\\telemetry\\opentelemetry\\__init__.py:150\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[1;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\chromadb\\db\\mixins\\sysdb.py:241\u001b[0m, in \u001b[0;36mSqlSysDB.create_collection\u001b[1;34m(self, id, name, configuration, segments, metadata, dimension, get_or_create, tenant, database)\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    235\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_collections(\n\u001b[0;32m    236\u001b[0m                 \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mcollection\u001b[38;5;241m.\u001b[39mid, tenant\u001b[38;5;241m=\u001b[39mtenant, database\u001b[38;5;241m=\u001b[39mdatabase\n\u001b[0;32m    237\u001b[0m             )[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    238\u001b[0m             \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    239\u001b[0m         )\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 241\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UniqueConstraintError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollection \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    243\u001b[0m collection \u001b[38;5;241m=\u001b[39m Collection(\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m,\n\u001b[0;32m    245\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    251\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    252\u001b[0m )\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtx() \u001b[38;5;28;01mas\u001b[39;00m cur:\n",
      "\u001b[1;31mUniqueConstraintError\u001b[0m: Collection pdf_embedding13 already exists"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import PyPDF2\n",
    "import chromadb\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "# Ollama server details\n",
    "OLLAMA_URL = \"http://127.0.0.1:12345\"\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Collection name in ChromaDB\n",
    "collection_name = \"pdf_embedding13\"\n",
    "\n",
    "# Create or load the collection\n",
    "if collection_name not in client.list_collections():\n",
    "    collection = client.create_collection(collection_name)\n",
    "else:\n",
    "    collection = client.get_collection(collection_name)\n",
    "\n",
    "# LangChain embedding class for Nomic model\n",
    "class NomicEmbedding(Embeddings):\n",
    "    def __init__(self, model_url):\n",
    "        self.model_url = model_url\n",
    "    \n",
    "    def embed_documents(self, texts):\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            embedding = self.get_embedding(text)\n",
    "            if embedding:\n",
    "                embeddings.append(embedding)\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.get_embedding(text)\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        payload = {\n",
    "            \"model\": \"nomic-embed-text\",\n",
    "            \"prompt\": text,\n",
    "            \"embed\": True\n",
    "        }\n",
    "        response = requests.post(f\"{self.model_url}/api/embeddings\", json=payload)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"embedding\")\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {response.text}\")\n",
    "            return None\n",
    "\n",
    "# Initialize Nomic embedding and LangChain's Chroma vector store\n",
    "nomic_embedding = NomicEmbedding(OLLAMA_URL)\n",
    "vectorstore = Chroma(collection_name=collection_name, embedding_function=nomic_embedding, client=client)\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "ollama_llm = Ollama(\n",
    "    model=\"llama3\",\n",
    "    base_url=OLLAMA_URL\n",
    ")\n",
    "\n",
    "def split_pdf_into_chunks_with_overlap(pdf_path, chunk_size=500, overlap_size=50):\n",
    "    \"\"\"\n",
    "    Split a PDF into overlapping chunks using LangChain's CharacterTextSplitter.\n",
    "    \"\"\"\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "\n",
    "        # Initialize LangChain's CharacterTextSplitter with overlap\n",
    "        text_splitter = CharacterTextSplitter(\n",
    "            chunk_size=chunk_size, \n",
    "            chunk_overlap=overlap_size,\n",
    "            length_function=len\n",
    "        )\n",
    "        \n",
    "        # Split the full text into chunks\n",
    "        chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def store_pdf_embeddings(pdf_path):\n",
    "    \"\"\"\n",
    "    Store PDF text chunks and their embeddings in ChromaDB using LangChain.\n",
    "    \"\"\"\n",
    "    chunks = split_pdf_into_chunks_with_overlap(pdf_path)\n",
    "    documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "    vectorstore.add_documents(documents)\n",
    "    print(f\"Stored {len(documents)} document chunks with overlap.\")\n",
    "\n",
    "def retrieve_pdf_embeddings(query_text):\n",
    "    \"\"\"\n",
    "    Retrieve similar document chunks based on a query text using LangChain's Chroma vector store.\n",
    "    \"\"\"\n",
    "    query_embedding = nomic_embedding.embed_query(query_text)\n",
    "    results = vectorstore.similarity_search_by_vector(query_embedding, k=3)  # Retrieve top 3 most similar chunks\n",
    "    return results\n",
    "\n",
    "def generate_answer_with_retrieved_chunks(query_text):\n",
    "    \"\"\"\n",
    "    Generate an answer using the retrieved document chunks and the Ollama model.\n",
    "    \"\"\"\n",
    "    # Retrieve similar chunks from Chroma\n",
    "    retrieved_chunks = retrieve_pdf_embeddings(query_text)\n",
    "\n",
    "    # Combine the content of the retrieved chunks\n",
    "    relevant_text = \"\\n\".join([chunk.page_content for chunk in retrieved_chunks])\n",
    "    \n",
    "    # Use the retrieved content and the query as a prompt for the LLM\n",
    "    prompt = f\"Here are some relevant excerpts from the document:\\n\\n{relevant_text}\\n\\nBased on this, answer the following query:\\n{query_text}\"\n",
    "\n",
    "    # Generate the answer using the LLM\n",
    "    response = ollama_llm(prompt)\n",
    "    return response\n",
    "\n",
    "# Example usage:\n",
    "pdf_path = \"machine learning.pdf\"\n",
    "store_pdf_embeddings(pdf_path)\n",
    "\n",
    "query_text = \"which year samrat became kaggle expert?\"\n",
    "answer = generate_answer_with_retrieved_chunks(query_text)\n",
    "\n",
    "print(\"Generated Answer:\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samra\\Downloads\\DataMate\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: machine_learning.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "pdf_path = \"machine_learning.pdf\"\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(f\"File not found: {pdf_path}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading PDF: Unable to get page count. Is poppler installed and in PATH?\n",
      "Failed to load the first page of the PDF.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "def load_first_page_pdf(pdf_path):\n",
    "    \"\"\"Loads only the first page of a PDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        A string containing the text content of the first page, or None if an error occurs or the PDF has no pages.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        loader = UnstructuredPDFLoader(pdf_path)\n",
    "        documents = loader.load()\n",
    "\n",
    "        if documents: # Check if any documents were loaded\n",
    "          return documents[0].page_content  # Access the first document's content\n",
    "        else:\n",
    "          print(f\"PDF {pdf_path} appears to be empty or has no pages.\")\n",
    "          return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "pdf_file = \"Downloads/DataMate/machine learning.pdf\"  # Replace with your PDF file path\n",
    "first_page_text = load_first_page_pdf(pdf_file)\n",
    "\n",
    "if first_page_text:\n",
    "    print(\"First Page Content:\")\n",
    "    print(first_page_text)\n",
    "else:\n",
    "    print(\"Failed to load the first page of the PDF.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unstructured\n",
      "  Using cached unstructured-0.16.11-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting chardet (from unstructured)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting filetype (from unstructured)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-magic (from unstructured)\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting lxml (from unstructured)\n",
      "  Using cached lxml-5.3.0-cp310-cp310-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting nltk (from unstructured)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured) (4.12.3)\n",
      "Collecting emoji (from unstructured)\n",
      "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured) (0.6.7)\n",
      "Collecting python-iso639 (from unstructured)\n",
      "  Downloading python_iso639-2024.10.22-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langdetect (from unstructured)\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 262.1/981.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 981.5/981.5 kB 4.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured) (1.26.4)\n",
      "Collecting rapidfuzz (from unstructured)\n",
      "  Downloading rapidfuzz-3.11.0-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: backoff in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured) (4.12.2)\n",
      "Collecting unstructured-client (from unstructured)\n",
      "  Downloading unstructured_client-0.28.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured) (1.17.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured) (5.9.0)\n",
      "Collecting python-oxmsg (from unstructured)\n",
      "  Downloading python_oxmsg-0.0.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting html5lib (from unstructured)\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from beautifulsoup4->unstructured) (2.5)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from dataclasses-json->unstructured) (3.23.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from html5lib->unstructured) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from html5lib->unstructured) (0.5.1)\n",
      "Requirement already satisfied: click in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from nltk->unstructured) (8.1.8)\n",
      "Collecting joblib (from nltk->unstructured)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk->unstructured)\n",
      "  Using cached regex-2024.11.6-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Collecting olefile (from python-oxmsg->unstructured)\n",
      "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from requests->unstructured) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from requests->unstructured) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from requests->unstructured) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from requests->unstructured) (2024.12.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from tqdm->unstructured) (0.4.6)\n",
      "Collecting aiofiles>=24.1.0 (from unstructured-client->unstructured)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting cryptography>=3.1 (from unstructured-client->unstructured)\n",
      "  Downloading cryptography-44.0.0-cp39-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting eval-type-backport<0.3.0,>=0.2.0 (from unstructured-client->unstructured)\n",
      "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured-client->unstructured) (0.27.0)\n",
      "Collecting jsonpath-python<2.0.0,>=1.0.6 (from unstructured-client->unstructured)\n",
      "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured-client->unstructured) (1.6.0)\n",
      "Collecting pydantic<2.10.0,>=2.9.2 (from unstructured-client->unstructured)\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Collecting pypdf>=4.0 (from unstructured-client->unstructured)\n",
      "  Using cached pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured-client->unstructured) (2.9.0.post0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from pydantic<2.10.0,>=2.9.2->unstructured-client->unstructured) (0.7.0)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic<2.10.0,>=2.9.2->unstructured-client->unstructured)\n",
      "  Using cached pydantic_core-2.23.4-cp310-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.21)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.2.0)\n",
      "Downloading unstructured-0.16.11-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 0.8/1.7 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.0/1.7 MB 2.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.3/1.7 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 2.2 MB/s eta 0:00:00\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
      "   ---------------------------------------- 0.0/586.9 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 262.1/586.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 586.9/586.9 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Using cached lxml-5.3.0-cp310-cp310-win_amd64.whl (3.8 MB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.5 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.0/1.5 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading python_iso639-2024.10.22-py3-none-any.whl (274 kB)\n",
      "Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Downloading python_oxmsg-0.0.1-py3-none-any.whl (31 kB)\n",
      "Downloading rapidfuzz-3.11.0-cp310-cp310-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.5/1.6 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.0/1.6 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.3/1.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 1.8 MB/s eta 0:00:00\n",
      "Downloading unstructured_client-0.28.1-py3-none-any.whl (62 kB)\n",
      "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading cryptography-44.0.0-cp39-abi3-win_amd64.whl (3.2 MB)\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.8/3.2 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.0/3.2 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.6/3.2 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.8/3.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.4/3.2 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.6/3.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.1/3.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.2/3.2 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
      "Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Using cached pydantic_core-2.23.4-cp310-none-win_amd64.whl (1.9 MB)\n",
      "Using cached pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "Using cached regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993251 sha256=4cd7e87c9341c81aa4dc9e5b7667f2f2b72109613fed5e58775ec2e831a61502\n",
      "  Stored in directory: c:\\users\\samra\\appdata\\local\\pip\\cache\\wheels\\95\\03\\7d\\59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
      "Successfully built langdetect\n",
      "Installing collected packages: filetype, regex, rapidfuzz, python-magic, python-iso639, pypdf, pydantic-core, olefile, lxml, langdetect, jsonpath-python, joblib, html5lib, eval-type-backport, emoji, chardet, aiofiles, python-oxmsg, pydantic, nltk, cryptography, unstructured-client, unstructured\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.27.2\n",
      "    Uninstalling pydantic_core-2.27.2:\n",
      "      Successfully uninstalled pydantic_core-2.27.2\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.10.4\n",
      "    Uninstalling pydantic-2.10.4:\n",
      "      Successfully uninstalled pydantic-2.10.4\n",
      "Successfully installed aiofiles-24.1.0 chardet-5.2.0 cryptography-44.0.0 emoji-2.14.0 eval-type-backport-0.2.2 filetype-1.2.0 html5lib-1.1 joblib-1.4.2 jsonpath-python-1.0.6 langdetect-1.0.9 lxml-5.3.0 nltk-3.9.1 olefile-0.47 pydantic-2.9.2 pydantic-core-2.23.4 pypdf-5.1.0 python-iso639-2024.10.22 python-magic-0.4.27 python-oxmsg-0.0.1 rapidfuzz-3.11.0 regex-2024.11.6 unstructured-0.16.11 unstructured-client-0.28.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\samra\\anaconda3\\envs\\localrag3.0\\Lib\\site-packages\\~ydantic_core'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall pdfminer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pi_heif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unstructured[all-docs] in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (0.16.11)\n",
      "Requirement already satisfied: chardet in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured[all-docs]) (5.2.0)\n",
      "Requirement already satisfied: filetype in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured[all-docs]) (1.2.0)\n",
      "Requirement already satisfied: python-magic in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured[all-docs]) (0.4.27)\n",
      "Requirement already satisfied: lxml in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured[all-docs]) (5.3.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured[all-docs]) (3.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured[all-docs]) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured[all-docs]) (4.12.3)\n",
      "Requirement already satisfied: emoji in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured[all-docs]) (2.14.0)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured[all-docs]) (0.6.7)\n",
      "Requirement already satisfied: python-iso639 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured[all-docs]) (2024.10.22)\n",
      "Requirement already satisfied: langdetect in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured[all-docs]) (1.0.9)\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured[all-docs]) (1.26.4)\n",
      "Requirement already satisfied: rapidfuzz in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured[all-docs]) (3.11.0)\n",
      "Requirement already satisfied: backoff in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured[all-docs]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured[all-docs]) (4.12.2)\n",
      "Requirement already satisfied: unstructured-client in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured[all-docs]) (0.28.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured[all-docs]) (1.17.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured[all-docs]) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured[all-docs]) (5.9.0)\n",
      "Requirement already satisfied: python-oxmsg in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured[all-docs]) (0.0.1)\n",
      "Requirement already satisfied: html5lib in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured[all-docs]) (1.1)\n",
      "Collecting networkx (from unstructured[all-docs])\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting unstructured-inference==0.8.1 (from unstructured[all-docs])\n",
      "  Downloading unstructured_inference-0.8.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting google-cloud-vision (from unstructured[all-docs])\n",
      "  Downloading google_cloud_vision-3.9.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pi-heif in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured[all-docs]) (0.21.0)\n",
      "Collecting python-pptx>=1.0.1 (from unstructured[all-docs])\n",
      "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting pikepdf (from unstructured[all-docs])\n",
      "  Downloading pikepdf-9.4.2-cp310-cp310-win_amd64.whl.metadata (8.4 kB)\n",
      "Collecting markdown (from unstructured[all-docs])\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured[all-docs]) (2.2.3)\n",
      "Requirement already satisfied: pypdf in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured[all-docs]) (5.1.0)\n",
      "Collecting openpyxl (from unstructured[all-docs])\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: pdfminer.six in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured[all-docs]) (20240706)\n",
      "Collecting pdf2image (from unstructured[all-docs])\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting effdet (from unstructured[all-docs])\n",
      "  Downloading effdet-0.4.1-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting xlrd (from unstructured[all-docs])\n",
      "  Downloading xlrd-2.0.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting onnx (from unstructured[all-docs])\n",
      "  Downloading onnx-1.17.0-cp310-cp310-win_amd64.whl.metadata (16 kB)\n",
      "Collecting pypandoc (from unstructured[all-docs])\n",
      "  Downloading pypandoc-1.14-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting python-docx>=1.1.2 (from unstructured[all-docs])\n",
      "  Using cached python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting unstructured.pytesseract>=0.3.12 (from unstructured[all-docs])\n",
      "  Downloading unstructured.pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting layoutparser (from unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Downloading layoutparser-0.3.4-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting python-multipart (from unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured-inference==0.8.1->unstructured[all-docs]) (0.27.0)\n",
      "Collecting opencv-python!=4.7.0.68 (from unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Using cached opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: onnxruntime>=1.17.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured-inference==0.8.1->unstructured[all-docs]) (1.20.1)\n",
      "Collecting matplotlib (from unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Using cached matplotlib-3.10.0-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting torch (from unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Using cached torch-2.5.1-cp310-cp310-win_amd64.whl.metadata (28 kB)\n",
      "Collecting timm (from unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Downloading timm-1.0.12-py3-none-any.whl.metadata (51 kB)\n",
      "Collecting transformers>=4.25.1 (from unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Using cached transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from python-pptx>=1.0.1->unstructured[all-docs]) (11.0.0)\n",
      "Collecting XlsxWriter>=0.5.7 (from python-pptx>=1.0.1->unstructured[all-docs])\n",
      "  Using cached XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured.pytesseract>=0.3.12->unstructured[all-docs]) (24.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from beautifulsoup4->unstructured[all-docs]) (2.5)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from dataclasses-json->unstructured[all-docs]) (3.23.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from dataclasses-json->unstructured[all-docs]) (0.9.0)\n",
      "Collecting torchvision (from effdet->unstructured[all-docs])\n",
      "  Using cached torchvision-0.20.1-cp310-cp310-win_amd64.whl.metadata (6.2 kB)\n",
      "Collecting pycocotools>=2.0.2 (from effdet->unstructured[all-docs])\n",
      "  Downloading pycocotools-2.0.8-cp310-cp310-win_amd64.whl.metadata (1.1 kB)\n",
      "Collecting omegaconf>=2.0 (from effdet->unstructured[all-docs])\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs])\n",
      "  Downloading google_api_core-2.24.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from google-cloud-vision->unstructured[all-docs]) (2.37.0)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-cloud-vision->unstructured[all-docs])\n",
      "  Downloading proto_plus-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from google-cloud-vision->unstructured[all-docs]) (5.29.2)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from html5lib->unstructured[all-docs]) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from html5lib->unstructured[all-docs]) (0.5.1)\n",
      "Requirement already satisfied: click in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from nltk->unstructured[all-docs]) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from nltk->unstructured[all-docs]) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from nltk->unstructured[all-docs]) (2024.11.6)\n",
      "Collecting et-xmlfile (from openpyxl->unstructured[all-docs])\n",
      "  Using cached et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from pandas->unstructured[all-docs]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from pandas->unstructured[all-docs]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from pandas->unstructured[all-docs]) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from pdfminer.six->unstructured[all-docs]) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from pdfminer.six->unstructured[all-docs]) (44.0.0)\n",
      "Requirement already satisfied: Deprecated in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from pikepdf->unstructured[all-docs]) (1.2.15)\n",
      "Requirement already satisfied: olefile in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from python-oxmsg->unstructured[all-docs]) (0.47)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from requests->unstructured[all-docs]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from requests->unstructured[all-docs]) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from requests->unstructured[all-docs]) (2024.12.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from tqdm->unstructured[all-docs]) (0.4.6)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured-client->unstructured[all-docs]) (24.1.0)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured-client->unstructured[all-docs]) (0.2.2)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured-client->unstructured[all-docs]) (0.27.0)\n",
      "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured-client->unstructured[all-docs]) (1.0.6)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured-client->unstructured[all-docs]) (1.6.0)\n",
      "Requirement already satisfied: pydantic<2.10.0,>=2.9.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured-client->unstructured[all-docs]) (2.9.2)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured-client->unstructured[all-docs]) (1.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]) (1.17.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.66.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.68.1)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs])\n",
      "  Downloading grpcio_status-1.68.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (4.9)\n",
      "Requirement already satisfied: anyio in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (4.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (0.14.0)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->unstructured[all-docs])\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from omegaconf>=2.0->effdet->unstructured[all-docs]) (6.0.2)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (24.12.23)\n",
      "Requirement already satisfied: sympy in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (1.13.3)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Using cached contourpy-1.3.1-cp310-cp310-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Using cached fonttools-4.55.3-cp310-cp310-win_amd64.whl.metadata (168 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Using cached pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from pydantic<2.10.0,>=2.9.2->unstructured-client->unstructured[all-docs]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from pydantic<2.10.0,>=2.9.2->unstructured-client->unstructured[all-docs]) (2.23.4)\n",
      "Collecting safetensors (from timm->unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Using cached safetensors-0.4.5-cp310-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]) (3.16.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]) (2024.12.0)\n",
      "Collecting sympy (from onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from sympy->onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (1.3.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers>=4.25.1->unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[all-docs]) (1.0.0)\n",
      "Collecting scipy (from layoutparser->unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Using cached scipy-1.14.1-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting iopath (from layoutparser->unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pdfplumber (from layoutparser->unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]) (2.21)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.6.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (1.2.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from coloredlogs->onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (10.0)\n",
      "Collecting portalocker (from iopath->layoutparser->unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from jinja2->torch->unstructured-inference==0.8.1->unstructured[all-docs]) (2.1.3)\n",
      "Collecting pdfminer.six (from unstructured[all-docs])\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber->layoutparser->unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Downloading pypdfium2-4.30.1-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (3.5.4)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from portalocker->iopath->layoutparser->unstructured-inference==0.8.1->unstructured[all-docs]) (305.1)\n",
      "Downloading unstructured_inference-0.8.1-py3-none-any.whl (48 kB)\n",
      "Using cached python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "Downloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
      "Downloading unstructured.pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
      "Downloading google_cloud_vision-3.9.0-py2.py3-none-any.whl (514 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Downloading onnx-1.17.0-cp310-cp310-win_amd64.whl (14.5 MB)\n",
      "   ---------------------------------------- 0.0/14.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/14.5 MB 2.1 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.8/14.5 MB 2.0 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.8/14.5 MB 2.0 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 1.0/14.5 MB 1.1 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 2.1/14.5 MB 1.8 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 2.6/14.5 MB 1.9 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 2.9/14.5 MB 1.9 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 2.9/14.5 MB 1.9 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 3.1/14.5 MB 1.6 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 3.9/14.5 MB 1.8 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 4.5/14.5 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 4.7/14.5 MB 1.9 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 5.2/14.5 MB 1.8 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 5.5/14.5 MB 1.8 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 6.0/14.5 MB 1.8 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 6.3/14.5 MB 1.8 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 6.8/14.5 MB 1.8 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 7.1/14.5 MB 1.8 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 7.1/14.5 MB 1.8 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 7.3/14.5 MB 1.7 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 8.4/14.5 MB 1.8 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 8.7/14.5 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 9.2/14.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 9.4/14.5 MB 1.8 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 10.0/14.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 10.2/14.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 10.7/14.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 11.0/14.5 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 11.5/14.5 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 11.8/14.5 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 12.1/14.5 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 12.6/14.5 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 12.8/14.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 13.4/14.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.6/14.5 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 14.2/14.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.5/14.5 MB 1.8 MB/s eta 0:00:00\n",
      "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Downloading pikepdf-9.4.2-cp310-cp310-win_amd64.whl (3.5 MB)\n",
      "   ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.5/3.5 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.0/3.5 MB 2.4 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 1.3/3.5 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.6/3.5 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 2.1/3.5 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 2.4/3.5 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.9/3.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.1/3.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.5/3.5 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading pypandoc-1.14-py3-none-any.whl (21 kB)\n",
      "Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
      "Downloading google_api_core-2.24.0-py3-none-any.whl (158 kB)\n",
      "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Using cached opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "Downloading proto_plus-1.25.0-py3-none-any.whl (50 kB)\n",
      "Downloading pycocotools-2.0.8-cp310-cp310-win_amd64.whl (84 kB)\n",
      "Using cached matplotlib-3.10.0-cp310-cp310-win_amd64.whl (8.0 MB)\n",
      "Downloading timm-1.0.12-py3-none-any.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.8/2.4 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.6/2.4 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.8/2.4 MB 2.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.1/2.4 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 2.1 MB/s eta 0:00:00\n",
      "Using cached torch-2.5.1-cp310-cp310-win_amd64.whl (203.1 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Downloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "   ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.8/10.1 MB 4.2 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.0/10.1 MB 3.1 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.3/10.1 MB 2.6 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.8/10.1 MB 2.3 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.1/10.1 MB 2.2 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.6/10.1 MB 2.1 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.9/10.1 MB 2.1 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 3.4/10.1 MB 2.0 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.7/10.1 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 4.2/10.1 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 4.5/10.1 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 5.0/10.1 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 5.2/10.1 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 5.8/10.1 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 6.0/10.1 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 6.3/10.1 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 6.8/10.1 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 7.3/10.1 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 7.6/10.1 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 7.9/10.1 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 8.4/10.1 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.7/10.1 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.2/10.1 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.4/10.1 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.1 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.1/10.1 MB 1.9 MB/s eta 0:00:00\n",
      "Using cached XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
      "Using cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Downloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
      "   ---------------------------------------- 0.0/19.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/19.2 MB 3.4 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.8/19.2 MB 2.4 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 1.0/19.2 MB 2.2 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 1.0/19.2 MB 2.2 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 1.0/19.2 MB 2.2 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 2.4/19.2 MB 1.8 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 2.9/19.2 MB 1.9 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 3.1/19.2 MB 1.9 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 3.4/19.2 MB 1.9 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 3.9/19.2 MB 1.9 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 4.2/19.2 MB 1.9 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 4.7/19.2 MB 1.9 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 5.0/19.2 MB 1.9 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 5.5/19.2 MB 1.9 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 5.8/19.2 MB 1.9 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 6.3/19.2 MB 1.9 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 6.6/19.2 MB 1.9 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 7.1/19.2 MB 1.9 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 7.3/19.2 MB 1.9 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 7.9/19.2 MB 1.9 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 8.1/19.2 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 8.7/19.2 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 8.9/19.2 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 9.2/19.2 MB 1.9 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 9.7/19.2 MB 1.9 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 10.0/19.2 MB 1.9 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 10.2/19.2 MB 1.9 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 10.2/19.2 MB 1.9 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 11.0/19.2 MB 1.8 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 11.5/19.2 MB 1.9 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 12.1/19.2 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 12.3/19.2 MB 1.9 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 12.8/19.2 MB 1.9 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 13.1/19.2 MB 1.8 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 13.6/19.2 MB 1.8 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 13.9/19.2 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 14.4/19.2 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 14.7/19.2 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 15.2/19.2 MB 1.8 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 15.5/19.2 MB 1.8 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 16.0/19.2 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 16.3/19.2 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 16.8/19.2 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 17.0/19.2 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 17.6/19.2 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 17.8/19.2 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 18.4/19.2 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 18.6/19.2 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  19.1/19.2 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 19.2/19.2 MB 1.8 MB/s eta 0:00:00\n",
      "Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Using cached torchvision-0.20.1-cp310-cp310-win_amd64.whl (1.6 MB)\n",
      "Using cached contourpy-1.3.1-cp310-cp310-win_amd64.whl (218 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.55.3-cp310-cp310-win_amd64.whl (2.2 MB)\n",
      "Downloading grpcio_status-1.68.1-py3-none-any.whl (14 kB)\n",
      "Downloading kiwisolver-1.4.8-cp310-cp310-win_amd64.whl (71 kB)\n",
      "Using cached pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "Using cached safetensors-0.4.5-cp310-none-win_amd64.whl (285 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
      "Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/5.6 MB 3.3 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.0/5.6 MB 2.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.6/5.6 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 1.8/5.6 MB 2.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.4/5.6 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 2.6/5.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.1/5.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 3.4/5.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 3.9/5.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.2/5.6 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.7/5.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.0/5.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.5/5.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 1.9 MB/s eta 0:00:00\n",
      "Using cached scipy-1.14.1-cp310-cp310-win_amd64.whl (44.8 MB)\n",
      "Downloading pypdfium2-4.30.1-py3-none-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.5/3.0 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.0/3.0 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.3/3.0 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.8/3.0 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 2.1/3.0 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.6/3.0 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
      "Building wheels for collected packages: antlr4-python3-runtime, iopath\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): started\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144577 sha256=fa9a1f958610cfac75745a41113a97e067f05d8d7d79c8aaea873422a4292a1b\n",
      "  Stored in directory: c:\\users\\samra\\appdata\\local\\pip\\cache\\wheels\\12\\93\\dd\\1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
      "  Building wheel for iopath (setup.py): started\n",
      "  Building wheel for iopath (setup.py): finished with status 'done'\n",
      "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31539 sha256=402072a68ba64b41150569b79594bb97af9ef0d97adf83ca776cef71ee382346\n",
      "  Stored in directory: c:\\users\\samra\\appdata\\local\\pip\\cache\\wheels\\9a\\a3\\b6\\ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
      "Successfully built antlr4-python3-runtime iopath\n",
      "Installing collected packages: antlr4-python3-runtime, XlsxWriter, xlrd, unstructured.pytesseract, sympy, scipy, safetensors, python-multipart, python-docx, pypdfium2, pyparsing, pypandoc, proto-plus, portalocker, pdf2image, opencv-python, onnx, omegaconf, networkx, markdown, kiwisolver, fonttools, et-xmlfile, cycler, contourpy, torch, python-pptx, pikepdf, openpyxl, matplotlib, iopath, grpcio-status, torchvision, tokenizers, pycocotools, pdfminer.six, google-api-core, transformers, timm, pdfplumber, layoutparser, google-cloud-vision, effdet, unstructured-inference\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.20.3\n",
      "    Uninstalling tokenizers-0.20.3:\n",
      "      Successfully uninstalled tokenizers-0.20.3\n",
      "  Attempting uninstall: pdfminer.six\n",
      "    Found existing installation: pdfminer.six 20240706\n",
      "    Uninstalling pdfminer.six-20240706:\n",
      "      Successfully uninstalled pdfminer.six-20240706\n",
      "Successfully installed XlsxWriter-3.2.0 antlr4-python3-runtime-4.9.3 contourpy-1.3.1 cycler-0.12.1 effdet-0.4.1 et-xmlfile-2.0.0 fonttools-4.55.3 google-api-core-2.24.0 google-cloud-vision-3.9.0 grpcio-status-1.68.1 iopath-0.1.10 kiwisolver-1.4.8 layoutparser-0.3.4 markdown-3.7 matplotlib-3.10.0 networkx-3.4.2 omegaconf-2.3.0 onnx-1.17.0 opencv-python-4.10.0.84 openpyxl-3.1.5 pdf2image-1.17.0 pdfminer.six-20231228 pdfplumber-0.11.4 pikepdf-9.4.2 portalocker-3.0.0 proto-plus-1.25.0 pycocotools-2.0.8 pypandoc-1.14 pyparsing-3.2.0 pypdfium2-4.30.1 python-docx-1.1.2 python-multipart-0.0.20 python-pptx-1.0.2 safetensors-0.4.5 scipy-1.14.1 sympy-1.13.1 timm-1.0.12 tokenizers-0.21.0 torch-2.5.1 torchvision-0.20.1 transformers-4.47.1 unstructured-inference-0.8.1 unstructured.pytesseract-0.3.13 xlrd-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chromadb 0.5.23 requires tokenizers<=0.20.3,>=0.13.2, but you have tokenizers 0.21.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install \"unstructured[all-docs]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"machine learning.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = []\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMRATABDULJALILAIandBackendEnthusiast\n",
      "\n",
      "SKILLS\n",
      "\n",
      "Back-End: Python,C/C++,Java,Php, FastAPI,Express.js,Node.jsDatabase: MySQL,MongoDB.Front-End:JavaScript,TypeScript,React.js,HTML,CSS.MachineLearning:TensorFlow,PyTorch,Keras,Scikit-learn,NumPy,Pandas,Seaborn,Matplotlib,OpenCV,MediaPipeetc.AppDevelopment:Flutter.Tools:Git,Docker,GithubAction,JIRA,VSCode,Anaconda.\n",
      "\n",
      "WORKEXPERIENCE\n",
      "\n",
      "UndergraduateThesis:GradeVision(incollaborationwithEdusoftbdLtd) DevelopedanOCR-basedapplicationtoautomateresultmanagementbycapturingresultdatafromimages.Featuresincludereal-timedataediting,updates,centralizeddatabasestorage,section-wisestatisticalviews,authentication,andOTPvericationviaemail. TechStack: Python,FastAPI,Flutter,MySQL.\n",
      "\n",
      "DLSprint2.0: BengaliDocumentLayoutAnalysis Fine-tunedYOLOandDetectronmodelstoanalyzeBengalidocumentlayouts,tacklingchallengeswithhistoricalnewspapers,governmentdocuments,magazines,andbooks.Themodelsidentiedparagraphs,textboxes,images,andtables,withDetectrondeliveringsuperiorresults TechStack: Pytorch,Detectron2,Yolo.\n",
      "\n",
      "BhashaBichitra\n",
      "\n",
      ": BengaliSpeech-to-TextforRegionalDialects Fine-tunedtheWhispermodelforAutomaticSpeechRecognition(ASR)toconvertspeechintoBengaliregionaltext.Themodelwastrainedondiversedatasetstohandlevariationsinpronunciationandvocabularyacrossdialects. TechStack: Pytorch,Whisper.\n",
      "\n",
      "Bhashamul:BengaliRegionalIPATranscription Fine-tunedGoogleByT5(smallandbaseversions)andmBART-5(baseandsmallversions)forBengaliregionalIPAtranscription.Afterthoroughevaluation,theByT5smallmodeloutperformedothers,providingsuperiortranscriptionaccuracyforregionalphoneticvariations.Thismodelimprovestranscriptionqualityforregionalspeech-to-textapplications. TechStack: Pytorch,UMT5smallandbase,byt5smallandbase\n",
      "\n",
      "CONTACT\n",
      "\n",
      "+8801518445128Email:samratabduljalil21@gmail.comLinkedIn:linkedin.com/samrat-abdul-jalilGitHub:github.com/samratabduljalilKaggle:kaggle/samratabduljalilLeetcode:leetcode/Samrat_Abdul_JalilBadda,Dhaka.\n",
      "\n",
      "AWARD\n",
      "\n",
      " KaggleExpert2024 FinalistinBhashamul:BengaliRegionalIPATranscription 2024 Top10FinalistinBhashaBichitra:ASRforRegionalDialects2024 Top10FinalistinDLEnigma1.0SUSTCSECarnival2024 8thinMLOlympiad-CO2EmissionsPredictionChallenge2024 ChampioninWinnerCareerLaunchpadQuizcompetition2023 FinalistinDLSprint2.0-BUETCSEFest2023 Top10FinalistinSkittoHackathon2022 ChampioninUIUOnlineCiscoQuizCompetitionOrganizedbyGPAcademy 2022 ChampioninUIUAIContest2022 ChampioninProgrammingforbeginners2021\n",
      "\n",
      "EDUCATIONB.Sc.inComputerScienceandEngineeringUnitedInternational UniversityJuly2020-PresentMajor In:DataScienceCurrentCGPA:3.30\n",
      "---Page Break---\n",
      "DLEnigma1.0:ObjectDetectionforBangladeshiAutonomousDriving Developedmodelby ne-tuning YOLOandDetectronmodelsforobjectdetectioninautonomousdrivingapplicationsinBangladesh.Addressedchallengeswithuniqueobjectssuchasthree-wheelersandwheelchairstoenhancedetectionaccuracy TechStack: Pytorch,Yolov8,Detectron2.\n",
      "\n",
      "Undergraduateproject:AgriCareHub Developedawebapplicationthatallowsfarmerstouploadimagesoftheircropstocheckfordiseases.Theappusesimagerecognitiontoanalyzecrophealthand,ifadiseaseisdetected,recommendsmedicationandpropertreatmentinBengali.Italsoprovidesaudioinstructionsonhowtoapplythemedication.. TechStack: Python,FastAPI,TensorFlow, Express,React.js,MySQL.\n",
      "\n",
      "MLOlympiad: CO2EmissionsPredictionChallenge DevelopedandevaluatedvariousforecastingmodelsforCO2emissionsprediction. Afterthoroughanalysis,ARIMA,LSTM,andLogisticRegressionmodelsdemonstratedthebestperformanceinaccuratelypredictingemissionstrends. TechStack: ARIMA,SARIMA,LSTM,LogisticRegression,FacebookProphet,pandas\n",
      "\n",
      "Undergraduateproject:Learn2Resume DevelopedaCVgeneratorande-learningplatformallowinguserstocreateandshareCVs,purchasecerticationcourses,anddownloadcerticates.Integratedlivechatsupport,secureonlinepayments,andimplementedfeaturesforcourseandusermanagement. TechStack: PHP,JavaScript,MySQL,HTML,CSS..\n",
      "\n",
      "Undergraduateproject:easy-Publication Developedaself-publishingplatformenablingauthorstoeasilypublishtheirworkwithexibleterms,revenuesharing,and24-hourdeliverythroughanetworkof64printingpressesacrossdistricts.Theplatformaddresseschallengesfacedbynewauthorsintraditionalpublishing.Additionally,itintegratesonlinepaymentsystemsforsmoothtransactionsandlivechatsupportforreal-timeauthorassistance.. TechStack: PHP,MySQL,Bootstrap,HTML,CSS.\n",
      "\n",
      "Undergraduateproject:Medind\n",
      "\n",
      "Medindisawebapplicationdesignedtohelpusersndthenearestmedicineshopsandchecktheavailabilityoftheirdesiredmedicines.Theappalsoprovidesinformationontheoperatinghoursoftheseshops,souserscanplantheirvisitaccordingly.Additionally,shopkeeperscanuploadandmanagethelistofmedicinesavailableattheirstores. TechStack: PHP,MySQL,Bootstrap,HTML,CSS.\n",
      "\n",
      "COURSESMachineLearning|DigitalImageProcessing |Bioinformatics|HumanComputerInteraction|DatabaseManagementSystems|SoftwareEngineering|DataStructuresandAlgorithms|Object-OrientedProgramming|SystemDesignandAnalysis|\n",
      "\n",
      "INTERESTMultimodalLargeLanguageModel |Retrieval-AugmentedGeneration| AIAgent| Back-enddevelopment |ComputerVision\n",
      "\n",
      "|NaturalLanguageProcessing\n",
      "\n",
      "|AutomaticSpeechRecognition\n",
      "\n",
      "|\n",
      "---Page Break---\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = []\n",
    "\n",
    "async for page in loader.alazy_load():\n",
    "    # Replace single newlines with double newlines to simulate paragraphs\n",
    "    page.page_content = page.page_content.replace('\\n', '\\n\\n')\n",
    "    pages.append(page)\n",
    "\n",
    "# Example usage:\n",
    "for page in pages:\n",
    "    print(page.page_content)\n",
    "    print(\"---Page Break---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'machine learning.pdf', 'page': 0}\n",
      "\n",
      "DLEnigma1.0:ObjectDetectionforBangladeshiAutonomousDriving Developedmodelby ne-tuning YOLOandDetectronmodelsforobjectdetectioninautonomousdrivingapplicationsinBangladesh.Addressedchallengeswithuniqueobjectssuchasthree-wheelersandwheelchairstoenhancedetectionaccuracy TechStack: Pytorch,Yolov8,Detectron2.\n",
      "Undergraduateproject:AgriCareHub Developedawebapplicationthatallowsfarmerstouploadimagesoftheircropstocheckfordiseases.Theappusesimagerecognitiontoanalyzecrophealthand,ifadiseaseisdetected,recommendsmedicationandpropertreatmentinBengali.Italsoprovidesaudioinstructionsonhowtoapplythemedication.. TechStack: Python,FastAPI,TensorFlow, Express,React.js,MySQL.\n",
      "MLOlympiad: CO2EmissionsPredictionChallenge DevelopedandevaluatedvariousforecastingmodelsforCO2emissionsprediction. Afterthoroughanalysis,ARIMA,LSTM,andLogisticRegressionmodelsdemonstratedthebestperformanceinaccuratelypredictingemissionstrends. TechStack: ARIMA,SARIMA,LSTM,LogisticRegression,FacebookProphet,pandas\n",
      "Undergraduateproject:Learn2Resume DevelopedaCVgeneratorande-learningplatformallowinguserstocreateandshareCVs,purchasecerticationcourses,anddownloadcerticates.Integratedlivechatsupport,secureonlinepayments,andimplementedfeaturesforcourseandusermanagement. TechStack: PHP,JavaScript,MySQL,HTML,CSS..\n",
      "Undergraduateproject:easy-Publication Developedaself-publishingplatformenablingauthorstoeasilypublishtheirworkwithexibleterms,revenuesharing,and24-hourdeliverythroughanetworkof64printingpressesacrossdistricts.Theplatformaddresseschallengesfacedbynewauthorsintraditionalpublishing.Additionally,itintegratesonlinepaymentsystemsforsmoothtransactionsandlivechatsupportforreal-timeauthorassistance.. TechStack: PHP,MySQL,Bootstrap,HTML,CSS.\n",
      "Undergraduateproject:Medind\n",
      "Medindisawebapplicationdesignedtohelpusersndthenearestmedicineshopsandchecktheavailabilityoftheirdesiredmedicines.Theappalsoprovidesinformationontheoperatinghoursoftheseshops,souserscanplantheirvisitaccordingly.Additionally,shopkeeperscanuploadandmanagethelistofmedicinesavailableattheirstores. TechStack: PHP,MySQL,Bootstrap,HTML,CSS.\n",
      "COURSESMachineLearning|DigitalImageProcessing |Bioinformatics|HumanComputerInteraction|DatabaseManagementSystems|SoftwareEngineering|DataStructuresandAlgorithms|Object-OrientedProgramming|SystemDesignandAnalysis|\n",
      "INTERESTMultimodalLargeLanguageModel |Retrieval-AugmentedGeneration| AIAgent| Back-enddevelopment |ComputerVision\n",
      "|NaturalLanguageProcessing\n",
      "|AutomaticSpeechRecognition\n",
      "|\n"
     ]
    }
   ],
   "source": [
    "print(f\"{pages[0].metadata}\\n\")\n",
    "print(pages[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: pikepdf C++ to Python logger bridge initialized\n"
     ]
    }
   ],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "loader = UnstructuredPDFLoader(\n",
    "    file_path=file_path\n",
    ")\n",
    "docs = []\n",
    "for doc in loader.lazy_load():\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_page_docs = [doc for doc in docs if doc.metadata.get(\"page_number\") == 1]\n",
    "\n",
    "for doc in first_page_docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-unstructured\n",
      "  Downloading langchain_unstructured-0.1.6-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.6 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain-unstructured) (0.3.28)\n",
      "Collecting onnxruntime<=1.19.2,>=1.17.0 (from langchain-unstructured)\n",
      "  Downloading onnxruntime-1.19.2-cp310-cp310-win_amd64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: unstructured-client<1,>=0.27.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain-unstructured) (0.28.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (1.33)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (0.2.6)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (4.12.2)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from onnxruntime<=1.19.2,>=1.17.0->langchain-unstructured) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from onnxruntime<=1.19.2,>=1.17.0->langchain-unstructured) (24.12.23)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from onnxruntime<=1.19.2,>=1.17.0->langchain-unstructured) (1.26.4)\n",
      "Requirement already satisfied: protobuf in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from onnxruntime<=1.19.2,>=1.17.0->langchain-unstructured) (5.29.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from onnxruntime<=1.19.2,>=1.17.0->langchain-unstructured) (1.13.1)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured-client<1,>=0.27.0->langchain-unstructured) (24.1.0)\n",
      "Requirement already satisfied: cryptography>=3.1 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured-client<1,>=0.27.0->langchain-unstructured) (44.0.0)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured-client<1,>=0.27.0->langchain-unstructured) (0.2.2)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured-client<1,>=0.27.0->langchain-unstructured) (0.27.0)\n",
      "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured-client<1,>=0.27.0->langchain-unstructured) (1.0.6)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured-client<1,>=0.27.0->langchain-unstructured) (1.6.0)\n",
      "Requirement already satisfied: pypdf>=4.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured-client<1,>=0.27.0->langchain-unstructured) (5.1.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured-client<1,>=0.27.0->langchain-unstructured) (2.9.0.post0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured-client<1,>=0.27.0->langchain-unstructured) (1.0.0)\n",
      "Requirement already satisfied: typing-inspect<0.10.0,>=0.9.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from unstructured-client<1,>=0.27.0->langchain-unstructured) (0.9.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from cryptography>=3.1->unstructured-client<1,>=0.27.0->langchain-unstructured) (1.17.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpx>=0.27.0->unstructured-client<1,>=0.27.0->langchain-unstructured) (4.6.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpx>=0.27.0->unstructured-client<1,>=0.27.0->langchain-unstructured) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpx>=0.27.0->unstructured-client<1,>=0.27.0->langchain-unstructured) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpx>=0.27.0->unstructured-client<1,>=0.27.0->langchain-unstructured) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpx>=0.27.0->unstructured-client<1,>=0.27.0->langchain-unstructured) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client<1,>=0.27.0->langchain-unstructured) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (3.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (2.32.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from python-dateutil<3.0.0,>=2.8.2->unstructured-client<1,>=0.27.0->langchain-unstructured) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from typing-inspect<0.10.0,>=0.9.0->unstructured-client<1,>=0.27.0->langchain-unstructured) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from coloredlogs->onnxruntime<=1.19.2,>=1.17.0->langchain-unstructured) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from sympy->onnxruntime<=1.19.2,>=1.17.0->langchain-unstructured) (1.3.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client<1,>=0.27.0->langchain-unstructured) (2.21)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime<=1.19.2,>=1.17.0->langchain-unstructured) (3.5.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.6->langchain-unstructured) (2.2.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from anyio->httpx>=0.27.0->unstructured-client<1,>=0.27.0->langchain-unstructured) (1.2.0)\n",
      "Downloading langchain_unstructured-0.1.6-py3-none-any.whl (7.0 kB)\n",
      "Downloading onnxruntime-1.19.2-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.1 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.6/11.1 MB 6.5 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.6/11.1 MB 5.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.4/11.1 MB 5.2 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.7/11.1 MB 5.1 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.0/11.1 MB 4.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.8/11.1 MB 4.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.3/11.1 MB 4.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.3/11.1 MB 4.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.1/11.1 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.9/11.1 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.7/11.1 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.5/11.1 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 4.0 MB/s eta 0:00:00\n",
      "Installing collected packages: onnxruntime, langchain-unstructured\n",
      "  Attempting uninstall: onnxruntime\n",
      "    Found existing installation: onnxruntime 1.20.1\n",
      "    Uninstalling onnxruntime-1.20.1:\n",
      "      Successfully uninstalled onnxruntime-1.20.1\n",
      "Successfully installed langchain-unstructured-0.1.6 onnxruntime-1.19.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\samra\\anaconda3\\envs\\localrag3.0\\Lib\\site-packages\\~nnxruntime'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chromadb 0.5.23 requires tokenizers<=0.20.3,>=0.13.2, but you have tokenizers 0.21.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Load pages and process text\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m loader\u001b[38;5;241m.\u001b[39malazy_load():\n\u001b[1;32m---> 18\u001b[0m     fixed_page \u001b[38;5;241m=\u001b[39m add_spaces(page)  \u001b[38;5;66;03m# Apply space-fixing function to the page content\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     pages\u001b[38;5;241m.\u001b[39mappend(fixed_page)\n",
      "Cell \u001b[1;32mIn[27], line 11\u001b[0m, in \u001b[0;36madd_spaces\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_spaces\u001b[39m(text):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# A simple heuristic: add spaces where there's a capital letter after a lowercase letter.\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     fixed_text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([a-z])([A-Z])\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Another heuristic: Add space after numbers if followed by a letter.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     fixed_text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md)([A-Za-z])\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m, fixed_text)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\re.py:209\u001b[0m, in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msub\u001b[39m(pattern, repl, string, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    203\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import re\n",
    "\n",
    "# Load the PDF file\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = []\n",
    "\n",
    "# Function to add spaces between words\n",
    "def add_spaces(text):\n",
    "    # A simple heuristic: add spaces where there's a capital letter after a lowercase letter.\n",
    "    fixed_text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
    "    # Another heuristic: Add space after numbers if followed by a letter.\n",
    "    fixed_text = re.sub(r'(\\d)([A-Za-z])', r'\\1 \\2', fixed_text)\n",
    "    return fixed_text\n",
    "\n",
    "# Load pages and process text\n",
    "async for page in loader.alazy_load():\n",
    "    fixed_page = add_spaces(page)  # Apply space-fixing function to the page content\n",
    "    pages.append(fixed_page)\n",
    "\n",
    "# Now `pages` should have the text with spaces added\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import re\n",
    "\n",
    "# Load the PDF file\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = []\n",
    "\n",
    "# Function to add spaces between words\n",
    "def add_spaces(text):\n",
    "    # A simple heuristic: add spaces where there's a capital letter after a lowercase letter.\n",
    "    fixed_text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
    "    # Another heuristic: Add space after numbers if followed by a letter.\n",
    "    fixed_text = re.sub(r'(\\d)([A-Za-z])', r'\\1 \\2', fixed_text)\n",
    "    return fixed_text\n",
    "\n",
    "# Load pages and process text\n",
    "async for page in loader.alazy_load():\n",
    "    text = page.get_text() if hasattr(page, 'get_text') else str(page)  # Extract text from page\n",
    "    fixed_page = add_spaces(text)  # Apply space-fixing function to the page content\n",
    "    pages.append(fixed_page)\n",
    "\n",
    "# Now `pages` should have the text with spaces added\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import re\n",
    "\n",
    "# Load the PDF file\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = []\n",
    "\n",
    "# Function to add spaces between words more intelligently\n",
    "def add_spaces(text):\n",
    "    # 1. Add space before capital letters following lowercase letters.\n",
    "    fixed_text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
    "    \n",
    "    # 2. Add space after numbers followed by a letter (e.g., '123abc' -> '123 abc')\n",
    "    fixed_text = re.sub(r'(\\d)([A-Za-z])', r'\\1 \\2', fixed_text)\n",
    "    \n",
    "    # 3. Add space between words that are concatenated without spaces (common in PDFs)\n",
    "    # e.g., 'wordword' should become 'word word'\n",
    "    fixed_text = re.sub(r'([a-zA-Z])([A-Z])', r'\\1 \\2', fixed_text)  # For mixed case\n",
    "    fixed_text = re.sub(r'([a-zA-Z])([a-zA-Z])', r'\\1 \\2', fixed_text)  # For lowercase\n",
    "    \n",
    "    # 4. Replace non-breaking spaces (if any) with normal spaces\n",
    "    fixed_text = fixed_text.replace('\\xa0', ' ')  # Non-breaking space to regular space\n",
    "    \n",
    "    return fixed_text\n",
    "\n",
    "# Load pages and process text\n",
    "async for page in loader.alazy_load():\n",
    "    text = page.get_text() if hasattr(page, 'get_text') else str(page)  # Extract text from page\n",
    "    fixed_page = add_spaces(text)  # Apply space-fixing function to the page content\n",
    "    pages.append(fixed_page)\n",
    "\n",
    "# Now `pages` should have the text with better spacing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"p ag e_c on te nt='S A M R A T A B D U L J A L I L A I an d B ac ke nd E nt hu si as t\\nS K I L L S\\nB ac k-E nd: P yt ho n,C/C++,J av a,P hp, F as t A P I,E xp re ss.j s,N od e.j s D at ab as e: M y S Q L,M on go D B.F ro nt-E nd:J av a S cr ip t,T yp e S cr ip t,R ea ct.j s,H T M L,C S S.M ac hi ne L ea rn in g:T en so r F lo w,P y T or ch,K er as,S ci ki t-l ea rn,N um P y,P an da s,S ea bo rn,M at pl ot li b,O pe n C V,M ed ia P ip ee tc.A pp D ev el op me nt:F lu tt er.T oo ls:G it,D oc ke r,G it hu b A ct io n,J I R A,V S Co de,A na co nd a.\\nW O R K E X P E R I E N C E\\nU nd er gr ad ua te T he si s:G ra de V is io n(i nc ol la bo ra ti on wi th E du so ft bd L td) D ev el op ed an O C R-b as ed ap pl ic at io nt oa ut om at er es ul tm an ag em en tb yc ap tu ri ng re su lt da ta fr om im ag es.F ea tu re si nc lu de re al-t im ed at ae di ti ng,u pd at es,c en tr al iz ed da ta ba se st or ag e,s ec ti on-w is es ta ti st ic al vi ew s,a ut he nt ic at io n,a nd O T Pv er ic at io nv ia em ai l. T ec h S ta ck: P yt ho n,F as t A P I,F lu tt er,M y S Q L.\\nD L Sp ri nt2.0: B en ga li D oc um en t L ay ou t A na ly si s F in e-t un ed Y O L O an d D et ec tr on mo de ls to an al yz e B en ga li do cu me nt la yo ut s,t ac kl in gc ha ll en ge sw it hh is to ri ca ln ew sp ap er s,g ov er nm en td oc um en ts,m ag az in es,a nd bo ok s.T he mo de ls id en tie dp ar ag ra ph s,t ex tb ox es,i ma ge s,a nd ta bl es,w it h D et ec tr on de li ve ri ng su pe ri or re su lt s T ec h S ta ck: P yt or ch,D et ec tr on2,Y ol o.\\nB ha sh a B ic hi tr a\\n: B en ga li S pe ec h-t o-T ex tf or R eg io na l D ia le ct s F in e-t un ed th e W hi sp er mo de lf or A ut om at ic S pe ec h R ec og ni ti on(A S R)t oc on ve rt sp ee ch in to B en ga li re gi on al te xt.T he mo de lw as tr ai ne do nd iv er se da ta se ts to ha nd le va ri at io ns in pr on un ci at io na nd vo ca bu la ry ac ro ss di al ec ts. T ec h S ta ck: P yt or ch,W hi sp er.\\nB ha sh am ul:B en ga li R eg io na l I P A T ra ns cr ip ti on F in e-t un ed G oo gl e B y T5(s ma ll an db as ev er si on s)a nd m B A R T-5(b as ea nd sm al lv er si on s)f or B en ga li re gi on al I P At ra ns cr ip ti on.A ft er th or ou gh ev al ua ti on,t he B y T5 s ma ll mo de lo ut pe rf or me do th er s,p ro vi di ng su pe ri or tr an sc ri pt io na cc ur ac yf or re gi on al ph on et ic va ri at io ns.T hi sm od el im pr ov es tr an sc ri pt io nq ua li ty fo rr eg io na ls pe ec h-t o-t ex ta pp li ca ti on s. T ec h S ta ck: P yt or ch,U M T5 s ma ll an db as e,b yt5 s ma ll an db as e\\nC O N T A C T\\n+8801518445128 E ma il:s am ra ta bd ul ja li l21@g ma il.c om L in ke d I n:l in ke di n.c om/s am ra t-a bd ul-j al il G it H ub:g it hu b.c om/s am ra ta bd ul ja li l K ag gl e:k ag gl e/s am ra ta bd ul ja li l L ee tc od e:l ee tc od e/S am ra t_A bd ul_J al il B ad da,D ha ka.\\nA W A R D\\n K ag gl e E xp er t2024 F in al is ti n B ha sh am ul:B en ga li R eg io na l I P A T ra ns cr ip ti on 2024 T op10 F in al is ti n B ha sh a B ic hi tr a:A S Rf or R eg io na l D ia le ct s2024 T op10 F in al is ti n D L En ig ma1.0S U S T C S E C ar ni va l2024 8 t hi n M L Ol ym pi ad-C O2 E mi ss io ns P re di ct io n C ha ll en ge2024 C ha mp io ni n W in ne r C ar ee r L au nc hp ad Q ui zc om pe ti ti on2023 F in al is ti n D L Sp ri nt2.0-B U E T C S E F es t2023 T op10 F in al is ti n S ki tt o H ac ka th on2022 C ha mp io ni n U I U O nl in e C is co Q ui z C om pe ti ti on O rg an iz ed by G P Ac ad em y 2022 C ha mp io ni n U I U A I C on te st2022 C ha mp io ni n P ro gr am mi ng fo rb eg in ne rs2021\\nE D U C A T I O N B.S c.i n C om pu te r S ci en ce an d E ng in ee ri ng U ni te d I nt er na ti on al U ni ve rs it y J ul y2020-P re se nt M aj or I n:D at a S ci en ce C ur re nt C G P A:3.30' m et ad at a={'s ou rc e': 'm ac hi ne l ea rn in g.p df', 'p ag e': 0}\", \"p ag e_c on te nt='D L En ig ma1.0:O bj ec t D et ec ti on fo r B an gl ad es hi A ut on om ou s D ri vi ng D ev el op ed mo de lb y n e-t un in g Y O L O an d D et ec tr on mo de ls fo ro bj ec td et ec ti on in au to no mo us dr iv in ga pp li ca ti on si n B an gl ad es h.A dd re ss ed ch al le ng es wi th un iq ue ob je ct ss uc ha st hr ee-w he el er sa nd wh ee lc ha ir st oe nh an ce de te ct io na cc ur ac y T ec h S ta ck: P yt or ch,Y ol ov8,D et ec tr on2.\\nU nd er gr ad ua te pr oj ec t:A gr i C ar e H ub D ev el op ed aw eb ap pl ic at io nt ha ta ll ow sf ar me rs to up lo ad im ag es of th ei rc ro ps to ch ec kf or di se as es.T he ap pu se si ma ge re co gn it io nt oa na ly ze cr op he al th an d,i fa di se as ei sd et ec te d,r ec om me nd sm ed ic at io na nd pr op er tr ea tm en ti n B en ga li.I ta ls op ro vi de sa ud io in st ru ct io ns on ho wt oa pp ly th em ed ic at io n.. T ec h S ta ck: P yt ho n,F as t A P I,T en so r F lo w, E xp re ss,R ea ct.j s,M y S Q L.\\nM L Ol ym pi ad: C O2 E mi ss io ns P re di ct io n C ha ll en ge D ev el op ed an de va lu at ed va ri ou sf or ec as ti ng mo de ls fo r C O2 e mi ss io ns pr ed ic ti on. A ft er th or ou gh an al ys is,A R I M A,L S T M,a nd L og is ti c R eg re ss io nm od el sd em on st ra te dt he be st pe rf or ma nc ei na cc ur at el yp re di ct in ge mi ss io ns tr en ds. T ec h S ta ck: A R I M A,S A R I M A,L S T M,L og is ti c R eg re ss io n,F ac eb oo k P ro ph et,p an da s\\nU nd er gr ad ua te pr oj ec t:L ea rn2 R es um e D ev el op ed a C V ge ne ra to ra nd e-l ea rn in gp la tf or ma ll ow in gu se rs to cr ea te an ds ha re C V s,p ur ch as ec er tic at io nc ou rs es,a nd do wn lo ad ce rt ic at es.I nt eg ra te dl iv ec ha ts up po rt,s ec ur eo nl in ep ay me nt s,a nd im pl em en te df ea tu re sf or co ur se an du se rm an ag em en t. T ec h S ta ck: P H P,J av a S cr ip t,M y S Q L,H T M L,C S S..\\nU nd er gr ad ua te pr oj ec t:e as y-P ub li ca ti on D ev el op ed as el f-p ub li sh in gp la tf or me na bl in ga ut ho rs to ea si ly pu bl is ht he ir wo rk wi the xi bl et er ms,r ev en ue sh ar in g,a nd24-h ou rd el iv er yt hr ou gh an et wo rk of64 p ri nt in gp re ss es ac ro ss di st ri ct s.T he pl at fo rm ad dr es se sc ha ll en ge sf ac ed by ne wa ut ho rs in tr ad it io na lp ub li sh in g.A dd it io na ll y,i ti nt eg ra te so nl in ep ay me nt sy st em sf or sm oo th tr an sa ct io ns an dl iv ec ha ts up po rt fo rr ea l-t im ea ut ho ra ss is ta nc e.. T ec h S ta ck: P H P,M y S Q L,B oo ts tr ap,H T M L,C S S.\\nU nd er gr ad ua te pr oj ec t:M ed in d\\nM ed in di sa we ba pp li ca ti on de si gn ed to he lp us er sn dt he ne ar es tm ed ic in es ho ps an dc he ck th ea va il ab il it yo ft he ir de si re dm ed ic in es.T he ap pa ls op ro vi de si nf or ma ti on on th eo pe ra ti ng ho ur so ft he se sh op s,s ou se rs ca np la nt he ir vi si ta cc or di ng ly.A dd it io na ll y,s ho pk ee pe rs ca nu pl oa da nd ma na ge th el is to fm ed ic in es av ai la bl ea tt he ir st or es. T ec h S ta ck: P H P,M y S Q L,B oo ts tr ap,H T M L,C S S.\\nC O U R S E S M ac hi ne L ea rn in g|D ig it al I ma ge P ro ce ss in g |B io in fo rm at ic s|H um an C om pu te r I nt er ac ti on|D at ab as e M an ag em en t S ys te ms|S of tw ar e E ng in ee ri ng|D at a S tr uc tu re sa nd A lg or it hm s|O bj ec t-O ri en te d P ro gr am mi ng|S ys te m D es ig na nd A na ly si s|\\nI N T E R E S T Mu lt im od al L ar ge L an gu ag e M od el |R et ri ev al-A ug me nt ed G en er at io n| A I Ag en t| B ac k-e nd de ve lo pm en t |C om pu te r V is io n\\n|N at ur al L an gu ag e P ro ce ss in g\\n|A ut om at ic S pe ec h R ec og ni ti on\\n|' m et ad at a={'s ou rc e': 'm ac hi ne l ea rn in g.p df', 'p ag e': 1}\"]\n"
     ]
    }
   ],
   "source": [
    "print(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import re\n",
    "\n",
    "# Load the PDF file\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = []\n",
    "\n",
    "# Function to add spaces between words more intelligently\n",
    "def add_spaces(text):\n",
    "    # 1. Add space before capital letters following lowercase letters.\n",
    "    fixed_text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
    "\n",
    "    # 2. Add space after numbers followed by a letter (e.g., '123abc' -> '123 abc')\n",
    "    fixed_text = re.sub(r'(\\d)([A-Za-z])', r'\\1 \\2', fixed_text)\n",
    "\n",
    "    # 3. Add space between consecutive capital letters (CamelCase words)\n",
    "    fixed_text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', fixed_text)  # Lower to Upper\n",
    "    \n",
    "    # 4. Remove extra spaces that may have been introduced\n",
    "    fixed_text = re.sub(r'\\s+', ' ', fixed_text).strip()\n",
    "    \n",
    "    return fixed_text\n",
    "\n",
    "# Load pages and process text\n",
    "async for page in loader.alazy_load():\n",
    "    text = page.get_text() if hasattr(page, 'get_text') else str(page)  # Extract text from page\n",
    "    fixed_page = add_spaces(text)  # Apply space-fixing function to the page content\n",
    "    pages.append(fixed_page)\n",
    "\n",
    "# Now `pages` should have the text with better spacing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"page_content='SAMRATABDULJALILAIand Backend Enthusiast SKILLS Back-End: Python,C/C++,Java,Php, Fast API,Express.js,Node.js Database: My SQL,Mongo DB.Front-End:Java Script,Type Script,React.js,HTML,CSS.Machine Learning:Tensor Flow,Py Torch,Keras,Scikit-learn,Num Py,Pandas,Seaborn,Matplotlib,Open CV,Media Pipeetc.App Development:Flutter.Tools:Git,Docker,Github Action,JIRA,VSCode,Anaconda. WORKEXPERIENCE Undergraduate Thesis:Grade Vision(incollaborationwith Edusoftbd Ltd) Developedan OCR-basedapplicationtoautomateresultmanagementbycapturingresultdatafromimages.Featuresincludereal-timedataediting,updates,centralizeddatabasestorage,section-wisestatisticalviews,authentication,and OTPvericationviaemail. Tech Stack: Python,Fast API,Flutter,My SQL. DLSprint2.0: Bengali Document Layout Analysis Fine-tuned YOLOand Detectronmodelstoanalyze Bengalidocumentlayouts,tacklingchallengeswithhistoricalnewspapers,governmentdocuments,magazines,andbooks.Themodelsidentiedparagraphs,textboxes,images,andtables,with Detectrondeliveringsuperiorresults Tech Stack: Pytorch,Detectron2,Yolo. Bhasha Bichitra : Bengali Speech-to-Textfor Regional Dialects Fine-tunedthe Whispermodelfor Automatic Speech Recognition(ASR)toconvertspeechinto Bengaliregionaltext.Themodelwastrainedondiversedatasetstohandlevariationsinpronunciationandvocabularyacrossdialects. Tech Stack: Pytorch,Whisper. Bhashamul:Bengali Regional IPATranscription Fine-tuned Google By T5(smallandbaseversions)andm BART-5(baseandsmallversions)for Bengaliregional IPAtranscription.Afterthoroughevaluation,the By T5 smallmodeloutperformedothers,providingsuperiortranscriptionaccuracyforregionalphoneticvariations.Thismodelimprovestranscriptionqualityforregionalspeech-to-textapplications. Tech Stack: Pytorch,UMT5 smallandbase,byt5 smallandbase CONTACT +8801518445128 Email:samratabduljalil21@gmail.com Linked In:linkedin.com/samrat-abdul-jalil Git Hub:github.com/samratabduljalil Kaggle:kaggle/samratabduljalil Leetcode:leetcode/Samrat_Abdul_Jalil Badda,Dhaka. AWARD  Kaggle Expert2024 Finalistin Bhashamul:Bengali Regional IPATranscription 2024 Top10 Finalistin Bhasha Bichitra:ASRfor Regional Dialects2024 Top10 Finalistin DLEnigma1.0SUSTCSECarnival2024 8 thin MLOlympiad-CO2 Emissions Prediction Challenge2024 Championin Winner Career Launchpad Quizcompetition2023 Finalistin DLSprint2.0-BUETCSEFest2023 Top10 Finalistin Skitto Hackathon2022 Championin UIUOnline Cisco Quiz Competition Organizedby GPAcademy 2022 Championin UIUAIContest2022 Championin Programmingforbeginners2021 EDUCATIONB.Sc.in Computer Scienceand Engineering United International University July2020-Present Major In:Data Science Current CGPA:3.30' metadata={'source': 'machine learning.pdf', 'page': 0}\", \"page_content='DLEnigma1.0:Object Detectionfor Bangladeshi Autonomous Driving Developedmodelby ne-tuning YOLOand Detectronmodelsforobjectdetectioninautonomousdrivingapplicationsin Bangladesh.Addressedchallengeswithuniqueobjectssuchasthree-wheelersandwheelchairstoenhancedetectionaccuracy Tech Stack: Pytorch,Yolov8,Detectron2. Undergraduateproject:Agri Care Hub Developedawebapplicationthatallowsfarmerstouploadimagesoftheircropstocheckfordiseases.Theappusesimagerecognitiontoanalyzecrophealthand,ifadiseaseisdetected,recommendsmedicationandpropertreatmentin Bengali.Italsoprovidesaudioinstructionsonhowtoapplythemedication.. Tech Stack: Python,Fast API,Tensor Flow, Express,React.js,My SQL. MLOlympiad: CO2 Emissions Prediction Challenge Developedandevaluatedvariousforecastingmodelsfor CO2 emissionsprediction. Afterthoroughanalysis,ARIMA,LSTM,and Logistic Regressionmodelsdemonstratedthebestperformanceinaccuratelypredictingemissionstrends. Tech Stack: ARIMA,SARIMA,LSTM,Logistic Regression,Facebook Prophet,pandas Undergraduateproject:Learn2 Resume Developeda CVgeneratorande-learningplatformallowinguserstocreateandshare CVs,purchasecerticationcourses,anddownloadcerticates.Integratedlivechatsupport,secureonlinepayments,andimplementedfeaturesforcourseandusermanagement. Tech Stack: PHP,Java Script,My SQL,HTML,CSS.. Undergraduateproject:easy-Publication Developedaself-publishingplatformenablingauthorstoeasilypublishtheirworkwithexibleterms,revenuesharing,and24-hourdeliverythroughanetworkof64 printingpressesacrossdistricts.Theplatformaddresseschallengesfacedbynewauthorsintraditionalpublishing.Additionally,itintegratesonlinepaymentsystemsforsmoothtransactionsandlivechatsupportforreal-timeauthorassistance.. Tech Stack: PHP,My SQL,Bootstrap,HTML,CSS. Undergraduateproject:Medind Medindisawebapplicationdesignedtohelpusersndthenearestmedicineshopsandchecktheavailabilityoftheirdesiredmedicines.Theappalsoprovidesinformationontheoperatinghoursoftheseshops,souserscanplantheirvisitaccordingly.Additionally,shopkeeperscanuploadandmanagethelistofmedicinesavailableattheirstores. Tech Stack: PHP,My SQL,Bootstrap,HTML,CSS. COURSESMachine Learning|Digital Image Processing |Bioinformatics|Human Computer Interaction|Database Management Systems|Software Engineering|Data Structuresand Algorithms|Object-Oriented Programming|System Designand Analysis| INTERESTMultimodal Large Language Model |Retrieval-Augmented Generation| AIAgent| Back-enddevelopment |Computer Vision |Natural Language Processing |Automatic Speech Recognition |' metadata={'source': 'machine learning.pdf', 'page': 1}\"]\n"
     ]
    }
   ],
   "source": [
    "print(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "import re\n",
    "\n",
    "# Load the PDF using UnstructuredPDFLoader\n",
    "file_path = 'your_pdf_file.pdf'\n",
    "loader = UnstructuredPDFLoader(file_path)\n",
    "\n",
    "# Function to add spaces between words more intelligently\n",
    "def add_spaces(text):\n",
    "    # 1. Add space before capital letters following lowercase letters.\n",
    "    fixed_text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
    "\n",
    "    # 2. Add space after numbers followed by a letter (e.g., '123abc' -> '123 abc')\n",
    "    fixed_text = re.sub(r'(\\d)([A-Za-z])', r'\\1 \\2', fixed_text)\n",
    "\n",
    "    # 3. Add space between consecutive capital letters (CamelCase words)\n",
    "    fixed_text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', fixed_text)  # Lower to Upper\n",
    "    \n",
    "    # 4. Remove extra spaces that may have been introduced\n",
    "    fixed_text = re.sub(r'\\s+', ' ', fixed_text).strip()\n",
    "    \n",
    "    return fixed_text\n",
    "\n",
    "# Load and process PDF document\n",
    "pages = []\n",
    "documents = loader.load()\n",
    "\n",
    "for document in documents:\n",
    "    text = document.page_content  # Extract the text from each page\n",
    "    fixed_page = add_spaces(text)  # Apply space-fixing function to the page content\n",
    "    pages.append(fixed_page)\n",
    "\n",
    "# Now `pages` should have the text with better spacing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "loader = UnstructuredPDFLoader(file_path)\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'machine learning.pdf'}, page_content='SAMRAT ABDUL JALIL AI and Backend Enthusiast\\n\\nSKILLS\\n\\nBack-End: Python, C/C++,Java ,Php, FastAPI, Express.js, Node.js Database: MySQL, MongoDB . Front-End: JavaScript, TypeScript, React.js, HTML, CSS . Machine Learning: TensorFlow, PyTorch, Keras, Scikit-learn, NumPy, Pandas, Seaborn,Matplotlib, OpenCV, MediaPipe etc. App Development: Flutter . Tools: Git, Docker, Github Action ,JIRA, VS Code, Anaconda .\\n\\nWORK EXPERIENCE\\n\\nUndergraduate Thesis: GradeVision (in collaboration with Edusoftbd Ltd)  Developed an OCR-based application to automate result\\n\\nmanagement by capturing result data from images. Features include real-time data editing, updates, centralized database storage, section-wise statistical views, authentication, and OTP verication via email.\\n\\nTech Stack : Python, FastAPI, Flutter, MySQL .\\n\\nDL Sprint 2.0: Bengali Document Layout Analysis  Fine-tuned YOLO and Detectron models to analyze Bengali\\n\\ndocument layouts, tackling challenges with historical newspapers, government documents, magazines, and books. The models identied paragraphs, text boxes, images, and tables, with Detectron delivering superior results\\n\\nTech Stack : Pytorch, Detectron2, Yolo .\\n\\nBhasha Bichitra: Bengali Speech-to-Text for Regional Dialects  Fine-tuned the Whisper model for Automatic Speech\\n\\nRecognition (ASR) to convert speech into Bengali regional text. The model was trained on diverse datasets to handle variations in pronunciation and vocabulary across dialects .\\n\\nTech Stack : Pytorch, Whisper .\\n\\nBhashamul: Bengali Regional IPA Transcription  Fine-tuned Google ByT5 (small and base versions) and\\n\\nmBART-5 (base and small versions) for Bengali regional IPA transcription. After thorough evaluation, the ByT5 small model outperformed others, providing superior transcription accuracy for regional phonetic variations. This model improves transcription quality for regional speech-to-text applications.\\n\\nTech Stack : Pytorch, UMT5 small and base , byt5 small and base\\n\\nCONTACT\\n\\n+880 1518445128 Email: samratabduljalil21@gmail.com LinkedIn: linkedin.com/samrat-abdul-jalil GitHub: github.com/samratabduljalil Kaggle: kaggle/samratabduljalil Leetcode: leetcode/Samrat_Abdul_Jalil Badda, Dhaka.\\n\\nAWARD\\n\\nKaggle Expert  2024  Finalist in Bhashamul: Bengali Regional IPA Transcription  2024  Top 10 Finalist in Bhasha Bichitra:\\n\\nASR for Regional Dialects  2024  Top 10 Finalist in DL Enigma 1.0 \\n\\nSUST CSE Carnival 2024\\n\\n8th in ML Olympiad - CO2 Emissions\\n\\nPrediction Challenge  2024  Champion in Winner Career\\n\\nLaunchpad Quiz competition  2023\\n\\nFinalist in DL Sprint 2.0 - BUET CSE\\n\\nFest  2023\\n\\nTop 10 Finalist in Skitto Hackathon \\n\\n2022\\n\\nChampion in UIU Online Cisco Quiz Competition Organized by GP Academy  2022\\n\\nChampion in UIU AI Contest  2022  Champion in Programming for beginners  2021\\n\\nEDUCATION\\n\\nB.Sc. in Computer Science and Engineering United International University July 2020 - Present\\n\\nMajor In : Data Science Current CGPA: 3.30\\n\\nDL Enigma 1.0 : Object Detection for Bangladeshi Autonomous Driving  Developed model by ne-tuning YOLO and Detectron models for object detection in autonomous driving applications in Bangladesh. Addressed challenges with unique objects such as three-wheelers and wheelchairs to enhance detection accuracy\\n\\nTech Stack : Pytorch, Yolov8, Detectron2 .\\n\\nUndergraduate project: AgriCareHub  Developed a web application that allows farmers to upload\\n\\nimages of their crops to check for diseases. The app uses image recognition to analyze crop health and, if a disease is detected, recommends medication and proper treatment in Bengali. It also provides audio instructions on how to apply the medication..\\n\\nTech Stack : Python, FastAPI, TensorFlow, Express, React.js, MySQL .\\n\\nML Olympiad : CO2 Emissions Prediction Challenge  Developed and evaluated various forecasting models for CO2 emissions prediction. After thorough analysis, ARIMA, LSTM, and Logistic Regression models demonstrated the best performance in accurately predicting emissions trends.\\n\\nTech Stack : ARIMA, SARIMA, LSTM, Logistic Regression,Facebook Prophet, pandas\\n\\nUndergraduate project: Learn2Resume  Developed a CV generator and e-learning platform allowing\\n\\nusers to create and share CVs, purchase certication courses, and download certicates. Integrated live chat support, secure online payments, and implemented features for course and user management.\\n\\nTech Stack : PHP, JavaScript, MySQL, HTML, CSS . . Undergraduate project: easy-Publication  Developed a self-publishing platform enabling authors to easily publish their work with exible terms, revenue sharing, and 24-hour delivery through a network of 64 printing presses across districts. The platform addresses challenges faced by new authors in traditional publishing. Additionally, it integrates online payment systems for smooth transactions and live chat support for real-time author assistance..\\n\\nTech Stack : PHP , MySQL, Bootstrap ,HTML, CSS .\\n\\nUndergraduate project: Medind  Medind is a web application designed to help users nd the nearest medicine shops and check the availability of their desired medicines. The app also provides information on the operating hours of these shops, so users can plan their visit accordingly. Additionally, shopkeepers can upload and manage the list of medicines available at their stores.\\n\\nTech Stack : PHP , MySQL, Bootstrap ,HTML, CSS .\\n\\nCOURSES\\n\\nMachine Learning | Digital Image\\n\\nProcessing | Bioinformatics |\\n\\nHuman Computer Interaction |\\n\\nDatabase Management Systems |\\n\\nSoftware Engineering | Data\\n\\nStructures and Algorithms |\\n\\nObject-Oriented Programming |\\n\\nSystem Design and Analysis |\\n\\nINTEREST\\n\\nMultimodal Large Language Model | Retrieval-Augmented Generation | AI Agent | Back-end development | Computer Vision | Natural Language Processing | Automatic Speech Recognition |')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m text_without_newlines \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m)\n\u001b[0;32m      2\u001b[0m text_without_newlines\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "text_without_newlines = re.sub(r'\\n+', ' ', documents.page_content)\n",
    "text_without_newlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: PDF text extraction failed, skip text extraction...\n"
     ]
    },
    {
     "ename": "PDFInfoNotInstalledError",
     "evalue": "Unable to get page count. Is poppler installed and in PATH?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\pdf2image\\pdf2image.py:581\u001b[0m, in \u001b[0;36mpdfinfo_from_path\u001b[1;34m(pdf_path, userpw, ownerpw, poppler_path, rawdates, timeout, first_page, last_page)\u001b[0m\n\u001b[0;32m    580\u001b[0m     env[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLD_LIBRARY_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m poppler_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m env\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLD_LIBRARY_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 581\u001b[0m proc \u001b[38;5;241m=\u001b[39m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[0;32m    968\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    969\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 971\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\subprocess.py:1456\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1455\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1456\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1457\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1458\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1466\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1470\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPDFInfoNotInstalledError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Load and process PDF document\u001b[39;00m\n\u001b[0;32m     25\u001b[0m pages \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 26\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m document \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[0;32m     29\u001b[0m     text \u001b[38;5;241m=\u001b[39m document\u001b[38;5;241m.\u001b[39mpage_content  \u001b[38;5;66;03m# Extract the text from each page\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\langchain_core\\document_loaders\\base.py:31\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[0;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\langchain_community\\document_loaders\\unstructured.py:107\u001b[0m, in \u001b[0;36mUnstructuredBaseLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlazy_load\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Document]:\n\u001b[0;32m    106\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_process_elements(elements)\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melements\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:74\u001b[0m, in \u001b[0;36mUnstructuredPDFLoader._get_elements\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_elements\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpartition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpdf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partition_pdf\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m partition_pdf(filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munstructured_kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\unstructured\\documents\\elements.py:581\u001b[0m, in \u001b[0;36mprocess_metadata.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Element]:\n\u001b[1;32m--> 581\u001b[0m     elements \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    582\u001b[0m     call_args \u001b[38;5;241m=\u001b[39m get_call_args_applying_defaults(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    584\u001b[0m     unique_element_ids: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m call_args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munique_element_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\unstructured\\file_utils\\filetype.py:725\u001b[0m, in \u001b[0;36madd_filetype.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    723\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Element]:\n\u001b[1;32m--> 725\u001b[0m     elements \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    727\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m elements:\n\u001b[0;32m    728\u001b[0m         \u001b[38;5;66;03m# NOTE(robinson) - Attached files have already run through this logic\u001b[39;00m\n\u001b[0;32m    729\u001b[0m         \u001b[38;5;66;03m# in their own partitioning function\u001b[39;00m\n\u001b[0;32m    730\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m element\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mattached_to_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\unstructured\\file_utils\\filetype.py:683\u001b[0m, in \u001b[0;36madd_metadata.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Element]:\n\u001b[1;32m--> 683\u001b[0m     elements \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    684\u001b[0m     call_args \u001b[38;5;241m=\u001b[39m get_call_args_applying_defaults(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m call_args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata_filename\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\unstructured\\chunking\\dispatch.py:74\u001b[0m, in \u001b[0;36madd_chunking_strategy.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The decorated function is replaced with this one.\"\"\"\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# -- call the partitioning function to get the elements --\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m elements \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# -- look for a chunking-strategy argument --\u001b[39;00m\n\u001b[0;32m     77\u001b[0m call_args \u001b[38;5;241m=\u001b[39m get_call_args_applying_defaults(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\unstructured\\partition\\pdf.py:209\u001b[0m, in \u001b[0;36mpartition_pdf\u001b[1;34m(filename, file, include_page_breaks, strategy, infer_table_structure, ocr_languages, languages, metadata_filename, metadata_last_modified, chunking_strategy, hi_res_model_name, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, starting_page_number, extract_forms, form_extraction_skip_tables, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m exactly_one(filename\u001b[38;5;241m=\u001b[39mfilename, file\u001b[38;5;241m=\u001b[39mfile)\n\u001b[0;32m    207\u001b[0m languages \u001b[38;5;241m=\u001b[39m check_language_args(languages \u001b[38;5;129;01mor\u001b[39;00m [], ocr_languages)\n\u001b[1;32m--> 209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m partition_pdf_or_image(\n\u001b[0;32m    210\u001b[0m     filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m    211\u001b[0m     file\u001b[38;5;241m=\u001b[39mfile,\n\u001b[0;32m    212\u001b[0m     include_page_breaks\u001b[38;5;241m=\u001b[39minclude_page_breaks,\n\u001b[0;32m    213\u001b[0m     strategy\u001b[38;5;241m=\u001b[39mstrategy,\n\u001b[0;32m    214\u001b[0m     infer_table_structure\u001b[38;5;241m=\u001b[39minfer_table_structure,\n\u001b[0;32m    215\u001b[0m     languages\u001b[38;5;241m=\u001b[39mlanguages,\n\u001b[0;32m    216\u001b[0m     metadata_last_modified\u001b[38;5;241m=\u001b[39mmetadata_last_modified,\n\u001b[0;32m    217\u001b[0m     hi_res_model_name\u001b[38;5;241m=\u001b[39mhi_res_model_name,\n\u001b[0;32m    218\u001b[0m     extract_images_in_pdf\u001b[38;5;241m=\u001b[39mextract_images_in_pdf,\n\u001b[0;32m    219\u001b[0m     extract_image_block_types\u001b[38;5;241m=\u001b[39mextract_image_block_types,\n\u001b[0;32m    220\u001b[0m     extract_image_block_output_dir\u001b[38;5;241m=\u001b[39mextract_image_block_output_dir,\n\u001b[0;32m    221\u001b[0m     extract_image_block_to_payload\u001b[38;5;241m=\u001b[39mextract_image_block_to_payload,\n\u001b[0;32m    222\u001b[0m     starting_page_number\u001b[38;5;241m=\u001b[39mstarting_page_number,\n\u001b[0;32m    223\u001b[0m     extract_forms\u001b[38;5;241m=\u001b[39mextract_forms,\n\u001b[0;32m    224\u001b[0m     form_extraction_skip_tables\u001b[38;5;241m=\u001b[39mform_extraction_skip_tables,\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    226\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\unstructured\\partition\\pdf.py:339\u001b[0m, in \u001b[0;36mpartition_pdf_or_image\u001b[1;34m(filename, file, is_image, include_page_breaks, strategy, infer_table_structure, languages, metadata_last_modified, hi_res_model_name, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, starting_page_number, extract_forms, form_extraction_skip_tables, **kwargs)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m strategy \u001b[38;5;241m==\u001b[39m PartitionStrategy\u001b[38;5;241m.\u001b[39mOCR_ONLY:\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;66;03m# NOTE(robinson): Catches file conversion warnings when running with PDFs\u001b[39;00m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[1;32m--> 339\u001b[0m         elements \u001b[38;5;241m=\u001b[39m _partition_pdf_or_image_with_ocr(\n\u001b[0;32m    340\u001b[0m             filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m    341\u001b[0m             file\u001b[38;5;241m=\u001b[39mfile,\n\u001b[0;32m    342\u001b[0m             include_page_breaks\u001b[38;5;241m=\u001b[39minclude_page_breaks,\n\u001b[0;32m    343\u001b[0m             languages\u001b[38;5;241m=\u001b[39mlanguages,\n\u001b[0;32m    344\u001b[0m             ocr_languages\u001b[38;5;241m=\u001b[39mocr_languages,\n\u001b[0;32m    345\u001b[0m             is_image\u001b[38;5;241m=\u001b[39mis_image,\n\u001b[0;32m    346\u001b[0m             metadata_last_modified\u001b[38;5;241m=\u001b[39mmetadata_last_modified \u001b[38;5;129;01mor\u001b[39;00m last_modified,\n\u001b[0;32m    347\u001b[0m             starting_page_number\u001b[38;5;241m=\u001b[39mstarting_page_number,\n\u001b[0;32m    348\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    349\u001b[0m         )\n\u001b[0;32m    350\u001b[0m         out_elements \u001b[38;5;241m=\u001b[39m _process_uncategorized_text_elements(elements)\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_elements\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\unstructured\\partition\\pdf.py:860\u001b[0m, in \u001b[0;36m_partition_pdf_or_image_with_ocr\u001b[1;34m(filename, file, include_page_breaks, languages, ocr_languages, is_image, metadata_last_modified, starting_page_number, **kwargs)\u001b[0m\n\u001b[0;32m    858\u001b[0m         elements\u001b[38;5;241m.\u001b[39mextend(page_elements)\n\u001b[0;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 860\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m page_number, image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[0;32m    861\u001b[0m         convert_pdf_to_images(filename, file), start\u001b[38;5;241m=\u001b[39mstarting_page_number\n\u001b[0;32m    862\u001b[0m     ):\n\u001b[0;32m    863\u001b[0m         page_elements \u001b[38;5;241m=\u001b[39m _partition_pdf_or_image_with_ocr_from_image(\n\u001b[0;32m    864\u001b[0m             image\u001b[38;5;241m=\u001b[39mimage,\n\u001b[0;32m    865\u001b[0m             languages\u001b[38;5;241m=\u001b[39mlanguages,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    870\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    871\u001b[0m         )\n\u001b[0;32m    872\u001b[0m         elements\u001b[38;5;241m.\u001b[39mextend(page_elements)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\unstructured\\partition\\pdf_image\\pdf_image_utils.py:400\u001b[0m, in \u001b[0;36mconvert_pdf_to_images\u001b[1;34m(filename, file, chunk_size)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    399\u001b[0m     f_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 400\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[43mpdf2image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpdfinfo_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    402\u001b[0m total_pages \u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start_page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, total_pages \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, chunk_size):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\localrag3.0\\lib\\site-packages\\pdf2image\\pdf2image.py:607\u001b[0m, in \u001b[0;36mpdfinfo_from_path\u001b[1;34m(pdf_path, userpw, ownerpw, poppler_path, rawdates, timeout, first_page, last_page)\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m d\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PDFInfoNotInstalledError(\n\u001b[0;32m    608\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to get page count. Is poppler installed and in PATH?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    609\u001b[0m     )\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    611\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PDFPageCountError(\n\u001b[0;32m    612\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to get page count.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m     )\n",
      "\u001b[1;31mPDFInfoNotInstalledError\u001b[0m: Unable to get page count. Is poppler installed and in PATH?"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "import re\n",
    "\n",
    "# Load the PDF using UnstructuredPDFLoader\n",
    "file_path = 'your_pdf_file.pdf'\n",
    "loader = UnstructuredPDFLoader(file_path)\n",
    "\n",
    "# Function to add spaces between words more intelligently\n",
    "def add_spaces(text):\n",
    "    # 1. Add space before capital letters following lowercase letters.\n",
    "    fixed_text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
    "\n",
    "    # 2. Add space after numbers followed by a letter (e.g., '123abc' -> '123 abc')\n",
    "    fixed_text = re.sub(r'(\\d)([A-Za-z])', r'\\1 \\2', fixed_text)\n",
    "\n",
    "    # 3. Add space between consecutive capital letters (CamelCase words)\n",
    "    fixed_text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', fixed_text)  # Lower to Upper\n",
    "    \n",
    "    # 4. Remove extra spaces that may have been introduced\n",
    "    fixed_text = re.sub(r'\\s+', ' ', fixed_text).strip()\n",
    "    \n",
    "    return fixed_text\n",
    "\n",
    "# Load and process PDF document\n",
    "pages = []\n",
    "documents = loader.load()\n",
    "\n",
    "for document in documents:\n",
    "    text = document.page_content  # Extract the text from each page\n",
    "    fixed_page = add_spaces(text)  # Apply space-fixing function to the page content\n",
    "    pages.append(fixed_page)\n",
    "\n",
    "# Now `pages` should have the text with better spacing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting poppler-utils\n",
      "  Downloading poppler_utils-0.1.0-py3-none-any.whl.metadata (883 bytes)\n",
      "Requirement already satisfied: Click>=7.0 in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from poppler-utils) (8.1.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\samra\\anaconda3\\envs\\localrag3.0\\lib\\site-packages (from Click>=7.0->poppler-utils) (0.4.6)\n",
      "Downloading poppler_utils-0.1.0-py3-none-any.whl (9.2 kB)\n",
      "Installing collected packages: poppler-utils\n",
      "Successfully installed poppler-utils-0.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install poppler-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
